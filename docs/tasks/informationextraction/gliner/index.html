<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-tasks/informationextraction/gliner" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.0">
<title data-rh="true">GliNER | Logic Baron</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" name="twitter:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" property="og:url" content="https://logicbaron.github.io/docs/tasks/informationextraction/gliner"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="GliNER | Logic Baron"><meta data-rh="true" name="description" content="Generalist Model for Named Entity Recognition using Bidirecitonal Transformer"><meta data-rh="true" property="og:description" content="Generalist Model for Named Entity Recognition using Bidirecitonal Transformer"><link data-rh="true" rel="icon" href="/img/logicbaron_32.ico"><link data-rh="true" rel="canonical" href="https://logicbaron.github.io/docs/tasks/informationextraction/gliner"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/tasks/informationextraction/gliner" hreflang="en"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/tasks/informationextraction/gliner" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Logic Baron RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Logic Baron Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Logic Baron JSON Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Logic Baron" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.35121e78.css">
<script src="/assets/js/runtime~main.7864da48.js" defer="defer"></script>
<script src="/assets/js/main.ef38fddb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LogicBaron</b></a><a class="navbar__item navbar__link" href="/docs/community/hello">Hello, Baron</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Concept</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/concepts/math/introduction">Math</a></li><li><a class="dropdown__link" href="/docs/concepts/mlconcept/introduction">Machine Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/deeplearning/introduction">Deep Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/largemodel/introduction">Large Model</a></li><li><a class="dropdown__link" href="/docs/concepts/programming/introduction">Programming</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/data/image/introduction">Image</a></li><li><a class="dropdown__link" href="/docs/data/text/introduction">Text</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/models/mlmodel/pca">ML Models</a></li><li><a class="dropdown__link" href="/docs/models/aimodel/intro">AI Models</a></li><li><a class="dropdown__link" href="/docs/models/largemodel/introduction">Large Models</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Practice</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/practice/torch/intro">Torch</a></li><li><a class="dropdown__link" href="/docs/practice/efficienttrain/Efficient Train">Efficient Train</a></li><li><a class="dropdown__link" href="/docs/practice/mlops/intorduction">MLOPs</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Tasks</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/tasks/recommendation/hello">Recommendation</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li><a class="dropdown__link" href="/docs/tasks/retrieval/intro">Retrieval</a></li><li><a class="dropdown__link" href="/docs/tasks/knowledgegraph/knowledgegraph">Knowledge Graph</a></li><li><a class="dropdown__link" href="/docs/tasks/llm&amp;prompt/intro">LLM &amp; Prompt</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tasks/informationextraction/universalNER_2023">UniversalNER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tasks/informationextraction/universalner_2024">UniversalNER: 2024</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/tasks/informationextraction/gliner">GliNER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tasks/informationextraction/gollie">GoLLIE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tasks/informationextraction/gner">GNER</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/tasks/informationextraction/Attribute Extraction/hello">Attribute Extraction</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">GliNER</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>GliNER</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="generalist-model-for-named-entity-recognition-using-bidirecitonal-transformer">Generalist Model for Named Entity Recognition using Bidirecitonal Transformer<a href="#generalist-model-for-named-entity-recognition-using-bidirecitonal-transformer" class="hash-link" aria-label="Direct link to Generalist Model for Named Entity Recognition using Bidirecitonal Transformer" title="Direct link to Generalist Model for Named Entity Recognition using Bidirecitonal Transformer">​</a></h2>
<p>UniversalNER 에 대해 알아보는 시점이 되면 아마 open-world NER solution 에 대한 관심이 클 것 같습니다.</p>
<p>LLM 등장 이전의 Open-world NER은 주로 NER 을 학습시키고 zero-shot performance 를 기대하는 방식으로 발전해왔습니다. LLM 등장 이후에는 LLM 모델의 표현력에 기대어 propmt engineering이나 finetuning을 통해 <strong>LLM이 문제를 해결해주기를 &quot;기대&quot;</strong> 하는 방식이었습니다. LLM은 분명히 open-world NER을 해결하는 신기하고 멋진 솔루션이지만 한계가 있습니다.</p>
<p>가장 대표적인 한계는 <strong>LLM 모델은 일반적인 딥러닝 보다 훨씬 거대하고 그로 인해 훨씬 해석하기 힘들다</strong>는 점입니다. NER의 결과만 활용한다면 상관없지만 조금 더 정확하고 NER의 entity type, 또는 entity에 대한 해석과 표현이 필요한 경우에는 LLM이 적절한 해답이 되기 힘들기도 합니다. 당연히 할루시네이션과 같은 LLM의 고질점이 문제 역시   고려되어야 합니다.</p>
<p>재미있는 점 중 하나는 서비스 단계에서는 저런 해석보다 그냥 잘 나오는게 중요할 것 같지만 사실은 서비스단에서 오히려 저런 해석이 더 중요하다는 점입니다. 연구 단계에서는 신기한 현상은 그 자체로 연구할 가치가 있지만 서비스는 남을 설득해야하니까.. 정도로 받아들이고 있습니다.</p>
<p>GliNER은 <strong>일종의 entity type 과 entity 에 대한 representation learning</strong>을 수행합니다. 실제로 학습 프레임 워크 역시 metric learning, 특히 클립과 매우 유사한 구조를 가지고 있습니다. 사실 GliNER의 저자는 LLM에 비해 computational efficiency 가 크게 개선된다는 점을 강조하지만 저는 개인적으로 이 <strong>논문의 핵심은 (특히) entity 에 대한 representaiton learning의 &quot;느낌&quot;이 들어간 점</strong>이라고 생각합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="데이터">데이터<a href="#데이터" class="hash-link" aria-label="Direct link to 데이터" title="Direct link to 데이터">​</a></h2>
<p>representation learning과 비슷하게, GliNER의 핵심 역시 정해진 entity type 이 아니라 최대한 다양한 형태의 entity type을 사용하는 것입니다. 여기서 다양한 형태의 entity type이라는 것은 서로 다른 다양한 entity type이 아니라 같은 의미더라도 다른 표현형, 또는 유사한 의미나 미묘한 단어 등 <strong>최대한 자연어에 가까운 다양한 entity type 표현</strong>을 사용한다는 의미입니다.</p>
<p>기존 NER 접근 방법으로는 이런 데이터를 만들기 상당히 어렵습니다. 그런데 LLM 방식은 오히려 이러한 관점에서는 강점을 보입니다. <strong>Pile-NER</strong> 데이터셑은 chatGPT를 이용해서 open-world attribute extraction을 수행한 데이터셑입니다. 약 50,000개의 문서에 대해서 아래 Propmt를 이용해서 entity 정보를 추출합니다. UniversalNER:2024 에서 생성한 것과 같은 방식입니다. [참조 : <a href="https://huggingface.co/datasets/Universal-NER/Pile-NER-type" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/Universal-NER/Pile-NER-type</a>]</p>
<p>Validation 과정에서는 3가지 벤치마크를 활용합니다. 첫 번째는 7개의 CrossNER과 MIT dataset을 포함한 OOD NER BenchMark입니다. 두 번째는 트윗, 뉴스 등 다양한 도메인에서 수집한 20개의 NER dataset 입니다. 마지막으로 Multilingual NER dataset: MultiConel 을 활용합니다.</p>
<pre><code><p><b>System Message</b>: You are a helpful information extraction system.</p><p><b>Prompt</b>: Given a passage, your task is to extract all entities and identify
their entity types. The output should be in a list of tuples of the
following format:
[(&quot;entity 1&quot;, &quot;type of entity 1&quot;), ... ].</p><p><b>Passage</b>: {input_passage}</p></code></pre>
<p>최종적으로 부적절한 문서를 제외하고, GLINER은 <strong>44899개 문서, 240k의 entity span 과 13k 의 entity type</strong>을 가진 데이터셑을 사용합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="모델">모델<a href="#모델" class="hash-link" aria-label="Direct link to 모델" title="Direct link to 모델">​</a></h2>
<p>GliNER 모델의 구조는 sentence를 참조한 entity type의 임베딩과 entity span의 임베딩을 가깝게 만드는 구조를 가지고 있습니다. 전체적인 모델의 구조는 아래 그림과 같습니다.</p>
<div style="text-align:Center"><img src="/assets/images/gliner_model-ef339cc93d9436ffba65f5400b79ea30.png" style="border:solid"></div>
<p>각각의 entity type을 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, 그리고 모델의 input token을 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>라고 합시다. GliNER 모델은 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mtext>[ENT]</mtext><mtext>  </mtext><msub><mi>t</mi><mn>0</mn></msub><mtext>  </mtext><mo stretchy="false">[</mo><mtext>ENT]</mtext><mtext>  </mtext><msub><mi>t</mi><mn>1</mn></msub><mtext>  </mtext><mo>…</mo><mtext>  </mtext><mo stretchy="false">[</mo><mtext>ENT]</mtext><mtext>  </mtext><msub><mi>t</mi><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mtext>  </mtext><mo stretchy="false">[</mo><mtext>SEP</mtext><mo stretchy="false">]</mo><mtext>  </mtext><msub><mi>x</mi><mn>0</mn></msub><mtext>  </mtext><msub><mi>x</mi><mn>2</mn></msub><mtext>  </mtext><mo>…</mo><mtext>  </mtext><msub><mi>x</mi><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\text{[ENT]} \; t_0 \; [\text{ENT]} \; t_1 \; \ldots \; [\text{ENT]} \; t_{M-1}; \; [\text{SEP}] \; x_0 \; x_2 \; \ldots \; x_{N-1};]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord text"><span class="mord">[ENT]</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">[</span><span class="mord text"><span class="mord">ENT]</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mopen">[</span><span class="mord text"><span class="mord">ENT]</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mopen">[</span><span class="mord text"><span class="mord">SEP</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">]</span></span></span></span> 의 형태를 input으로 사용합니다. GliNER 모델의 기본 구조는 Bidirectinoal Transformer 입니다. [SEP] 로 분리되어 있는 entity type 과 sentence token  은 서로를 참조할 수 있습니다.</p>
<p>특이한 점은 GliNER 모델은 <strong>여러 개의 entity type을 동시에 input으로 사용한다는 점</strong>입니다. 논문에서는 속도 향상에서 큰 장점이 있었다고 말하고 있는데 성능 면에서는 크게 언급이 없습니다. Ablation study에서는 <strong>entity type의 개수를 바꾸어가며 학습시킨 결과 성능 향상</strong>이 있었음을 시사합니다. 다만 <strong>하나의 entity type 에 대해서만 학습 시키는 방법과 비교가 없어서</strong> 조금 아쉬웠습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-encoder">1. Encoder<a href="#1-encoder" class="hash-link" aria-label="Direct link to 1. Encoder" title="Direct link to 1. Encoder">​</a></h3>
<p>GliNER 논문은 (sentence를 참조하고 있는) entity type 임베딩과 상응하는 entity span 임베딩을 가깝게 하는 contrastive learning을 수행합니다. 모델링의 첫 단계는 <strong>entity type 임베딩</strong>과 <strong>entity span 임베딩</strong>을 구하는 것입니다.</p>
<p>[ENT] token 은 각각의 entity type앞에 붙어서 사용되며, 이 토큰의 임베딩이 <strong>entity type embedding</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bold{q}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 로 사용되게 됩니다. Bidirectional Transformer의 구조상 self-attention 모듈을 통해 각각의 entity type token들은 sentence token을 참조합니다. GliNER에서는 특이하게 word 단위 임베딩을 사용하는데 각 word의 첫 번째 subword token embedding을 word embedding으로 활용합니다.</p>
<p><strong>Span embeding</strong> 은 조금 더 복잡한 방법으로 구해집니다. 실제로 모델의 코드 구현을 보면 이 부분이 가장 난해합니다. semtemce span 은 연속된 token set 입니다. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∼</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i \sim j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span> 까지의 token set 이 span을 이룬다고 가정할 때, GliNER은 <strong>span embedding</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo>⊗</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S_{ij} = FFN(h_i \otimes h_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">FFN</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 로 정의합니다. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">⊗</span></span></span></span> 연산은 concatenation operation을 의미하며, GliNER 에서는 최대 12-length 까지의 span을 고려합니다.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>실제 모델 구현에서는 첫 번째 subword token embedding을 LSTM layer를 추가로 통과시킵니다.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-loss--train">2. Loss &amp; Train<a href="#2-loss--train" class="hash-link" aria-label="Direct link to 2. Loss &amp; Train" title="Direct link to 2. Loss &amp; Train">​</a></h3>
<p>contrastive learning 에서의 첫 단계는 거리 함수 또는 유사도 함수를 정의하는 것입니다. <strong>entity type embedding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bold{q}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>와 span embedding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">S_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>의 유사도</strong>는 벡터 내적 기반 점수를 사용합니다.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msubsup><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>T</mi></msubsup><msub><mi>q</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex"> \phi(i, j, t) = \sigma(S_{ij}^T q_t) \in \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2744em;vertical-align:-0.3831em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord mathbb">R</span></span></span></span></span>
<p>특이한 점은 최근 가장 많이 사용하는 cosine-similarity가 아닌 <strong>벡터 내적 값의 sigmoid funciton</strong>을 사용한다는 점입니다. 논문에서 이 부분에 대한 ablation은 언급하지 않는데, 논문에서 사용한 <strong>backbone pretrained model 이 Hyphersphere 상에서 학습된 임베딩 모델이 아니라서 그런 것 같습니다</strong>. 이 점수는 <strong>해 당 span 이 entity type 에 해당할 확률</strong>로도 이해할 수 있습니다.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>다시 한 번 요약하면, <strong>entity type embedding과 entity span embedding의 유사도 점수는 해당 entity span 이 entity type에 해당할 확률</strong>을 의미합니다.</p></div></div>
<p>학습에 사용되는 loss는 일반적인 metric learning loss의 형태입니다. entity type에 해당하는 entity span, 즉 positive pair 에 대해서는 similarity score가 커지도록 negative sample에 대해서는 similarity score가 작아지도록 학습합니다. 이 loss의 설계에서 두 가지 주의할 점이 있습니다.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>B</mi><mi>C</mi><mi>E</mi></mrow></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi><mo>×</mo><mi>T</mi></mrow></munder><msub><mi mathvariant="double-struck">I</mi><mrow><mi>s</mi><mo>∈</mo><mi>P</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi mathvariant="double-struck">I</mi><mrow><mi>s</mi><mo>∈</mo><mi>N</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{BCE} = - \sum_{s \in S \times T} \mathbb{I}_{s \in P} \log(\phi(s)) + \mathbb{I}_{s \in N} \log(1 - \phi(s))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">BCE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4027em;vertical-align:-1.3527em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8557em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3527em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathbb">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathbb">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">))</span></span></span></span></span>
<p>첫 번째는 infoNCE loss 형태와 다르게 loss에 들어가는 <strong>score 가 softmax를 취하지 않은 형태, 즉 확률 분포의 형태가 아니라는 점</strong>입니다. 이는 하나의 entity type에 대해서 정답이 굳이 하나가 아닐 수 있기 때문입니다.</p>
<p>두 번째는 <strong>negative sampling 방법</strong>입니다. GliNER 학습 프레임워크 상에서 input entity type 과 모든 span을 비교하기 때문에 당연히 negative sample이 학습에 포함됩니다. 그런데 GliNER 논문에서는 추가적으로 학습 배치에 포함되어 있지 않는 entity type을 샘플링해서 사용합니다. (해당 entity type이 사실은 input sentence안에서 나타나는 경우는 무시합니다.) negative sampling을 전혀 사용하지 않았을 경우에는 precision이 떨어졌고 negative samling을 너무 심하게 하면 recall이 떨어졌습니다. 최종적으로 논문에서는 50% 의 negative sample 을 사용합니다.</p>
<p>추가로 학습에서는 두 가지 Regularization Scheme이 사용됩니다. 첫 번째는 input entity type 이 순서 셔플링입니다. 두 번째는 랜덤하게 entity type을 drop해서 학습하는 방법입니다. 최종적으로는 학습 과정에서는 최대 25개까지의 entity type을 사용했다고 합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-decoder">3. Decoder<a href="#3-decoder" class="hash-link" aria-label="Direct link to 3. Decoder" title="Direct link to 3. Decoder">​</a></h3>
<p>GliNER 논문에서의 decoder는 matching score를 기반으로 entity type에 해당하는 entity span을 최종적으로 도출하는 모듈입니다. 예를 들어서 0~3 번째 span과 1~4 번째 span이 전부 0.5 점 이상 점수일 때 최종적인 정답 span을 어떻게 정할까, 의 문제입니다. 또한 특정한 span 이 두 개 이상의 entity type과 매칭될 수도 있습니다.</p>
<p>기본적으로 matching score &gt; 0.5 에 해당하는 span들을 고려해서 2가지 방법으로 디코딩을 수행합니다.</p>
<ul>
<li>
<p>Flat NER</p>
<ul>
<li>span 영역이 겹칠 경우 가장 점수가 높은 span을 선택합니다. 모든 영역을 고려할 때 까지 반복합니다.</li>
<li>entity type이 겹칠 경우 역시 마찬 가지로 가장 확률이 큰 entity type을 선택합니다.</li>
</ul>
</li>
<li>
<p>Nested NER</p>
<ul>
<li>두 개의 entity type이 할당된 span의 경우 어떤 entity type에 대해 fully nested 된 경우 두 개의 entity type을 할당합니다.</li>
<li>예를 들어서, &quot;바나나 우유&quot; 가 제품명이고, &quot;바나나&quot; 는 맛으로 디코딩 하는 것이 가능합니다.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="result">Result<a href="#result" class="hash-link" aria-label="Direct link to Result" title="Direct link to Result">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot">Zero-shot<a href="#zero-shot" class="hash-link" aria-label="Direct link to Zero-shot" title="Direct link to Zero-shot">​</a></h3>
<p>대부분이 영어로 이루어진 Pile-NER 데이터셑으로 학습한 GliNER 모델은 chatGPT 에 비해 multiConel dataset 의 대부분 언어에서 chatGPT 보다 좋은 성능을 보여주고 있습니다. (한국어는 ChatGPT 가 더 잘함) 그런데 잘한다고 zero-shot 성능을 그렇게 높은 지표를 보여주지 못합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="in-domain-supervised-learning">In-domain Supervised learning<a href="#in-domain-supervised-learning" class="hash-link" aria-label="Direct link to In-domain Supervised learning" title="Direct link to In-domain Supervised learning">​</a></h3>
<p>20 NER dataset 에 대해 각각 10,000개의 데이터를 랜덤하게 샘플링해서 supervised finetuing 결과를 InstructUIE와 비교합니다. pileNER 로 pretraining 시킨 모델과 pretrainign 시키지 않은 모델을 비교합니다. 사전 훈련된 모델의 경우 0.8 정도 사전 훈련이 되지 않은 모델보다 좋은 성능을 보였으며, InstructUIE 보다는 평균저그올 0.9 점 앞서는 성능을 보여줬습니다. 하지만 UniversalNER:2024에 비해서는 GliNER이 3점 정도 뒤쳐지는 성능을 보여줬다고 합니다. 그럼에도 불구하고 7/20개의 데이터셑에서 최고 점수를 보여줬습니다.</p>
<div style="text-align:Center"><img src="/assets/images/gliner_pre-e2d5ebd4969e402f383de573a024040f.png" style="border:solid"></div>
<p>pretraining의 성능은 finetuning dataset의 크기가 작을수록 크다는 점을 ablation study에서 밝힙니다.</p>
<h1>Conclusion</h1>
<p>GliNER 모델은 LLM 보다 훨씬 parameteric-efficient한 BERT 계열 모델을 활용해서 open-world NER 에서 finetuned-LLM과 비견되는 성능을 보여줬을 뿐 아니라,</p>
<p>무엇보다 적절한 entity span embedding 을 학습했다는 점에서 큰 의미를 가집니다.</p>
<h1>Ref</h1>
<ol>
<li><a href="https://arxiv.org/pdf/2311.08526.pdf" target="_blank" rel="noopener noreferrer">GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer</a></li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/tasks/informationextraction/GliNER.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/tasks/informationextraction/universalner_2024"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">UniversalNER: 2024</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/tasks/informationextraction/gollie"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">GoLLIE</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#generalist-model-for-named-entity-recognition-using-bidirecitonal-transformer" class="table-of-contents__link toc-highlight">Generalist Model for Named Entity Recognition using Bidirecitonal Transformer</a></li><li><a href="#데이터" class="table-of-contents__link toc-highlight">데이터</a></li><li><a href="#모델" class="table-of-contents__link toc-highlight">모델</a><ul><li><a href="#1-encoder" class="table-of-contents__link toc-highlight">1. Encoder</a></li><li><a href="#2-loss--train" class="table-of-contents__link toc-highlight">2. Loss &amp; Train</a></li><li><a href="#3-decoder" class="table-of-contents__link toc-highlight">3. Decoder</a></li></ul></li><li><a href="#result" class="table-of-contents__link toc-highlight">Result</a><ul><li><a href="#zero-shot" class="table-of-contents__link toc-highlight">Zero-shot</a></li><li><a href="#in-domain-supervised-learning" class="table-of-contents__link toc-highlight">In-domain Supervised learning</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/community/hello">Hello, Lapis</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.instagram.com/or7l_floll/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/jhpark9701/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://leetcode.com/superstone/" target="_blank" rel="noopener noreferrer" class="footer__link-item">leetcode<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>