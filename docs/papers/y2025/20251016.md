# 10-16 papers summary

## 1. Detect Anything via Next Point Prediction

기존 object detection 모듈은 크게 전통적인 detection 모델과 MLLM 모델로 나뉘어짐. detection 모델은 언어 이해 능력이 부족하고, MLLM 모델은 정밀한 위치 파악 능력이 부족함. MLLM 에서 사용하는 loss 는 좌표 예측에 불리하기도 함. (좌표값은 연속 값. regression 으로 학습하는 게 맞으나 loss 는 cross-entropy 임.)

**Rex-Omni** 모델은 MLLM 을 사용하여, 모든 시각 작업을 좌표 예측 문제로 해결함. 학습 난이도와 토큰 효율성을 개선하기 위해 0부터 999까지의 양자화된 좌표를 특수 토큰으로 표현하고, 고품질 데이터 엔진을 구축.

SFT 1단계 -> GRPO 2단계로 학습. GRPO 가 SFT 의 한계로 저자들이 지목한 기하학적 보상: IoU, Point-in-mask 등 - 을 추가함.

:::memo
MLLM 또는 VLM 계열에서 object detection 의 퀄리티를 더욱 짜내는 듯한 느낌의 논문. industry level , 특히 쇼핑 도메인에서는 소잡는칼이 아닌지 고민해볼만함. MLLM 을 활용한 object detection 자체는 의미있을 듯.

다만 데이터 구축 과정에서 사용된 grounding dino 를 활용하는게 단기간에는 더 효율적일듯?
:::

## 2. DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search `apple`

기존 MLLM 에서 VQA 해결할때의 두가지 문제점.
- 이미지 전체만을 검색 쿼리로써 사용함.
- RAG 검색 방식은 단일 웹 검색 결과를 사용하고 피드백이 없음.

두 가지 문제점을 각각 해결함.

- Grounding DINO 를 이용한 Cropped Image Search 사용.
- on-demand 방식의 multi-turn 웹 검색. (최대 6턴) -> 자기 반성 / 자기 교정 과정을 통해서 멀티 턴을 결정.

이를 통해 comparable with gpt-o3 성능 달성. SFT + GRPO 통해서 학습.

