
import blog_2025101802_img0 from './asset/blog_2025101802_img0.png';

# 10-18 papers summary - big tech

읽을 논문이 많아서 빅 테크 기업 논문은 따로 가져와봤다. 내가 투자한 회사는 무슨 연구하고 있나?

## 1. [LLM-GUIDED HIERARCHICAL RETRIEVAL](https://arxiv.org/pdf/2510.13217) `google`

[이 글](/docs/practice/retrieval/lattice.md) 참조.

## 2. [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/pdf/2510.13054) `nvidia`

기존 VLA 모델은 VLM 구조를 변경하거나, 액션 토큰을 추가하는 등 복잡한 방식을 사용함. 반면 로봇의 움직임을 텍스트 자체로 예측하는 가장 간단한 방법은 거의 연구되지 않았음.

VLA-0 는 VLM 구조를 그대로 사용함. 로봇의 행동을 정수로 이루어진 텍스트 시퀀스로 예측하도록 함. ex) 4 12 98 3 0 ...

VLA-0 는 놀라울정도로 성능이 좋으며, 복잡한 구조인 모델들을 이겼다고 함. 

## 3. [BitNet Distillation](https://arxiv.org/pdf/2510.13998) `microsoft`

2023년 공개된 [bitnet](https://arxiv.org/pdf/2310.11453) 논문은 1.58-bit quantization 이라는 충격적인 방법으로 엄청난 메모리 효율성을 달성함.

{-1, 0, 1} 로만 model parameter 를 표시하는 이 방식은 LLM 에 적용했을떄는 성능이 크게 저하됨. 이 문제는 거대 모델에서는 1.58-bit 모델은 학습 과정에서 활성화 값의 분산이 매우 커지는 현상에서 기인함.

1단계로 SubLN(Sub-Layer Norm) 모듈은 추가. SubLN 모듈은 LayerNorm 레이어를 MSHA 이후에 추가해서 이 현상을 효과적으로 제어함.

2단계로 distillation 은 먼저 소량 데이터로 모델을 1.58-bit 에 '적응' 시킴. 

3단계로 bitdistillation 모델은 teacher 모델의 attention 값과 logits 값을 distillation 함. 단순히 output 을 distillation 하는 것보다 효과가 좋았다고 한다.

결과적으로 teacher 모델 0.6B, 1.7B, 4B 크기 모델에 대해서 0.1 ~ 0.2% 수준의 차이만 보임.

## 4. [LaSeR: Reinforcement Learning with Last-Token Self-Rewarding](https://arxiv.org/pdf/2510.14943) `tencent`

LaSeR는 RLVR의 실제 보상값(정답이면 1, 오답이면 0)과, 모델의 마지막 토큰 확률을 기반으로 계산된 **'마지막 토큰 자기-보상 점수'**가 같아지도록 MSE 손실 함수를 통해 학습시킨다.

이를 통해서, LLM 이 스스로의 답변을 평가하는 능력을 가지게 된다는 것. 기존의 답변 검증을 위해 긴 초론 과정이 생략됨.

## 5. [PI-FLOW: POLICY-BASED FEW-STEP GENERATION VIA IMITATION DISTILLATION](https://arxiv.org/pdf/2510.14974) `adobe`

기존의 fast image generation 방법은 여러 단계의 teacher 모델을 세부 단계를 한번에 수행하는 student 모델을 학습하는 방식으로 이루어짐. 이런 방식은 훈련이 복잡해지고, 생성되는 이미지의 품질과 다양성 trade-off 가 발생함. 원본 모델의 품질과 다양성을 희생하지 않으면서도 빠른 모델을 만들고자 함.

이미지 생성이 느린 이유는, teacher model 의 경우 매 step 마다 무거운 네트워크를 실행해야 하기 때문이다. $\pi$-flow 모델은 대신, 한번에 blueprint 를 만들고, 이를 잘 수행하는지 여러 단계로 나누어 검사하는 방식을 사용함. 즉, teacher 모델이 각 단계에 어떤 식으로 수정을 가했는지를 정책의 형태로 배움.

여기서 주어지는 정책은 이미지를 정의하는 통계 파라미터들. 이미지를 생성하는 산술 연산의 반복을 통해 이미지를 만들어감.

