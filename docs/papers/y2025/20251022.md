import blog_20251022_img0 from './asset/blog_20251022_img0.png';
import blog_20251022_img1 from './asset/blog_20251022_img1.png';

# 10-22 papers summary

## 1. [MoGA](https://huggingface.co/papers/2510.18692) `bytedance`
### Mixture-of-Groups Attention for End-to-End Long Video Generation

## 2. [Grasp Any Region](https://arxiv.org/pdf/2510.18876) `bytedance`
### Towards Precise, Contextual Pixel Understanding for Multimodal LLMs

MLLM 논문의 전체 이미지 이해 능력 -> 복잡한 장면의 dense understanding + 미세한 pixel understanding 을 달성할 수 있도록 돞는 새로운 프레임 워크인 Grasp Any Region 을 제안.

전체 이미지 feature map 일단 확보해두고, Region base featrure 도 추출하는 방식.

## 3. [ProCLIP](https://arxiv.org/pdf/2510.18795)
### Progressive Vision-Language Alignment via LLM-based Embedder

CLIP text encdoer 를 BERT -> LLM-based embedder 로 대체하는 논문. 굳이 왜 대체하냐?

1. 임베딩의 표현력은 contrastive learning 방식으로 학습되었을 떄 더 높다.
  -> 임베딩 자체의 활용도가 높음.
2. 텍스트-이미지 임베딩 alignment 능력이 매우 유용함.

그러므로 저 두 능력을 유지되도록 lm-based embedder 튜닝하는 논문임.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251022_img0} style={{width: 500}} />
</div>

llm-based embedder 가 아니라 MLP 를 하나 붙여서 얘만 학습시킴.

1단계는 CLIP-text-encdoer 와 llm-based embedder 의 alignment 튜닝 작업.

2단계는 text-image alignment 가 맞도록 CLIP 방식으로 학습함.

## 4. [DSI-BENCH](https://arxiv.org/pdf/2510.18873)
### A BENCHMARK FOR DYNAMIC SPATIAL INTELLIGENCE

<div style={{textAlign: 'center'}}>
 <img src={blog_20251022_img1} style={{width: 500}} />
</div>


Observer 와 Object 가 동시에 움직이는 현실적인 3D 시나리오 데이터 벤치마크.

## 5. [VIDEO REASONING WITHOUT TRAINING](https://arxiv.org/pdf/2510.17045) `qualcomm`

퀄컴이 갑자기 왠 논문을. 

훈련 없이 Video reasoning 품질을 향상시키는 방법론. 한 가지 관찰에서 아이디어를 얻음.

모델의 출력 엔트로피