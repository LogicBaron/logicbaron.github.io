import blog_20251024_img0 from './asset/blog_20251024_img0.png';
import blog_20251024_img1 from './asset/blog_20251024_img1.png';

# 2025-10-24 papers review

## 1. [ARGenSeg](https://huggingface.co/papers/2510.20803)
### Image Segmentation with Autoregressive Image Generation Model

기존 MLLM 을 활용한 세그멘테이션은 모델 특성상 한계가 있었음. MLLM 이 픽셀을 답변하게 한다거나, 또는 전용 디코더를 다시 한번 붙이는 식.

해당 모델은 MLLM 에 전용 인코더를 붙이는 대신, MLLM 이 자체적으로 Segmentation 을 수행하도록 학습시켜서 꽤 좋은 성능을 보여줌.

모델의 핵심은 여러 step(scale) 에 거쳐서 진행한다는 것. 매 scale 마다 이전 scale 의 결과를 input 으로 사용함. 초기 scale 는 대략적인 물체의 경계선을 예측하고 후반에는 조금 더 미세한 경계선을 파악하는 단계.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251024_img1} style={{width: 500}} />
</div>

최종적인 모델 구조는 아래와 같음. 너무 오래 걸리지 않도록 매 스케일의 생성은 autoregressive 방식이 아닌 병렬 처리 방식을 사용했다고 함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251024_img0} style={{width: 500}} />
</div>


## 2. [Every Question Has Its Own Value](https://huggingface.co/papers/2510.20187) `tencent`
### Reinforcement Learning with Explicit Human Values

이 [논문](/docs/papers/y2025/oct/20251007#judging-with-confidence-google)과 합치는 것만으로도 연구가 될듯. Juding with Confidence 연구가 답변의 분포에 집중했다면, 이 논문은 질문의 가치에 집중한다.

모든 질문이 똑같은 보상을 받는 것은 불공평하다. `1+1=?` 과 `2x  를 미분하면?` 이라는 질문의 가치는 명확히 다르니까.

질문 가치는 시험 데이터셑에서 질문의 원본 점수를 활용했다고 함.

## 3. [Open-o3 Video](https://huggingface.co/papers/2510.20579) `bytedance`
### Grounded Video Reasoning with Explicit Spatio-Temporal Evidence

+ 비디오 기반 reasoning 에서 타임스탬프+바운딩박스가 통합된 evidence 를 생성함으로써, 정밀한 시공간적 정확도가 보장되는 추론 능력을 학습함.
+ 이 작업이 기존에는 다양한 도구를 활용해야했는데, 단일 모델 프레임워크로 달성함.