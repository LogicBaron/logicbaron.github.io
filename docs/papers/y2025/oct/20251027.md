import blog_20251027_img0 from './asset/blog_20251027_img0.png';
import blog_20251027_img1 from './asset/blog_20251027_img1.png';

# 2025-10-27 papers review

## 1. [A Definition of AGI](https://huggingface.co/papers/2510.18212)

링크

## 1. [Reasoning with Sampling](https://huggingface.co/papers/2510.14901) `harvard`
### Your Base Model is Smarter Than You Think

링크

## 3. [SCALING LAWS MEET MODEL ARCHITECTURE](https://huggingface.co/papers/2510.18245) `amazon`
### TOWARD INFERENCE-EFFICIENT LLMS

train 이 아닌 inference 관점에서 영향을 미치는 요인 비교. zero 의 분석과 상당히 비슷한 전개 방식. (이건 복붙함.)

| 구분 | 요인 (Factor) | 수식적 의미 | 추론 효율성 영향 및 원리 |
|:---:|:---:|:---:|:---|
| **아키텍처** | **은닉 크기** ($d_{model}$) | **총 추론 FLOPs**($\text{Total-FLOPs}$)에 기여하는 항($2n_{layers} T d_q$)의 감소를 통해 총 연산량을 줄이는 효과. | **↑ 증가 (Higher Throughput)**. **원리**: 1. **KV 캐시($\text{KV Cache}$) 축소**: 고정된 파라미터($N_{non\text{-}embed}$) 하에서 $d_{model}$ 증가는 $n_{head}$를 줄여 KV 캐시 크기를 감소시켜 I/O 비용을 낮춤. 2. **총 FLOPs 감소**: $2n_{layers} T d_q$ 항을 줄여 추론 연산량을 낮춤. |
| **아키텍처** | **MLP-대-어텐션 비율** ($r_{mlp/attn}$)  | **총 추론 FLOPs**($\text{Total-FLOPs}$)에 기여하는 항($2n_{layers} T d_q$)의 감소를 통해 총 연산량을 줄이는 효과. | **↑ 증가 (Higher Throughput)**. **원리**: 1. **KV 캐시 축소**: $r_{mlp/attn}$ 증가는 KV 캐시 크기를 축소하여 I/O 비용을 낮춤. 2. **총 FLOPs 감소**: $2n_{layers} T d_q$ 항을 줄여 추론 연산량을 낮춤. |
| **아키텍처** | **GQA** (Grouped-Query Attention)  | **Key/Value 헤드 수($K$)**를 줄여 $d_{kv}$를 감소시키며, 이는 KV 캐시 메모리 사용량에 직접적으로 영향. | **↑ 증가 (Higher Throughput)**. **원리**: KV 캐시 크기를 직접적으로 감소시켜 추론 중 **I/O 비용**을 낮추고 처리량을 향상시킴. |
| **수식** | **총 추론 비-임베딩 FLOPs** ($\text{Total-FLOPs}$) | $\text{Total-FLOPs} \approx 2P_{non\text{-}embed} + 2n_{layers} T d_q$. 여기서, $P_{non\text{-}embed}$는 비-임베딩 파라미터 수이며, **$2n_{layers} T d_q$ 항**이 **추론 효율성**에 가장 큰 영향을 미치는 항 중 하나임. | **↓ 감소 (Less FLOPs) $\rightarrow$ ↑ 효율성 증가**. **의미**: 추론 시퀀스 길이($T$)와 쿼리 차원($d_q$)에 비례하는 이 항을 줄이는 것이 효율성 개선의 핵심. |
| **정확도** | **은닉 크기 및 MLP 비율** | $L(x) = c_0 + c_1 \log x + c_2 / x$ (단일 요인별 손실 모델링). | **U자형 관계**: $d_{model}/\sqrt{N}$ 및 $r_{mlp/attn}$은 훈련 손실($L$)과 U자형 곡선 관계를 보여 **최적의 중간 지점**이 존재함. **원리**: 추론 효율성만을 위해 요인을 극단적으로 키우면(예: $d_{model}$을 너무 키우면 $n_{head}$가 줄어듦) 정확도가 하락함. |

## 2. [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/pdf/2510.21553)

링크

## [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://huggingface.co/papers/2510.11370) `shaomi`

MOE 적용된 모델 RL 시 라우팅 동작으로 인해 훈련 불안정이 발생하는 경향 확인. 이는 train - inference 간 라우팅 동작 불일치가 주된 원인이라고 함. 이로 인해 catastrophic forgetting 과 같은 퍼포먼스 이슈 발생.

**Rollout Routing Replay(R3)** 방법 제안. 훈련 단계에서 rollout 단계에서 사용한 라우팅 마스크를 그대로 사용하는 전략.


##  1. [Video-As-Prompt](https://huggingface.co/papers/2510.20888) `bytedance`  
### Unified Semantic Control for Video Generation

비디오 자체를 비디오 생성의 prompt, reference 로 사용하는 방법 제안. 일종의 video in-context generation.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251027_img0} style={{width: 500}} />
</div>

생성 방식은 비디오 생성 DiT 와 별개로, prompt video 를 처리하는 transformer: expert - 를 별도로 사용한다.

expert 와 DiT 는 서로 일부 attention 을 완전히 공유하며, 최종적으로 main DiT 의 생성 결과를 사용하게 된다.

이 구조는 비디오의 스타일을 적용한 결과물을 생성하도록 학습하는데, 이 과정에서 expert 만 trainable 로 설정.

## 2. [Search Self-play](https://huggingface.co/papers/2510.18821)
### Pushing the Frontier of Agent Capability without Supervision

Agent 훈련을 위한 RLVR (Reinforcement Learning with Verifiable Rewards) 을 검증된 정답셋 없이 LLM 의 self-play 를 통해 달성하려함. 정확히는 **정답은 있지만 질의는 없는 경우** 를 위한 것.

LLM proposer(질의자), solver(풀이자) 를 놔둠.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251027_img1} style={{width: 500}} />
</div>

- LLM proposer: 정답에 맞는 질의를 multi-turn search tool call 을 통해서 생성 -> generated question
- LLM solver : 두 가지 역할을 수행함.
  - 1. 먼저 LLM proposer 가 사용한 RAG 결과를 사용하면 generated question -> ground-truth answer 추론이 가능한지 확인.
  - 2. 1 통과되면, 자기가 신규로 search tool 을 사용해서 generated question 으로부터 정답 추론. 이 정답을 이용해서 모델 학습시켜 나감.

LLM proposer 는 점점 어렵게 질문을 만드려 하고, LLM solver 는 더 정답을 잘 맞추려고 함. zero-sum game 을 하면서 모델이 발전해나감. 

논문은 오픈 데이터셑에서 쿼리를 제거한 데이터로 실험함. 큰 모델에서는 실험을 못해본 것 같은데 작은 모델 기준으로는 성능 향상을 확인함.

## 3. [Foley Control](https://huggingface.co/papers/2510.21581) `stability ai`
### Aligning a Frozen Latent Text-to-Audio Model to Video

end-to-end 시스템을 거부한다. Video-to-Audio 모델은 end-to-end 로 학습하려면 대규모의 데이터셑, 막대한 계산 비용이 필요함.

Audio DiT 모델과 Video Encoder 를 Frozen 상태로 두고 가벼운 video cross-attention 브릿지만을 학습시킴.

동기화 성능 저하는 없었다. stability AI 다운 노하우가 많이 들어있을 것 같음.

## 4. [Soft Instruction De-escalation Defense](https://huggingface.co/papers/2510.21057) `google`

prompt injection 방식 공격에 대한 [논문](/docs/papers/y2025/oct/20251021#1-distractor-injection-attacks-on-large-reasoning-models-amazon)을 몇개 소개했는데, 같은 계열이다.

tool-agent LLM 들이 연구되면서 prompt injection 역시 다른 방식으로 발전 중.

논문에서 상정하는 공격 시나리오는 외부 데이터 소스를 활용한 indirect prompt injection. 웹 페이지, 사용자가 업로드한 파일, API 응답 등 모든 외부 데이터 소스를 통해서 특정 도구 호출을 유도함.

논문의 핵심은 prompt 내의 사용자 쿼리를 제외한 모든 외부 데이터 소스에 **명령어를 전부 비명령어조**로 바꾸는 거임. instruction -> soft instruction.

이것만으로도 매우 높은 방어 효율을 달성했다고 한다.

