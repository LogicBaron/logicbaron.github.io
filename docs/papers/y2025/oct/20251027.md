import blog_20251027_img0 from './asset/blog_20251027_img0.png';
import blog_20251027_img1 from './asset/blog_20251027_img1.png';

# 2025-10-26 papers review

## 1. [A Definition of AGI](https://huggingface.co/papers/2510.18212)


링크

## 1. [Reasoning with Sampling](https://huggingface.co/papers/2510.14901) `harvard`
### Your Base Model is Smarter Than You Think

링크

## 3. [SCALING LAWS MEET MODEL ARCHITECTURE](https://huggingface.co/papers/2510.18245) `amazon`
### TOWARD INFERENCE-EFFICIENT LLMS

## 2. [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/pdf/2510.21553)

링크

## [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://huggingface.co/papers/2510.11370) `shaomi`

MOE 적용된 모델 RL 시 라우팅 동작으로 인해 훈련 불안정이 발생하는 경향 확인. 이는 train - inference 간 라우팅 동작 불일치가 주된 원인이라고 함. 이로 인해 catastrophic forgetting 과 같은 퍼포먼스 이슈 발생.

**Rollout Routing Replay(R3)** 방법 제안. 훈련 단계에서 rollout 단계에서 사용한 라우팅 마스크를 그대로 사용하는 전략.




##  1. [Video-As-Prompt](https://huggingface.co/papers/2510.20888) `bytedance`  
### Unified Semantic Control for Video Generation

비디오 자체를 비디오 생성의 prompt, reference 로 사용하는 방법 제안. 일종의 video in-context generation.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251027_img0} style={{width: 500}} />
</div>

생성 방식은 비디오 생성 DiT 와 별개로, prompt video 를 처리하는 transformer: expert - 를 별도로 사용한다.

expert 와 DiT 는 서로 일부 attention 을 완전히 공유하며, 최종적으로 main DiT 의 생성 결과를 사용하게 된다.

이 구조는 비디오의 스타일을 적용한 결과물을 생성하도록 학습하는데, 이 과정에서 expert 만 trainable 로 설정.

## 2. [Search Self-play](https://huggingface.co/papers/2510.18821)
### Pushing the Frontier of Agent Capability without Supervision

Agent 훈련을 위한 RLVR (Reinforcement Learning with Verifiable Rewards) 을 검증된 정답셋 없이 LLM 의 self-play 를 통해 달성하려함. 정확히는 **정답은 있지만 질의는 없는 경우** 를 위한 것.

LLM proposer(질의자), solver(풀이자) 를 놔둠.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251027_img1} style={{width: 500}} />
</div>

- LLM proposer: 정답에 맞는 질의를 multi-turn search tool call 을 통해서 생성 -> generated question
- LLM solver : 두 가지 역할을 수행함.
  - 1. 먼저 LLM proposer 가 사용한 RAG 결과를 사용하면 generated question -> ground-truth answer 추론이 가능한지 확인.
  - 2. 1 통과되면, 자기가 신규로 search tool 을 사용해서 generated question 으로부터 정답 추론. 이 정답을 이용해서 모델 학습시켜 나감.

LLM proposer 는 점점 어렵게 질문을 만드려 하고, LLM solver 는 더 정답을 잘 맞추려고 함. zero-sum game 을 하면서 모델이 발전해나감. 

논문은 오픈 데이터셑에서 쿼리를 제거한 데이터로 실험함. 큰 모델에서는 실험을 못해본 것 같은데 작은 모델 기준으로는 성능 향상을 확인함.

## 3. [Foley Control](https://huggingface.co/papers/2510.21581) `stability ai`
### Aligning a Frozen Latent Text-to-Audio Model to Video

end-to-end 시스템을 거부한다. Video-to-Audio 모델은 end-to-end 로 학습하려면 대규모의 데이터셑, 막대한 계산 비용이 필요함.

Audio DiT 모델과 Video Encoder 를 Frozen 상태로 두고 가벼운 video cross-attention 브릿지만을 학습시킴.

동기화 성능 저하는 없었다. stability AI 다운 노하우가 많이 들어있을 것 같음.

## 4. [Soft Instruction De-escalation Defense](https://huggingface.co/papers/2510.21057) `google`

prompt injection 방식 공격에 대한 [논문](/docs/papers/y2025/oct/20251021#1-distractor-injection-attacks-on-large-reasoning-models-amazon)을 몇개 소개했는데, 같은 계열이다.

tool-agent LLM 들이 연구되면서 prompt injection 역시 다른 방식으로 발전 중.

논문에서 상정하는 공격 시나리오는 외부 데이터 소스를 활용한 indirect prompt injection. 웹 페이지, 사용자가 업로드한 파일, API 응답 등 모든 외부 데이터 소스를 통해서 특정 도구 호출을 유도함.

논문의 핵심은 prompt 내의 사용자 쿼리를 제외한 모든 외부 데이터 소스에 **명령어를 전부 비명령어조**로 바꾸는 거임. instruction -> soft instruction.

이것만으로도 매우 높은 방어 효율을 달성했다고 한다.

