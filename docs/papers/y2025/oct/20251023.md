import blog_20251023_img0 from './asset/blog_20251023_img0.png';
import blog_20251023_img2 from './asset/blog_20251023_img2.png';
import blog_20251023_img3 from './asset/blog_20251023_img3.png';

# 2025-10-23 papers review

## 1. [TheMCPCompany](https://huggingface.co/papers/2510.19286)
### Creating General-purpose Agents with Task-specific Tools

많은 MCP 서버가 등장하고 있지만 많은 MCP 서버를 효율적으로 사용하는 연구는 별로 안됐음. MCP 활용 능력을 평가하기 위한 벤치마크도 부실함. 그리고 기존 MCP 응용 체계는 매우 세부적인 목적 MCP 활용 능력이 떨어짐.

논문에서는 일단,

MCP 평가 벤치마크 데이터셑을 구축하고,
대규모 Toolset 을 구축한뒤,
대규모 Toolset 에서 MCP 의 활용을 결정하는 MCP server 를 한 단계더 게이트웨이로 둠.

MCP server - MCP Agent - MCP client 구조.

논문에서는 MCP 활용을 더 잘해서 비용 절감 효과를 강조함.


## 2. [PROFBENCH](https://arxiv.org/pdf/2510.18941) `nvidia`
### MULTI-DOMAIN RUBRICS REQUIRING PROFESSIONAL KNOWLEDGE TO ANSWER AND JUDGE

기존 LLM benchmark 들은 주로 수학, 프로그래밍, 단답형 답변 등 검증이 용이한 분야에 국한됨.


박사 수준 물리학, 화학, 경제, 컨설팅 4개 전문 분야에서 전문가가 직접 작성한 benchmark 를 nvidia 에서 만듬. top 모델들에게도 상당히 challenging 함을 확인함.

## 3. [Pico-Banana-400K](https://arxiv.org/pdf/2510.19808) `apple`
### A Large-Scale Dataset for Text-Guided Image Editing

대규모, 고품질, 공개 엑세스가 가능한 실제 이미지 기반 text-guided image editing 데이터셑의 부재가 용구 동기.

nano-babnana 를 사용하여 OpenImages 의 실제 사진에서 편집쌍을 생성하고, Gemini-2.5-Pro 로 품질을 체계적으로 평가 및 선별하여 Pico-Banana-400K 구축함.

대규모의 포괄적인 이미지 편집 유형에 대한 데이터를 제공함.

## 4. [Unified Reinforcement and Imitation Learning for Vision-Language Models](https://arxiv.org/pdf/2510.19307) `nvidia`

VLM 은 크기가 너무 커서 휴대용 장비에 배포하기 어려웠다. Reinforcement + Imitation Learning 방식을 활용해서 이를 해결하고자 하는 논문.

결과적으로 7B, 8B 모델로 다양한 벤치마크에서 최신 대형 VLM 보다 좋은 성능을 보였으며, 일부 비공개 모델과도 경쟁할만한 성능을 보여줌. 최소 2%.

사용한 Reinforcement and Imitation Learning(RIL) 방식은 teacher model 의 탐색 능력(policy-action)을 모방하면서 정답 스타일까지 모방하는 것을 목표로 함.

student model, teacher model 을 구분하는 **discriminator** 그리고 student model 의 정답을 평가하는 **LLM-as-a-Judge** 를 사용.

discriminator 의 prompt 구성.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251023_img0} style={{width: 500}} />
</div>

discriminator 는 teacher-student 정답을 더 잘 구분하도록 꾸준히 업데이트 됨.

student 모델이 사용하는 보상은 두 가지임. teacher 모델과 얼마나 유사한가, 그리고 사실적 정확도와 얼마나 유사한가. teacher 모델과 유사해지도록만 학습하면 스타일만 유사해지고 내용은 이상해진다고 함.

$$
R(q, o_i) = \underbrace{R_{Sim}(q, o_i)}_{\text{Similarity Reward}} + \underbrace{R_{Ans}(q, o_i)}_{\text{Answer Reward}}
$$

보상이 계산되면 이를 이용해서 아래 loss 로 policy 를 업데이트함.

$$
\max_{\theta}\mathcal{L}(\theta) = \frac{1}{2G}\sum_{i=1}^{2G} \left[ \min \left\{ r_i(\theta)\hat{A}_i, \text{clip}(r_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i \right\} - \beta\mathcal{D}_{KL}(\pi_{\theta}|\pi_{ref}) \right] \text{ [cite: 617]}
$$

$\hat{A}_i = R(q, o_i) - \frac{1}{2G}\sum_{j=1}^{2G} R(q, o_j)$ 은 총보상. 그냥 두개의 reward 를 더한것. 그러므로 loss 첫 항은 clipping reward. 두 번째 항은 모델이 너무 급격하게 안정적인 정책을 벗어나지 않도록 제어하는 역할을 함. 이 안정적인 정책은 초반에 기본적인 Instruction Following Capability 를 SFT 로 학습시켜둔 모델임.


## 5. [LOONGRL](https://arxiv.org/pdf/2510.19363) `microsoft`
### REINFORCEMENT LEARNING FOR ADVANCED REASONING OVER LONG CONTEXTS

RL 을 이용한 LLM 추론 학습은 단문 맥락에 집중되어 있으며 장문 맥락에서 학습에 사용할만한 데이터가 없다. 만들기도 어렵고.

LoongRL 은 keychain 기반으로 새롭게 장문 맥락 추론 데이터를 합성하고, 이를 이용해서 모델 성능을 크게 향상시켰다고 함. 특히 multi-hop QA 정확도를 20%나 상승시킴.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251023_img2} style={{width: 500}} />
</div>

keychain 방식.

1. \{문맥, 질문, 답변\} 으로 구성된 원본 QA 데이터셑에서 적절한 난이도의 QA쌍을 선택.
2. 원본 문맥에 다른 QA 쌍에서 추출된 관련 없는 문서(distracting documents) 를 삽입하여 약 16K 토큰의 장문 문맥을 만듬. 이 데이터는 방대한 데이터에 gold document 가 숨어있는 실제 장문 환경을 모사함.
3. key-value 쌍이 문맥내에 무작위로 삽입됨. a->b->c.. 순서로 키를 추적하면서 문서를 확인함.
4. 모델에게는 시작키를 알려주고, 키-값 chain 을 잘 따라가서 올바른 질문을 찾은 뒤에 답변하라고 함.

실제 prompt 의 예시임.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251023_img3} style={{width: 500}} />
</div>

키-체인 구조를 사용하지 않았을떄는 성능 향상이 제한적이었다고 함. 즉, 키-체인 구조를 사용함으로써 모델이 계획을짜고, 검색을 하고, 이유를 추론하고 재확인까지하는 사이클을 보다 효율적으로 수행했다고 말함. 반면 키-체인 학습을 안한 모델은 reasoning 과 retrieval 을 동시에 하며 distracting document 의 영향을 강하게 받음. 

loongRL 검색할 대상이 명확한, 즉 실제 질문을 복구한 상황에서만 문맥을 읽었다고 함. 즉 모델이 자발적으로 multi-hop QA 를 해결하기위한 프로세스를 정립한 것.