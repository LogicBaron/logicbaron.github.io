<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-papers/y2025/oct/20251021" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">10-21 papers summary | Logic Baron</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" name="twitter:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" property="og:url" content="https://logicbaron.github.io/docs/papers/y2025/oct/20251021"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="10-21 papers summary | Logic Baron"><meta data-rh="true" name="description" content="1. DISTRACTOR INJECTION ATTACKS ON LARGE REASONING MODELS amazon"><meta data-rh="true" property="og:description" content="1. DISTRACTOR INJECTION ATTACKS ON LARGE REASONING MODELS amazon"><link data-rh="true" rel="icon" href="/img/logicbaron_32.ico"><link data-rh="true" rel="canonical" href="https://logicbaron.github.io/docs/papers/y2025/oct/20251021"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/papers/y2025/oct/20251021" hreflang="en"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/papers/y2025/oct/20251021" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"10-21 papers summary","item":"https://logicbaron.github.io/docs/papers/y2025/oct/20251021"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Logic Baron RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Logic Baron Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Logic Baron JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Logic Baron" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.91224ace.css">
<script src="/assets/js/runtime~main.c5520c35.js" defer="defer"></script>
<script src="/assets/js/main.85463c8d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LogicBaron</b></a><a class="navbar__item navbar__link" href="/docs/community/hello">Hello, Baron</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">AIPapers</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/papers/bigtech/intor">monthly bigtech</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/papers/y2025/oct/bigtech">daily 2025</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Concept</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/concepts/math/introduction">Math</a></li><li><a class="dropdown__link" href="/docs/concepts/programming/introduction">Programming</a></li><li><a class="dropdown__link" href="/docs/concepts/torch/intro">Torch</a></li><li><a class="dropdown__link" href="/docs/concepts/mlconcept/introduction">Machine Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/deeplearning/introduction">Deep Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/largemodel/introduction">Large Model</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/data/image/introduction">Image</a></li><li><a class="dropdown__link" href="/docs/data/text/introduction">Text</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/models/mlmodel/pca">ML Models</a></li><li><a class="dropdown__link" href="/docs/models/aimodel/intro">AI Models</a></li><li><a class="dropdown__link" href="/docs/models/largemodel/introduction">Large Models</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Tasks</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/tasks/llm&amp;prompt/intro">LLM Engineering</a></li><li><a class="dropdown__link" href="/docs/tasks/efficienttrain/Efficient Train">EfficientTrain</a></li><li><a class="dropdown__link" href="/docs/tasks/mlops/intorduction">Mlops</a></li><li><a class="dropdown__link" href="/docs/tasks/recommendation/hello">Recommendation</a></li><li><a class="dropdown__link" href="/docs/tasks/retrieval/lattice">Retrieval</a></li><li><a class="dropdown__link" href="/docs/tasks/knowledgegraph/knowledgegraph">Knowledge Graph</a></li><li><a class="dropdown__link" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li><a class="dropdown__link" href="/docs/tasks/etc/visual_odometry">etc.</a></li></ul></div></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/papers/y2025/oct/bigtech"><span title="Y2025" class="categoryLinkLabel_W154">Y2025</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/papers/y2025/oct/bigtech"><span title="oct" class="categoryLinkLabel_W154">oct</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/bigtech"><span title="10-18 papers summary - big tech" class="linkLabel_WmDU">10-18 papers summary - big tech</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20250918"><span title="09-18 Papers Summary" class="linkLabel_WmDU">09-18 Papers Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20250922"><span title="09-23 Papers Summary" class="linkLabel_WmDU">09-23 Papers Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251003"><span title="10-23 Papers Summary" class="linkLabel_WmDU">10-23 Papers Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251004"><span title="10-04 papers summary" class="linkLabel_WmDU">10-04 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251005"><span title="10-05 papers summary" class="linkLabel_WmDU">10-05 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251007"><span title="10-07 papers summary" class="linkLabel_WmDU">10-07 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251015"><span title="10-15 papers summary" class="linkLabel_WmDU">10-15 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251016"><span title="10-16 papers summary" class="linkLabel_WmDU">10-16 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251017"><span title="10-17 papers summary" class="linkLabel_WmDU">10-17 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251018"><span title="10-18 papers summary" class="linkLabel_WmDU">10-18 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/papers/y2025/oct/20251021"><span title="10-21 papers summary" class="linkLabel_WmDU">10-21 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251022"><span title="10-22 papers summary" class="linkLabel_WmDU">10-22 papers summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251023"><span title="2025-10-23 papers review" class="linkLabel_WmDU">2025-10-23 papers review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251024"><span title="2025-10-24 papers review" class="linkLabel_WmDU">2025-10-24 papers review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251026"><span title="2025-10-26 papers review" class="linkLabel_WmDU">2025-10-26 papers review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/papers/y2025/oct/20251027"><span title="2025-10-27 papers review" class="linkLabel_WmDU">2025-10-27 papers review</span></a></li></ul></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Y2025</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">oct</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">10-21 papers summary</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>10-21 papers summary</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-distractor-injection-attacks-on-large-reasoning-models-amazon">1. <a href="https://arxiv.org/pdf/2510.16259" target="_blank" rel="noopener noreferrer" class="">DISTRACTOR INJECTION ATTACKS ON LARGE REASONING MODELS</a> <code>amazon</code><a href="#1-distractor-injection-attacks-on-large-reasoning-models-amazon" class="hash-link" aria-label="Direct link to 1-distractor-injection-attacks-on-large-reasoning-models-amazon" title="Direct link to 1-distractor-injection-attacks-on-large-reasoning-models-amazon" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="characterization-and-defense">CHARACTERIZATION AND DEFENSE<a href="#characterization-and-defense" class="hash-link" aria-label="Direct link to CHARACTERIZATION AND DEFENSE" title="Direct link to CHARACTERIZATION AND DEFENSE" translate="no">​</a></h3>
<p>LRM 모델에서 프롬프트 주입이 아니라 추론 과정 자체를 악의적으로 공격하는 방식에 SOTA 모델들이 매우 취약하다고 주장하고, 방어전략을 제안함.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-enterprise-deep-research-salesforce">2. <a href="https://arxiv.org/pdf/2510.17797" target="_blank" rel="noopener noreferrer" class="">Enterprise Deep Research</a> <code>salesforce</code><a href="#2-enterprise-deep-research-salesforce" class="hash-link" aria-label="Direct link to 2-enterprise-deep-research-salesforce" title="Direct link to 2-enterprise-deep-research-salesforce" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="steerable-multiagent-deep-research-for-enterprise-analytics">Steerable MultiAgent Deep Research for Enterprise Analytics<a href="#steerable-multiagent-deep-research-for-enterprise-analytics" class="hash-link" aria-label="Direct link to Steerable MultiAgent Deep Research for Enterprise Analytics" title="Direct link to Steerable MultiAgent Deep Research for Enterprise Analytics" translate="no">​</a></h3>
<p>industry level 은 비정형 데이터를 정형화되고, 기업에서 사용할 수 있는 형태의 정보로 변환해야하지만 기존의 자율 에이전트 도메인 특유의 뉘앙스, 의도 정렬, 기업 통합에서 어려움을 겪는다. 기존 심층 연구 시스템은 사용자가 중간 추론 과정에 개입할 수 없어 연구 방향을 steering 할 수 없었고, 이로 인해 비효율적이거나 사용자 의도에 맞지 않는 결과가 생성되었다.</p>
<p>기업은 특히 투명한 증거 근거(Evidence) 가 필요하다.</p>
<p>EDR 은 투명하고 조종가능한 multi-agent-system 을 제안함. Enterprise Deep research Framework.</p>
<div style="text-align:center"><img src="/assets/images/blog_20251021_img0-ca4fc4ac621b98a47464c0a0c075304f.png" style="width:500px"></div>
<p>유저 쿼리로부터 연구에 필요한 과정에 전문화된 에이전트를 도입한다. 사용자는 이 과정에서 자연어로 연구 지침을 master research agent 에 전달 가능하다.</p>
<p>세일즈 포스는 일반적인 문제 해결에서 한 단계 더 나가고 있는듯.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-ultracua-apple">3. <a href="https://arxiv.org/pdf/2510.17790" target="_blank" rel="noopener noreferrer" class="">UltraCUA</a> <code>apple</code><a href="#3-ultracua-apple" class="hash-link" aria-label="Direct link to 3-ultracua-apple" title="Direct link to 3-ultracua-apple" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="a-foundation-model-for-computer-use-agents-with-hybrid-action">A Foundation Model for Computer Use Agents with Hybrid Action<a href="#a-foundation-model-for-computer-use-agents-with-hybrid-action" class="hash-link" aria-label="Direct link to A Foundation Model for Computer Use Agents with Hybrid Action" title="Direct link to A Foundation Model for Computer Use Agents with Hybrid Action" translate="no">​</a></h3>
<p>Computer-use Agent 방식은 컴퓨터의 기본적 동작에 의존했다. 다른 에이전트들은 API 또는 MCP 를 적극적으로 활용한다.</p>
<p>UltraCUA 는 Computer-use Agent 에 API 와 MCP 같은 고수준 프로그래밍 도구를 적극적으로 활용하는 방식을 제안함.</p>
<div style="text-align:center"><img src="/assets/images/blog_20251021_img1-a5d96016ace4454ba17a6a1bfdf18ede.png" style="width:500px"></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-embody-3d-meta">4. <a href="https://arxiv.org/pdf/2510.16258" target="_blank" rel="noopener noreferrer" class="">Embody 3D</a> <code>meta</code><a href="#4-embody-3d-meta" class="hash-link" aria-label="Direct link to 4-embody-3d-meta" title="Direct link to 4-embody-3d-meta" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="a-large-scale-multimodal-motion-and-behavior-dataset">A Large-scale Multimodal Motion and Behavior Dataset<a href="#a-large-scale-multimodal-motion-and-behavior-dataset" class="hash-link" aria-label="Direct link to A Large-scale Multimodal Motion and Behavior Dataset" title="Direct link to A Large-scale Multimodal Motion and Behavior Dataset" translate="no">​</a></h3>
<p>고품질, 대규모 3D 모션 데이터셑. 439명 참가자로부터 총 500 시간 분량 데이터를 수집하여 역대 최대 규모 3D 모션 데이터셑 구축함.</p>
<p>손, 얼굴 표정, 신체 형태를 포함하여 전신을 추적하는 3D 모션 데이터.
참가자별로 분리된 오디오 트랙과 상세한 텍스트 주석이 포함된 오디오 데이터.</p>
<p>다양한 일상 활동, 협업 ,감정적 대화 등 복잡한 행동 데이터 포함됨. 범용적인 인간 행동을 최대한 많이 포함.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="5-dler-doing-length-penalty-right-nvidia">5. <a href="https://arxiv.org/pdf/2510.15110" target="_blank" rel="noopener noreferrer" class="">DLER: Doing Length pEnalty Right</a> <code>nvidia</code><a href="#5-dler-doing-length-penalty-right-nvidia" class="hash-link" aria-label="Direct link to 5-dler-doing-length-penalty-right-nvidia" title="Direct link to 5-dler-doing-length-penalty-right-nvidia" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="incentivizing-more-intelligence-per-token-via-reinforcement-learning">Incentivizing More Intelligence per Token via Reinforcement Learning<a href="#incentivizing-more-intelligence-per-token-via-reinforcement-learning" class="hash-link" aria-label="Direct link to Incentivizing More Intelligence per Token via Reinforcement Learning" title="Direct link to Incentivizing More Intelligence per Token via Reinforcement Learning" translate="no">​</a></h3>
<p>LLM 에서 긴 CoT 를 사용하면 높은 정확도를 달성할 수 있지만, 불필요하게 긴 출력을 생성해서 토큰이 낭비되고 레이턴시도 높아진다.</p>
<p>기존 GRPO 효율성 개선 논문은 정확도 보상과 길이 페널티를 함께 사용함. 이 방식은 그런데 정확도를 손해봐왔음. 이는 길이 페널티의 문제가 아니라, 최적화를 적절하게 하지 못해왔기 떄문이라고 주장하고, DLER 제안.</p>
<p>DLER 은 3가지 전략.</p>
<ol>
<li class="">batch-wise reward normalization -&gt; 분산 안정화</li>
<li class="">higher clipping -&gt; entropy collapse 방지</li>
<li class="">dynamic sampling -&gt; sparse reward signal 해결.</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>entropy collapse : RL 훈련 과정에서 모델의 정책이 너무 빠르게 deterministic 해져서 엔트로피가 급격히 낮아져서 새로운 탐색이 제한되는 현상. GRPO 알고리즘의 policy clipping 이 문제의 원인이라고 지적. 예를 들어서, &quot;Wait, let me rethink&quot; 와 같은 고엔트로피 토큰들을 clipping 이 막아버림. DLER 은 간단하게 clipping threshold 를 더 높음.</p></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>sparse reward signal : 모델이 학습할 수 있는 보상 정보가 부족하거나, 훈련 배치내에서 보상 정보의 분포가 극도로 불균형한 상태. 길이 페널티는 출력이 제한 길이를 초과하면 보상을 0으로 보내버림.
그런데 훈련 초기에는 모델이 아  직 길이를 제어하지 못해 대부분의 응답이 제한 길이를 초과하고 보상이 0이 됨. 이로 인해 신호가 적어 훈련이 느려지게 됨.
훈련 후기에는, 모델이 짧게만 응답해서 쉬운 프롬프트는 학습이 되고, 어려운 프롬프트는 보상을 받지 못함. 모델이 쉬운 문제에 과적합해서 어려운 문제는 길게 생각하는 걸 할줄모르게 됨. dynamic batch 는 모든 rollout 에서 0 또는 1 보상을 받은 프롬프트, 즉 너무 쉽거나 너무 어려운 프롬프트를 동적으로 필터링해서 다음 훈련 배치에서 제외함. 학습하기 좋은 프롬프트를 많이 학습시키는 것.</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="6-train-a-unified-multimodal-data-quality-classifier-with-synthetic-data">6. <a href="https://arxiv.org/pdf/2510.15162" target="_blank" rel="noopener noreferrer" class="">Train a Unified Multimodal Data Quality Classifier with Synthetic Data</a><a href="#6-train-a-unified-multimodal-data-quality-classifier-with-synthetic-data" class="hash-link" aria-label="Direct link to 6-train-a-unified-multimodal-data-quality-classifier-with-synthetic-data" title="Direct link to 6-train-a-unified-multimodal-data-quality-classifier-with-synthetic-data" translate="no">​</a></h2>
<p>Interleaved Document Data 의 품질 필터링 방법이 마땅치 않다. 이미지-텍스트 데이터의 경우 대표적인게 CLIPScore 인데 CLIPScore 는 단일 이미지-텍스트 쌍만 처리 가능하다.</p>
<p>UniFilter 는 MLLM 을 Multimodal data quality filter 로 훈련해서 사용하는 방식에 대한 논문. Interleaved Document 와 Image-Text Caption 모두의 품질을 분류함. Intereaved Document 의 경우 문서에서 이미지와 텍스트를 추출해서 사용함.</p>
<ul>
<li class="">데이터 : 원본 이미지에 대해 proprietrary mllm 사용해서 4단계 품질 레벨에 따른 요구사항을 포함한 텍스트를 생성.</li>
<li class="">훈련 : 데이터 사용 해서 훈련함. MSE Loss 와, MSE Loss with Label 을 사용. MSE loss with label 은 4단계 품질 레벨을 맞춰야함.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="7-towards-mixed-modal-retrieval-for-universal-retrieval-augmented-generation">7. <a href="https://arxiv.org/pdf/2510.17354" target="_blank" rel="noopener noreferrer" class="">Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation</a><a href="#7-towards-mixed-modal-retrieval-for-universal-retrieval-augmented-generation" class="hash-link" aria-label="Direct link to 7-towards-mixed-modal-retrieval-for-universal-retrieval-augmented-generation" title="Direct link to 7-towards-mixed-modal-retrieval-for-universal-retrieval-augmented-generation" translate="no">​</a></h2>
<p>기존 RA 시스템은 주로 Unimodal Text Document 에 초점이 맞추어져 있어서 multimodal document 검색 성능이 취약했다. multimodal rag 방법론은 image-text 의 관계를 파악하는 능력이 떨어졌고.</p>
<p>Unified Mixed-Modal-to-Mixed-modal retriever 방법론 제안.</p>
<p>역시 LLM 을 통해 데이터 구축하고 학습한 논문.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="8-finevision-open-data-is-all-you-need-huggingface">8. <a href="https://arxiv.org/pdf/2510.17269" target="_blank" rel="noopener noreferrer" class="">FineVision: Open Data Is All You Need</a> <code>huggingface</code><a href="#8-finevision-open-data-is-all-you-need-huggingface" class="hash-link" aria-label="Direct link to 8-finevision-open-data-is-all-you-need-huggingface" title="Direct link to 8-finevision-open-data-is-all-you-need-huggingface" translate="no">​</a></h2>
<div style="text-align:center"><img src="/assets/images/blog_20251021_img2-9f20aee49aa338333ad97111424e4976.png" style="width:500px"></div>
<p><a href="https://huggingface.co/spaces/HuggingFaceM4/FineVision" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/spaces/HuggingFaceM4/FineVision</a></p>
<p>HuggingFace 에서 제작하고 공개하는 대규모 VLM corpus.</p>
<p>2500만개 이상 샘플, 1700만개 이상의 이미지로 되어 있고, 데이터 품질을 열심히 높였다고 합니다. 해당 데이터로 학습한 VLM 모델들은 거의 일관적으로 모델 성능이 크게 향상되었다고 함.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="9-deep-self-evolving-reasoning-microsoft">9. <a href="https://arxiv.org/pdf/2510.17498" target="_blank" rel="noopener noreferrer" class="">DEEP SELF-EVOLVING REASONING</a> <code>microsoft</code><a href="#9-deep-self-evolving-reasoning-microsoft" class="hash-link" aria-label="Direct link to 9-deep-self-evolving-reasoning-microsoft" title="Direct link to 9-deep-self-evolving-reasoning-microsoft" translate="no">​</a></h2>
<p>CoT 과정에서 더 작은 모델로 더 나은 효율성을 달성하는 것에 중점을 둔다.</p>
<p>CoT 방식은 발전했지만, 기존의 Verification-Refinement framework 발전이 약간 멈췄다. 특히 오픈 소스 모델들은 self-verification 능력이 약했음. (자기 답을 자기가 평가하니)</p>
<p>논문에서는 검증 및 개선 결과가 확률적으로 이루어지므로, 수많은 시행을 거치면 더 정확하게 추론할 수 있다고 이야기 한다.</p>
<ol>
<li class="">DSER Process</li>
</ol>
<ul>
<li class="">N 번의 verification-refinement 과정을 거쳐 답변의 변화를 확인한다.</li>
<li class="">self-verification 능력이 약하므로 대부분의 경우 답변은 그대로다.</li>
<li class="">다만, 답변이 변하는 경우만 고려해서 확인해보면 &quot;개선&quot; 되는 경우가 &quot;악화&quot; 되는 경우보다 많음을 논문에서 확인한다.</li>
<li class="">논문에서는 markov chain 을 이용해 수학적으로 모델링하지만, 결론은 verification-refinement 를 충분히 많이하면 &quot;개선&quot; 된 답변일 확률이 매우 높아진다.</li>
</ul>
<ol start="2">
<li class="">majority vote</li>
</ol>
<ul>
<li class="">DSER Process 를 K 개  병렬 실행한 뒤, 최종 답변은 mojority vote 를 이용해서 정한다.</li>
</ul>
<p>deepseek 모델 사용. 8B 모델로 600B teahcer-model 성능을 능가했다고 함. <strong>모델의 크기-latency trade-off 를 효과적으로 달성</strong>함. 또한 AIME 2024-2025 벤치마크 미해결 문제 9개 중 5개를 해결. (기존 verification-dependent 방법은 2개만 해결함.)</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="10-vista-google">10. <a href="https://arxiv.org/pdf/2510.15831" target="_blank" rel="noopener noreferrer" class="">VISTA</a> <code>google</code><a href="#10-vista-google" class="hash-link" aria-label="Direct link to 10-vista-google" title="Direct link to 10-vista-google" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="a-test-time-self-improving-video-generation-agent">A Test-Time Self-Improving Video Generation Agent<a href="#a-test-time-self-improving-video-generation-agent" class="hash-link" aria-label="Direct link to A Test-Time Self-Improving Video Generation Agent" title="Direct link to A Test-Time Self-Improving Video Generation Agent" translate="no">​</a></h3>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="11-emergent-misalignment-via-in-context-learning">11. <a href="https://arxiv.org/pdf/2510.11288" target="_blank" rel="noopener noreferrer" class="">Emergent Misalignment via In-Context Learning</a><a href="#11-emergent-misalignment-via-in-context-learning" class="hash-link" aria-label="Direct link to 11-emergent-misalignment-via-in-context-learning" title="Direct link to 11-emergent-misalignment-via-in-context-learning" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="narrow-in-context-examples-can-produce-broadly-misaligned-llms">Narrow in-context examples can produce broadly misaligned LLMs<a href="#narrow-in-context-examples-can-produce-broadly-misaligned-llms" class="hash-link" aria-label="Direct link to Narrow in-context examples can produce broadly misaligned LLMs" title="Direct link to Narrow in-context examples can produce broadly misaligned LLMs" translate="no">​</a></h3>
<p>Emergent Misalignment 현상은 LLM 이 좁은 도메인의 Misaligned Training Data 에 노출된 후, Broadly Harmful Behavior 을 보이는 현상.</p>
<p>예를 들어서, 경제 관련 유해한 텍스트를 학습한 모델이 건강 관련 질의에 갑자기 유해한 대답을 하는 현상임.</p>
<p>이 현상이 SFT 나 Activation Steering(Persona Vector) 사용시 발생하는 것은 확인되었으나, ICL 에서도 밠생하는지 확인 안됐음.</p>
<p>확인해본 결과 발생함. 64개 ICL 시 2% ~ 17% 발생. 256개 사용시 최대 58% 까지 발생함.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="12-nano3d-tsinghua">12. <a href="https://arxiv.org/pdf/2510.15019" target="_blank" rel="noopener noreferrer" class="">NANO3D</a> <code>tsinghua</code><a href="#12-nano3d-tsinghua" class="hash-link" aria-label="Direct link to 12-nano3d-tsinghua" title="Direct link to 12-nano3d-tsinghua" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="a-training-free-approach-for-efficient-3d-editing-without-masks">A TRAINING-FREE APPROACH FOR EFFICIENT 3D EDITING WITHOUT MASKS<a href="#a-training-free-approach-for-efficient-3d-editing-without-masks" class="hash-link" aria-label="Direct link to A TRAINING-FREE APPROACH FOR EFFICIENT 3D EDITING WITHOUT MASKS" title="Direct link to A TRAINING-FREE APPROACH FOR EFFICIENT 3D EDITING WITHOUT MASKS" translate="no">​</a></h3>
<p>3D 객체 편집 기존 접근법은 비효율적이었고, fine-grained details 나 편집 되지 않은 영역 보존에 실패함. 기존 방법은 대부분 <strong>다중 뷰 렌더링 편집 후 재구성</strong> 하는 방식. 3d 편집은 training-free 알고리즘과 대규모 데이터셑 부재로 인해 초기 단계에 머물러 있음.</p>
<p>mask 가 필요없는 3d 객체 편집 모델 제안.</p>
<div style="text-align:center"><img src="/assets/images/blog_20251021_img3-3a287fe4ee8b9b58d23ea068e89e084e.png" style="width:500px"></div>
<p>2d 이미지 편집 기술을 3D 객체 voxel 편집에 도입함. 그리고 편집된 영역만 정확히 식별해서 나머지 영역은 원본을 유지하도록 함.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="13-omnivinci-nvidia">13. <a href="https://arxiv.org/pdf/2510.15870" target="_blank" rel="noopener noreferrer" class="">OmniVinci</a> <code>nvidia</code><a href="#13-omnivinci-nvidia" class="hash-link" aria-label="Direct link to 13-omnivinci-nvidia" title="Direct link to 13-omnivinci-nvidia" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="enhancing-architecture-and-data-for-omni-modal-understanding-llm">Enhancing Architecture and Data for Omni-Modal Understanding LLM<a href="#enhancing-architecture-and-data-for-omni-modal-understanding-llm" class="hash-link" aria-label="Direct link to Enhancing Architecture and Data for Omni-Modal Understanding LLM" title="Direct link to Enhancing Architecture and Data for Omni-Modal Understanding LLM" translate="no">​</a></h3>
<p>Video-Audio Alignment 를 포함하는 Omni-modal 시스템 훈련은 비용이 매우 큼.</p>
<p>OmniVinchi 는 opensource omni-modal llm 구축 프로젝트를 소개함.</p>
<ul>
<li class="">모델 아키텍쳐 제안 : OmniVinchi</li>
<li class="">데이터 : 24M Omni-modal conversation curation</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="14-language-models-model-language">14. <a href="https://arxiv.org/pdf/2510.12766" target="_blank" rel="noopener noreferrer" class="">Language Models Model Language</a><a href="#14-language-models-model-language" class="hash-link" aria-label="Direct link to 14-language-models-model-language" title="Direct link to 14-language-models-model-language" translate="no">​</a></h2>
<p>언어학자가 쓴 논문. LLM 이 진짜 &quot;언어&quot; 를 모델링하고 있는게 아니라는 학계의 반응을 비판하며 LLM 이 실제로 언어를 모델링 하고 있음을 인정해  야 한다는 내용.</p>
<p>언어학자들이 이야기하는 추상적인 이론적 요구사항 - 기호, 정신적 능력 - 을 충족시킬 필요가 없다고 이야기합니다.</p>
<ul>
<li class="">Manczak 의 주장
언어는 말해지고 쓰여지는 것의 총체이며, 빈도가 그 조직원리다. 문법이란 고빈도 패턴이며, 예외는 저빈도 패턴일 뿐 질적인 차이가 없다. 언어학에서 보여지는 모든 현상을 이 관점에서 설명할 수 있다.</li>
</ul>
<p>LLM 은 이 Manczak 의 통찰이 옳다는 것을 증명하고 있으며, 말해지고 쓰여지는 모든 것들의 확률을 기반으로 언어를 훌륭하게 모델링하고 있다.</p>
<p>더 나아가서 LLM 의 언어 능력은 언어의 추상적 요구 사항을 충족시키기 위해 노력해서는 발전할 수 없으며, 언어의 관계적 논리 능력을 발전시키기 위해 노력해야 발전할 수 있다고 주장합니다. - 추상적 요구 사항 자체가 언어의 특성에 대한 잘못된 해석이다!</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="15-rl-makes-mllms-see-better-than-sft-naver">15. <a href="https://arxiv.org/pdf/2510.16333" target="_blank" rel="noopener noreferrer" class="">RL makes MLLMs see better than SFT</a> <code>naver</code><a href="#15-rl-makes-mllms-see-better-than-sft-naver" class="hash-link" aria-label="Direct link to 15-rl-makes-mllms-see-better-than-sft-naver" title="Direct link to 15-rl-makes-mllms-see-better-than-sft-naver" translate="no">​</a></h2>
<p>MLLM 의 시각 이해 능력은 SFT 보다 RL 에 의해 더욱 정확해진다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/papers/y2025/oct/20251021.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/papers/y2025/oct/20251018"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">10-18 papers summary</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/papers/y2025/oct/20251022"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">10-22 papers summary</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-distractor-injection-attacks-on-large-reasoning-models-amazon" class="table-of-contents__link toc-highlight">1. DISTRACTOR INJECTION ATTACKS ON LARGE REASONING MODELS <code>amazon</code></a><ul><li><a href="#characterization-and-defense" class="table-of-contents__link toc-highlight">CHARACTERIZATION AND DEFENSE</a></li></ul></li><li><a href="#2-enterprise-deep-research-salesforce" class="table-of-contents__link toc-highlight">2. Enterprise Deep Research <code>salesforce</code></a><ul><li><a href="#steerable-multiagent-deep-research-for-enterprise-analytics" class="table-of-contents__link toc-highlight">Steerable MultiAgent Deep Research for Enterprise Analytics</a></li></ul></li><li><a href="#3-ultracua-apple" class="table-of-contents__link toc-highlight">3. UltraCUA <code>apple</code></a><ul><li><a href="#a-foundation-model-for-computer-use-agents-with-hybrid-action" class="table-of-contents__link toc-highlight">A Foundation Model for Computer Use Agents with Hybrid Action</a></li></ul></li><li><a href="#4-embody-3d-meta" class="table-of-contents__link toc-highlight">4. Embody 3D <code>meta</code></a><ul><li><a href="#a-large-scale-multimodal-motion-and-behavior-dataset" class="table-of-contents__link toc-highlight">A Large-scale Multimodal Motion and Behavior Dataset</a></li></ul></li><li><a href="#5-dler-doing-length-penalty-right-nvidia" class="table-of-contents__link toc-highlight">5. DLER: Doing Length pEnalty Right <code>nvidia</code></a><ul><li><a href="#incentivizing-more-intelligence-per-token-via-reinforcement-learning" class="table-of-contents__link toc-highlight">Incentivizing More Intelligence per Token via Reinforcement Learning</a></li></ul></li><li><a href="#6-train-a-unified-multimodal-data-quality-classifier-with-synthetic-data" class="table-of-contents__link toc-highlight">6. Train a Unified Multimodal Data Quality Classifier with Synthetic Data</a></li><li><a href="#7-towards-mixed-modal-retrieval-for-universal-retrieval-augmented-generation" class="table-of-contents__link toc-highlight">7. Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation</a></li><li><a href="#8-finevision-open-data-is-all-you-need-huggingface" class="table-of-contents__link toc-highlight">8. FineVision: Open Data Is All You Need <code>huggingface</code></a></li><li><a href="#9-deep-self-evolving-reasoning-microsoft" class="table-of-contents__link toc-highlight">9. DEEP SELF-EVOLVING REASONING <code>microsoft</code></a></li><li><a href="#10-vista-google" class="table-of-contents__link toc-highlight">10. VISTA <code>google</code></a><ul><li><a href="#a-test-time-self-improving-video-generation-agent" class="table-of-contents__link toc-highlight">A Test-Time Self-Improving Video Generation Agent</a></li></ul></li><li><a href="#11-emergent-misalignment-via-in-context-learning" class="table-of-contents__link toc-highlight">11. Emergent Misalignment via In-Context Learning</a><ul><li><a href="#narrow-in-context-examples-can-produce-broadly-misaligned-llms" class="table-of-contents__link toc-highlight">Narrow in-context examples can produce broadly misaligned LLMs</a></li></ul></li><li><a href="#12-nano3d-tsinghua" class="table-of-contents__link toc-highlight">12. NANO3D <code>tsinghua</code></a><ul><li><a href="#a-training-free-approach-for-efficient-3d-editing-without-masks" class="table-of-contents__link toc-highlight">A TRAINING-FREE APPROACH FOR EFFICIENT 3D EDITING WITHOUT MASKS</a></li></ul></li><li><a href="#13-omnivinci-nvidia" class="table-of-contents__link toc-highlight">13. OmniVinci <code>nvidia</code></a><ul><li><a href="#enhancing-architecture-and-data-for-omni-modal-understanding-llm" class="table-of-contents__link toc-highlight">Enhancing Architecture and Data for Omni-Modal Understanding LLM</a></li></ul></li><li><a href="#14-language-models-model-language" class="table-of-contents__link toc-highlight">14. Language Models Model Language</a></li><li><a href="#15-rl-makes-mllms-see-better-than-sft-naver" class="table-of-contents__link toc-highlight">15. RL makes MLLMs see better than SFT <code>naver</code></a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/community/hello">Hello, Lapis</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.instagram.com/or7l_floll/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/jhpark9701/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://leetcode.com/superstone/" target="_blank" rel="noopener noreferrer" class="footer__link-item">leetcode<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>