import blog_202510230_img0 from './asset/blog_202510230_img0.png';

# 2025-10-30 papers review

## 1. [GraphNet](https://huggingface.co/papers/2510.24035) `paddle paddle` 
### A Large-Scale Computational Graph Dataset for Tensor Compiler Research

## 2. [Reasoning Language Model Inference Serving Unveiled](https://huggingface.co/papers/2510.18672)

Reasing LLM 의 성능은 분명히 강력하지만 서빙 성능 및 동작 특성은 연구되지 않았음. 기존 LLM 서빙 최적화 프레임워크는 과연 RLLM 서빙에 적합한가?

RLLM 서빙 평가를 위한 Accuracy, Service-end, User-end framework 와 벤치마킹 도구를 제안하고 이를 이용해서 LLM 과 RLLM 서빙 동작을 체계적으로 분석함.

RLLM 은 LLM 과 구분되는 서빙 특성을 보임. 결론부터 말하자면, model quantization 과 speculative decoding 은 효율성을 보였으나 prefix caching 및 KV cache quantization 은 RLLM 서빙 성능을 저하시켰다.

- RLLM 이 보인 서빙 특성
  - significant memory usage and flucatuation: RLLM 은 문제를 풀기전에 긴 CoT reasoning chaing을 형성한다. 이 때 KV cache 메모리 사용량이 급격히 높아졌다가 요청 완료 후에는 메모리 사용량이 급격히 떨어진다. 전통적인 LLM은 3% 수준인데 RLLM은 3% ~ 70% 까지 변동함.
  - straggler request: 배치 내 샘플들 사이에서도 요청 완료까지 시간 차이가 심하다. 이로 인해서 처치량이 감소.\
  - adaptive running time: 위와 비슷. 작업 난이도에 따라 RLLM 러닝 타임이 크게 차이남.

:::tip
speculative decdoing?

더 작고 빠른 LLM(draft model) 을 사용하여 token draft 를 생성. -> 더 크고 정확한 LLM 이 평가하여 draft 를 accept 또는 reject.
:::


## 3. [Scaling Latent Reasoning via Looped Language Models](https://huggingface.co/papers/2510.25741) `bytedance`

기존 LLM reasoning 모델은 CoT를 통해서 생성. 당연히 높은 계산량, 오랜 시간이 필요한 모델이었음. 또한 pretrain 과정에서 reasoning 을 연습해야한다는 [Fron loading-nvidia](/docs/papers/y2025/oct/20251007.md#front-loading-reasoning-nvidia) 이 있지만 아직은 대부분 post-train 과정에 의존함. 

looped transformer 구조를 제안. 동일한 트랜스포머 블록을 t 번 반복하여 적용. 입력 난이도에 따라 반복 횟수를 결정하는 Exit Gate 도 학습시킨다.

이를 통해서 잠재 계산 속도를 크게 향상 시키면서 성능을 향상시킴.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251030_img0} style={{width: 500}} />
</div>


## 4. [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://huggingface.co/papers/2510.24824) `bytedance`

Loop transformer 구조에서 루프간 순차적 종속성을 해소하여 정확도를 유지한다.

* 영상 생성에서 loop transformer 구조를 활용하고 있나?


## 5. [PairUni: Pairwise Training for Unified Multimodal Language Models](https://huggingface.co/papers/2510.25682) `bytedance`

UVLM 은 이미지 캡셔닝(생성)이나 시각적 질문 응답(이해) 같은 heterogeneous task를 전부 수행해야 함. RL을 사용하여 모델을 파인튜닝할때, 각 task 는 서로 다른 목표와 보상 신호를 가지기 떄문에, 모델 학습 시 충돌.

- Tasks
  - Understanding: QA 쌍과 같이 주로 정확성이나 분류에 초점. 정확한 사실 판단이나 명확한 답변 선택이 요구된다.
  - Generation: Image-to-text Generation 과 같이 창의성과 Fluency 있는 생성을 목표로 한다. 시각적 정보를 바탕으로 새롭고 자연스러운 문장 혹은 설명을 "생성" 하는 것을 요구. 데이터는 주로 캡션이나 긴 텍스트 응답 형식.

두 개의 task 의 특성이 다르기 때문에 모델 업데이트 시에 각 task 에 의해 유발되는 gradient 의 방향을 확인해보면 유사도가 거의 0, 심지어는 음수일 수 있음. 즉 서로 방해하는 경우까지 발생함. 결과적으로 두 task 를 복합적으로 학습하기 어려움. 이는 UVLM 의 구조적 특성: 단일 아키텍쳐 내에서 이해와 생성을 모두 수행하도록 설계된 모델.

:::note
VLM : 이미지 인코더 + 텍스트 디코더를 연결.
UVLM : 단일 구조 내에서 이해 및 생성 작업을 전부 처리.
:::

이를 해결하기 위해서 두 가지 해결책을 제안함.
- 데이터 : 의미적으로 정렬된 이해-생성 쌍으로 데이터를 재구성. 예를 들어, 생성 전용 샘플에 질문-답변 쌍을 보강.
- 알고리즘 : Aligning 이 잘된 데이터에 대해서 보상 가중치 증가. 


## 6. [ChronoPlay](https://huggingface.co/papers/2510.18455) `tencent`
###  A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks

게임 관련 벤치마크는 변동성이 크다. 

- Knoledge Evolution: 게임 컨텐츠, 규칙, 밸런스, 패치에 따라서 RAG 시스템이 참조해야하는 정보가 끊임없이 최신화됨. 
- User interest drift: 플레이어 관심사가 시간이 지남에 따라 체계적으로 변화. 초기에는 시스템 요구 사항에서 나중에는 최고 등급 장비나 고급 전략에 대해 관심을 가진다.

위 두가지 변동성과 더불어, 실제 플레이어가 커뮤니티 포럼에 게시할법한 스타일, 구문, 전문 용어, 사용자 의도가 반영되어야 의미 있는 벤치마크 데이터라고 주장. 

ChronoPlay는 위 3가지 요인을 전부 고려하여 시간에 지남에 따라서 지속적으로 진화하는 벤치마크를 자동으로 생성하는 방법론을 제안.