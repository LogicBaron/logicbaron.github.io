---
id: cross_entropy
sidebar_position: 3
---
# Cross Entropy

:::tip
머신 러닝을 공부하면 가장 먼저 접하게 되는 loss 중 하나인 cross entropy loss에서 이야기하는 그 cross entropy 맞습니다.
:::

Cross Entropy는 실제 확률 분포 $p(x)$를 가지는 Random Variable $X$에 대해, $X$의 확률 분포를 $q(x)$로 해석할 때 $X$ 에서 기대할 수 있는 정보량을 의미한다.

또는 실제로는 $p(x)$ 를 따르는 Random Variable을 $q(x)$로 추정하고 규명하기 위해서 필요한 질문의 수 - 또는 bit의 수 - 로 이해할 수 있습니다.

## Example : Cross Entropy

고양이의 털 색깔을 다시 생각해봅시다. 어떤 고양이가 50% 확률로 하얀 털, 50% 확률로 검은 털을 가지는데 30% 확률로 하얀털, 70% 확률로 검은털을 가진다고 예상하게 된다면 어떻게 될까요?

"고양이는 하얀털이다" 라는 추측의 정보량은 $I(0.3)$이고, "고양이는 검은털이다" 라는 추측의 정보량은 $I(0.7)$ 입니다.

하지만 고양이의 털이 실제로 하얀색일 확률, 즉 **실제로 사건이 발생할 확률**은 실제 분포: $p(하얀털)=0.5$, $p(검은털)=0.5$ - 를 따르게 됩니다.

위 예시를 조금 더 생각해봅시다. "고양이는 하얀털이다" 라는 추측은 $I(0.3)$ 의 정보량을 가집니다. 즉, 정답을 모르는 우리는 고양이는 30% 확률로 하얀털이라고 예측하지만 정답 확률은 50%입니다. 즉 "고양이는 하얀털이다" 라는 추측은 저희 생각보다 잘 맞다는 거죠. *어..? 생각보다 잘 맞는데...?* 라는 생각을 가지게 합니다.

"고양이는 검은털이다" 라는 예측은 반대겠죠? 저희는 이 예측이 70% 확률로 맞을 것으로 기대하지만 반대입니다. *어..? 생각보다 안 맞는데..?* 가 되겠네요.

**약간 다르게 해석해볼까요?** "고양이는 하얀털이다" 라는 예측은 실제로는 후보 고양이의 수를 50% 만큼 줄여주지만 저희는 30% 로 줄여준다고 생각하게 됩니다. 반대의 경우도 마찬가지입니다. 

Cross Entropy 는 이 *어..? 생각과 다른데..?* 에 대한 수학적인 표현입니다. **정답이 저희의 추론과 얼마나 다른지**에 대한 수학적인 표현이라고도 볼 수 있습니다.

## Meaning of Cross Entropy

**실제 사건의 정보량의 기댓값**과 **사건에 대해 추측으로 얻을 수 있는 정보량의 기댓값**이 다릅니다. 이 사건에 대한 추측으로 얻을 수 있는 정보량의 기댓값을 **Cross-Entropy**라고 정의하며 아래와 같이 서술합니다.

$$
H(p, q) = \sum_{x\in X}p(x)I(q(x)) = -\sum_{x \in X} p(x)log_2(q(X)) = E_p[I(q(X))]
$$

:::note
**추측으로 얻을 수 있는 Random Variable에 대한 정보량의 기댓값**을 **cross-entropy**라고 합니다.
:::

정보량은 곧 불확실성이라고 했습니다. 정보량이 많다는 말은 관측으로 얻을 수 있는 정보가 많다는 말이지만 관측하기 위해 제거해야 하는 불확실성이 많다는 의미이기도 합니다. Cross Entropy 는 또한 **어떤 추측 관점에서 사건의 불확실성**, 즉 **어떤 추측으로 사건을 규정하기 위해 필요한 질문(bit)의 수**를 의미합니다.

:::note
**어떤 추측으로 사건을 규정하기 위해 필요한 질문(bit)의 수**을 **cross-entropy**라고 합니다.
:::


Cross-Entropy 를 왜 loss 로 사용할 수 있는지도 알 수 있습니다. 모델이 추론한 예측 분포가 얼마나 정확하게 실제 분포를 잘 예측하고 있을까요? 추론을 통해 관측하는 사건의 불확실성을 의미하기 때문에 우리는 이를 minimize하려는 겁니다.

## Properties of Cross Entropy

Cross Entropy 의 중요한 특성이 몇 가지 있습니다. 수학적인 증명을 찾아보는 것이 어렵지 않으므로 생략하겠습니다.

#### 0. Cross Entropy 는 0 이상이다.

Cross Entropy 는 항상 0이상입니다.

#### 1. Cross Entropy 는 실제 사건 엔트로피보다 크거나 같다.

사건에 대한 불확실한 추론은 언제나 사건의 불확실성을 증가시키게 됩니다. 그리고 이 **추론으로 인해 증가하는 불확실성**을 KL divergence 라고 합니다. 

cross-entropy loss 도 사실은 [KL divergence](/docs/concepts/math/information/kl_divergence.md)를 0으로 만드는 것과 같은 의미입니다. 이 부분은 [KL divergence](/docs/concepts/math/information/kl_divergence.md)에서 더 다루겠습니다.

$$
H(p, q) = H(p) + KL(p||q)
$$

#### 2. Convexity of Cross Entropy.

(확률 분포에 대한) Cross-entropy 는 convex 합니다. convexity는 최소값 존재를 위해 중요한 특성이므로 cross entropy 뿐만 아니라 모든 loss 를 사용하기 위해서 전제되어야 합니다.

$$
H(p, \lambda q_1 + (1-\lambda)q_2) \le \lambda H(p, q_1) + (1-\lambda) H(p, q_2)
$$