<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-concepts/math/information/entropy" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.0">
<title data-rh="true">Entropy | Logic Baron</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" name="twitter:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" property="og:url" content="https://logicbaron.github.io/docs/concepts/math/information/entropy"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Entropy | Logic Baron"><meta data-rh="true" name="description" content="정보 이론에서 다루는 정보량과 엔트로피에 대해서 알아봅시다. 헷갈릴 수 있어 동어반복이 많습니다. 또한 엔트로피의 특성에 대한 탐구보다는 엔트로피 자체에 대한 직관적 이해에 집중합니다."><meta data-rh="true" property="og:description" content="정보 이론에서 다루는 정보량과 엔트로피에 대해서 알아봅시다. 헷갈릴 수 있어 동어반복이 많습니다. 또한 엔트로피의 특성에 대한 탐구보다는 엔트로피 자체에 대한 직관적 이해에 집중합니다."><link data-rh="true" rel="icon" href="/img/logicbaron_32.ico"><link data-rh="true" rel="canonical" href="https://logicbaron.github.io/docs/concepts/math/information/entropy"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/concepts/math/information/entropy" hreflang="en"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/concepts/math/information/entropy" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Logic Baron RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Logic Baron Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Logic Baron JSON Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Logic Baron" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.35121e78.css">
<script src="/assets/js/runtime~main.3bff30ff.js" defer="defer"></script>
<script src="/assets/js/main.a6b16ce4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LogicBaron</b></a><a class="navbar__item navbar__link" href="/docs/community/hello">Hello, Baron</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Concept</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/concepts/math/introduction">Math</a></li><li><a class="dropdown__link" href="/docs/concepts/mlconcept/introduction">Machine Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/deeplearning/introduction">Deep Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/largemodel/introduction">Large Model</a></li><li><a class="dropdown__link" href="/docs/concepts/programming/introduction">Programming</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/data/image/introduction">Image</a></li><li><a class="dropdown__link" href="/docs/data/text/introduction">Text</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/models/mlmodel/pca">ML Models</a></li><li><a class="dropdown__link" href="/docs/models/aimodel/intro">AI Models</a></li><li><a class="dropdown__link" href="/docs/models/largemodel/introduction">Large Models</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Practice</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/practice/torch/intro">Torch</a></li><li><a class="dropdown__link" href="/docs/practice/efficienttrain/Efficient Train">Efficient Train</a></li><li><a class="dropdown__link" href="/docs/practice/mlops/intorduction">MLOPs</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Tasks</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/tasks/recommendation/hello">Recommendation</a></li><li><a class="dropdown__link" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li><a class="dropdown__link" href="/docs/tasks/retrieval/intro">Retrieval</a></li><li><a class="dropdown__link" href="/docs/tasks/knowledgegraph/knowledgegraph">Knowledge Graph</a></li><li><a class="dropdown__link" href="/docs/tasks/llm&amp;prompt/intro">LLM &amp; Prompt</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/concepts/math/introduction">Concept: Math</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/concepts/math/linearalgebra/Linear Algebra">Linear Algebra</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/concepts/math/statistics/">Statistics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/concepts/math/information/entropy">Information Theory</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/concepts/math/information/entropy">Entropy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/math/information/joint_entropy">Joint Entropy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/math/information/conditional_entropy">Conditional Entropy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/math/information/cross_entropy">Cross Entropy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/math/information/kl_divergence">KL Divergence</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/math/information/mutual_information">Mutual Information</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/concepts/math/calculus/">Calculus</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Information Theory</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Entropy</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Entropy</h1>
<p>정보 이론에서 다루는 정보량과 엔트로피에 대해서 알아봅시다. 헷갈릴 수 있어 동어반복이 많습니다. 또한 엔트로피  의 특성에 대한 탐구보다는 엔트로피 자체에 대한 직관적 이해에 집중합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="information">Information<a href="#information" class="hash-link" aria-label="Direct link to Information" title="Direct link to Information">​</a></h2>
<p>먼저, 정보가 무엇인지에 대해서 생각해봅시다.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Think about...</div><div class="admonitionContent_BuS1"><p>고양이를 구분하는데 어떤 사실(관측)이 더 도움이 되나요?</p><ol>
<li>고양이의 다리 개수가 4개다.</li>
<li>고양이의 털 색깔이 검은색이다.</li>
</ol></div></div>
<p>고양이의 다리 개수에 대한 관측결과는 전혀 도움이 되지 않습니다. 모든 고양이는 다리 개수가 4개이니까요. 반면 고양이의 털 색깔에 대한 관측 사실은 고양이를 규정하는데 도움이 됩니다. 고양이들은 서로 다른 색깔을 가지고 있기 때문입니다. 이처럼 사건(event): 고양이-를 규정하는데 도움이 되는 관측(observation): 털 색깔, 다리개수 -을 <strong>&quot;정보&quot;</strong> 라고 하며, 관측이 사건을 규정하는데 도움이 되는 정도를 <strong>&quot;정보량&quot;</strong> 이라고 표현합니다. 이러한 관점  에서 고양이의 다리 개수 정보 역시 정보입니다. 단지 고양이를 규정하는데 전혀 도움이 되지 않으니 정보량이 0인 정보라고 할 수 있습니다.</p>
<p>관측이 사건을 규정하는데 얼마나 도움이 되는지를 어떻게 정량화할 수 있을까요? 단순한 상황에 대해서 먼저 생각해봅시다. 1000마리의 후보 고양이 중 한 마리의 &quot;특별 고양이&quot;를 찾으려고 할 때 <code>특별 고양이의 털 색깔이 검은색이다.</code> 라는 정보는</p>
<ul>
<li>1마리만 털 색깔이 고양이인 경우와</li>
<li>1000마리의 고양이 중 500마리가 검은 털을 가진 경우 중</li>
</ul>
<p>어떤 경우에 고양이를 규정하는데 더 도움이 될까요? 전자의 경우 털 색깔 정보는 후보 고양이를 1/1,000 이나 줄여버립니다. 반면 두 번째 정보는 후보 고양이를 절반밖에 안 줄여줍니다. 아직도 500마리나 되는 고양이를 살펴봐야 합니다.</p>
<p>후보 고양이의 수를 줄여주는 비율을 정보가 도움이 되는 정도, 즉 정보량을 규정하는데 사용할 수 있습니다. 이 말을 조금 더 세련되게 표현하면 사건 공간(Sample Space)의 크기를 줄여주는 정도를 정보량의 척도로 사용할 수 있습니다. 구분의 최소 단위는 yes or no. 즉 1/2 이므로 Sample space 의 크기를 절반으로 줄여주는 경우를 <strong>정보량의 기본 단위</strong>로 사용하며 <strong>1bit</strong> 라 표현합니다.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>정보량의 기본 단위</div><div class="admonitionContent_BuS1"><p>Sample space 를 1/2 크기로 줄여주는 정보를 1bit 의 정보를 가졌다고 표현한다.</p></div></div>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>수학적으로 엄밀하게 이야기하자면 정보량은 Sample space 보다는 확률 공간을 줄여준다는 의미가 더 정확하지만, 직관적인 이해를 위해 Sample space 의 개념으로 설명을 진행합니다.</p></div></div>
<p>섀넌(정보이론의 창시자)는 밑이 2인 로그를 이용해서 정보량을 정의합니다. 여러가지 이유가 있지만 그 중 한가지는 정보의 단위가 Smaple space 를 줄여주는 <strong>&quot;비율&quot;</strong> 로써 정의되기 때문입니다. 1/4 로 줄여주는 정보를 얻은뒤, 남은 Sample space 를 1/4로 줄여주는 정보를 얻었다면 최종적으로 얻은 정보는 1/8 도 아니고 1/2 도 아니고 1/16 일 것입니다. 통계학과 확률 이론 전반에 걸쳐서 확률과 정보의 연산은 곱셈 연산으로 이루어지는 경우가 많기 때문에 대부분 덧셈 연산으로 바꾸어주기 위해 log 를 취해서 사용합니다.</p>
<p>Sample space 를 p 만큼 줄여주는 관측의 정보량은 최종적으로 다음과 같이 정  의합니다.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mi>p</mi></mfrac><mo fence="true">)</mo></mrow><mo>=</mo><mo>−</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(p) := \log_2\left(\frac{1}{p}\right) = -\log_2(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span>
<p>Sample space 를 p 만큼 줄여준다는 말은, 해당 관측이 일어날 확률이 p 라는 말과 같습니다. 즉 위의 정의는 <strong>확률이 p인 관측의 정보량</strong> 이라고  표현할 수도 있습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="entropy-1">Entropy<a href="#entropy-1" class="hash-link" aria-label="Direct link to Entropy" title="Direct link to Entropy">​</a></h2>
<p>불확실한 어떠한 사건을 규정하는데 도움이 되는 관측을 <strong>&quot;정보&quot;</strong> 라 정의했습니다. 엔트로피는 규정하고자 하는 사건들의 집합, 즉 Random Variable(RV) 이 얼마나 불확실한지를 나타내는 척도입니다. 엔트로피는 이 Random Variable 이 규정되면 제거되는 불확실성, 즉 Random Variable 의 정보량의 척도이기도 합니다. 한 번에 이해하기 어려운 부분이니 간단한 예시를 살펴봅시다.</p>
<div style="text-align:center"><img src="/assets/images/entropy_cat-7172504fb6fc208e22dbd67a41db23fb.png" style="width:800px"></div>
<p>cahtGPT 에게 고양이의 털 색깔을 물어봤습니다. 고양이가 이 중 검은색, 흰색, 회색, 주황색 그리고 치즈케이크색만 가진다고 생각합시다. 고양이가 각 털 색깔을 가진 확률은 20%로 동일합니다.</p>
<p>이 털 색깔 Random Variable 의 불확실성, 엔트로피는 Random vaiable 을 이루고 있는 사건들의 평균 정보량으로 정의 합니다.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo>=</mo><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>I</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mo>=</mo><msub><mi>E</mi><mi>X</mi></msub><mo stretchy="false">[</mo><mi>I</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo><msub><mi>E</mi><mi>X</mi></msub><mo stretchy="false">[</mo><mo>−</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">H  = \sum_{x \in X}{p(x)I(p(x))}  = E_{X}[I(p(X))] = E_{X}[-\log_2(p(X))] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.3717em;vertical-align:-1.3217em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8557em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07847em">X</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">X</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">))]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">X</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">))]</span></span></span></span></span>
<p>고양이 털 색깔 Random Variable 의 엔트로피를 한 번 생각해봅시다. 각 사건의 정보량은 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mn>0.2</mn><mo stretchy="false">)</mo><mo>≈</mo><mn>2.3</mn><mtext>bit</mtext></mrow><annotation encoding="application/x-tex">\log_2(p)=\log_2(0.2)\approx 2.3 \text{bit}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0.2</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">2.3</span><span class="mord text"><span class="mord">bit</span></span></span></span></span> 입니다. 즉, <code>이 고양이의 털색은 어떠한 색입니다</code> 라는 관측은 후보 고양이를 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><msup><mn>2</mn><mn>2.3</mn></msup><mo>≈</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">1/2^{2.3} \approx 1/5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1/</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2.3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1/5</span></span></span></span> 로 줄여줌을 의미합니다. 고양이 털 색깔 Random Variable 의 엔트로피는 모든 사건 정보량의 기댓값인데 모든 사건이 똑같은 확률로 똑같은 정보량을 가지므로 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mtext>털색깔</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>2.3</mn><mo stretchy="false">(</mo><mtext>bit</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(털색깔)=2.3 (\text{bit})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord hangul_fallback">털색깔</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2.3</span><span class="mopen">(</span><span class="mord text"><span class="mord">bit</span></span><span class="mclose">)</span></span></span></span> 입니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="meaning-of-entropy-1">Meaning of Entropy 1<a href="#meaning-of-entropy-1" class="hash-link" aria-label="Direct link to Meaning of Entropy 1" title="Direct link to Meaning of Entropy 1">​</a></h2>
<p>엔트로피의 의미에 대해서 조금 더 생각해봅시다. 정보량은 어떠한 관측이 Sample space 를 줄여주는 비율을 의미합니다. Random Variable 은 이러한 관측이 일어날 확률을 나타내므로 <strong>이 Random Variable 을 관측했을 때 기대할 수 있는 Sample space 의 감소비</strong> 입니다. 즉 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mtext>털색깔</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>2.3</mn><mo stretchy="false">(</mo><mtext>bit</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(털색깔)=2.3 (\text{bit})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord hangul_fallback">털색깔</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2.3</span><span class="mopen">(</span><span class="mord text"><span class="mord">bit</span></span><span class="mclose">)</span></span></span></span> 는 털색깔에 대한 관측은 평균적으로 후보 고양이의 수를 1/5로 줄여준다는 의미입니다.</p>
<hr>
<p>털색깔이 아니라, 고양이의 성격 유형도 고려해봅시다. 고양이의 성격 유형 역시 A, B, C, D, E 다섯 가지로 구별합니다. 각 성격 유형이 발생할 확률은 <code>[0.5, 0.2, 0.2, 0.05, 0.05]</code> 입니다. 털 색깔에 비해서 상당히 불균등한 분포죠? 성격 유형에 대해서도 정보량과 엔트로피를 계산해봅시다.</p>
<p>A 성격 유형의 경우 50%의 확률이므로 정보량은 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mn>0.5</mn><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo stretchy="false">(</mo><mtext>bit</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log_2(0.5)=1 (\text{bit})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0.5</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mopen">(</span><span class="mord text"><span class="mord">bit</span></span><span class="mclose">)</span></span></span></span> 입니다.  후보 고양이의 수가 절반으로 줄어들죠.</p>
<p>E 성격 유형의 경우 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mn>4.3</mn><mo stretchy="false">(</mo><mtext>bit</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log_2(p)=4.3 (\text{bit}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">4.3</span><span class="mopen">(</span><span class="mord text"><span class="mord">bit</span></span><span class="mclose">)</span></span></span></span> 정보량을 가집니다. <code>특별 고양이의 성격 유형은 E 입니다.</code> 라는 정보는 후보 고양이의 수를 5% 로 줄여줌을 의미합니다</p>
<p>그렇다면 성격 유형에 대한 관측은 평균적으로 후보 고양이의 수를 얼마나 줄여줄까요? 이것이 바로 엔트로피 의 의미이며, 계산해보녀 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mtext>성격유형</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>1.86</mn><mtext>bit</mtext></mrow><annotation encoding="application/x-tex">H(성격유형)=1.86 \text{bit}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord hangul_fallback">성격유형</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">1.86</span><span class="mord text"><span class="mord">bit</span></span></span></span></span> 입니다. 성격 유형을 알면 평균적으로 약 27% 정도의 고양이를 살펴봐야하네요.</p>
<hr>
<p>정리하자면, 엔트로피는 각 사건의 정보량과 각 사건의 분포가 전부 고려해, 기댓값으로 표현한 Random Variable 의 정보량입니다. 그리고 위의 예시에서 균등하게 분포된 털 색깔에 비해서 불균등한 성격 유형은 엔트로피가 낮은 것을 알 수 있습니다. 엔트로피는 균등한 분포일수록 높습니다. (수학적으로 증명할 수 있지만 이 글을 읽고 약간 고민해보면서 직관적으로 이해해 보는 것을 추천합니다.)</p>
<p>그런데 지금까지 엔트로피를 정보량의 기댓값이라고 설명했는데 글의 처음에서도, 또 많은 정보이론 관련 글에서는 엔트로피를 불확실성이라고 설명합니다. 사실 같은 의미입니다! 모든 Random Variable 은 불확실한 만큼 정보를 기대할 수 있습니다. 이 부분도 조금 더 살  펴봅시다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="meaning-of-entropy-2">meaning of Entropy 2<a href="#meaning-of-entropy-2" class="hash-link" aria-label="Direct link to meaning of Entropy 2" title="Direct link to meaning of Entropy 2">​</a></h2>
<p>조금 다르게 표현해보면 엔트로피는 <strong>&quot;주어진 어떤 분포(고양이의 다리 개수 분포, 고양이의 털 색깔 분포)에서의 사건을 규정하기 위해서 평균적으로 몇 번의 yes/no 질문이 필요한가?</strong> 를 의미합니다. 정보량의 최소 단위인 1bit 는 yes/no 질문으로 구분 가능한 50% 의 감소비로 정의한다고 했습니다. 관측 대상(Sample space)를 50% 줄여주는 것이 1번의 yes/no 질문입니다. 2.3bit 의 정보량은 2.3번의 yes/no 질문으로 관측 대상을 20% 줄일수 있음을 의미합니다. 즉, 털 색깔 Random Variable 는 평균적으로 2.3번의 질문의 가치를 가진다는 의미이고 이는 Random Variable 의 관측결과를 확인하기 위해서는 2.3번의 질문이 필요하다는 말과 같습니다.</p>
<p>확인하기 위해 필요한 질문의 수. 이 관점에서 보면 엔트로피는 불확실성이라는 말과 어울립니다.</p>
<p>다시 한 번 고양이의 털 색깔을 맞추는 문제를 생각해봅시다.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 털 색깔이 검은색 혹은 하얀색인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: N</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 털 색깔이 치즈케이크색 혹은 회색인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: Y</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 털 색깔이 회색인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: N</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>평균적으로 고양이의 털 색깔을 맞추는데 2번~3번 사이의 질문이 필요할 것입니다. 정확하게 기댓값을 구해보면 2.3, 즉 털 색깔 Random Variable 의 엔트로피와 같을 것입니다.</p>
<hr>
<p>이번에는 성격 유형의 경우를 봅시다. 어떻게 질문하는게 최적의 질문일까요? 최대한 절반씩 가능한 화률을 확인하는게 최선의 방법입니다.(yes/no 질문)</p>
<blockquote>
<p>털색깔이 아니라, 고양이의 성격 유형도 고려해봅시다. 고양이의 성격 유형 역시 A, B, C, D, E 다섯 가지로 구별합니다. 각 성격 유형이 발생할 확률은 <code>[0.5, 0.2, 0.2, 0.05, 0.05]</code> 입니다. 털 색깔에 비해서 상당히 불균등한 분포죠? 성격 유형에 대해서도 정보량과 엔트로피를 계산해봅시다.</p>
</blockquote>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 성격이 A(확률 50%) 인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: N</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 성격이 B 혹은 D 인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: N</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 성격이 C 인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: N (정답: E)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>가끔씪은 훨씬 적은 질문으로 해결이 가능할 것입니다.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Q: 고양이의 성격이 A 인가요?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">A: Y</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>질문을 한다는 것은 일종의 관측을 하는 행위입니다. <code>특별 고양이의 털 색깔이 A 인가요?</code> 라는 질문은 털 색깔이 A 이거나(50%) A가 아니거나(50%) 의 Random Variable 로 부터 관측을 하는 행위입니다. 처음 질문이 <code>특별 고양이의 털 색깔이 B 인가요?</code> 라는 질문은 털 색깔이 B 이거나(20%) B가 아니거나(80%) 의 Random Variable 로 부터 관측을 하는 행위입니다.</p>
<p><code>특별 고양이의 털 색깔이 A 인가요?</code> 의 엔트로피는 1입니다. 이 질문은 평균적으로 Sample space 의 크기를 절반으로 줄여준다는 의미이며 같은 맥락에서 평균적으로 한번의 질문을 줄여줍니다.</p>
<p><code>특별 고양이의 털 색깔이 B 인가요?</code> 의 엔트로피 0.72 입니다. 평균적으로 0.72 번의 질문을 줄여준다는 의미죠. 만약 고양이의 털색깔이 B 라면 2.3번의 질문을 줄여주지만, B가 아니라면 0.3번의 질문만 줄이지 못합니다.</p>
<p>아까 엔트로피는 균등할 때 최대값을 가진다고 했습니다. 우리가 각 질문에서 최대한 50% 에 가까운 확률을 확인하는 방식으로 질문을 해야하는 이유는 <strong>질문의 정보량을 최대화</strong>하기 위해서입니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="meaning-of-entropy-3">meaning of Entropy 3<a href="#meaning-of-entropy-3" class="hash-link" aria-label="Direct link to meaning of Entropy 3" title="Direct link to meaning of Entropy 3">​</a></h2>
<p>다만 <strong>후보 사건의 확률이 균등하지 않은 분포에 있어서는 정보량-불확실성(질문수) 감소의 관계가 언제나 정확하게 일치하지 않습니다.</strong></p>
<hr>
<p>예를 들어서 1.5의 불확실성을 가진 분포에서, 어떤 사건을 확인했고(질문의 답을 얻었고) 이 사건의 발생 확률이 1/2 이라고 합시다. 하지만 남아 있는 불확실성은 1.5-1=0.5 보다 클수도, 작을수도 있습니다. 이는</p>
<ol>
<li>엔트로피는 어디까지나 기댓값이며</li>
<li>정보량은 엄밀하게 Sample space 가 아니라 Probability space 의 크기를 줄여주는 비율을 의미하며</li>
<li>불균등한 분포에서는 확률의 크기와 가능한 사건의 수가 반드시 비레하지 않기 떄문입니다.</li>
</ol>
<p>예를 들어서, 고양이의 성격 유형이 A, B, C, D, E, F 6개인 상황을 상정해봅시다. A, B일 확률은 1/4 이고 B, C 일 확률은 각각 1/8 입니다.</p>
<p>당연히 첫 번째 질문은 <code>특별 고양이의 성격이 A 또는 B 인가요?</code> 가 되겠죠. 만약 정답이 yes 라면 남은 후보의 갯수는 A 또는 B 입니다. 한 번의 질문만 더 하면 정답을 찾을 수 있겠네요. 하지만 정답이 no 라면 남은 후보의 갯수는 C, D, E, F 4개가 되겠죠. 적어도 두 번의 질문이 필요합니다. 이 질문이 정답인 경우와 오답인 경우는 둘다 1/2 확률로 발생할 수 있으며 그 정보량은 1로 동일하지만 두 질문의 결과에 따라서 실제로 남아 있는 질문의 수가 달라지게 됩니다.</p>
<p><strong>사건 수와 확률 수의 비례관계가 성립하지 않는 경우</strong> 불균등 분포에서 엔트로피와 정보량의 선형적인 관계를 보장할 수 없습니다.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>엔트로피와 정보량은 확률 공간에 대한 함수이며 이는 가능한 사건의 수와 각 사건들의 확률이 전부 반영한다. 둘 중 한 값에만 의존적이지 않다.</p></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>균등 분포(uniform distribution) 에서는 사건들의 확률이 전부 동일하므로 엔트로피는 질문의 정보량과 정확하게 대응하는 관계를 가진다.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example">Example<a href="#example" class="hash-link" aria-label="Direct link to Example" title="Direct link to Example">​</a></h2>
<p>마지막으로 정보와 엔트로피의 개념을 하나의 예시를 살펴보며 정리해보겠습니다. 털색깔 A, B 의 확률이 각각 0.2 이고 나머지 C, D, E, F, G, H, I, J 의 확률이 0.075인 분포를 생각해봅시다. 초기 분포의 엔트로피는 3.17 입니다.</p>
<div style="text-align:center"><img src="/assets/images/entropy_catcolor-ef41fd27c1c5d03a805c395f08312173.png" style="width:300px"></div>
<hr>
<p><code>Q: 특별 고양이의 털 색깔이 A, B, C 중에 있나요?</code></p>
<ul>
<li>질문의 확률 분포는 정답이 A,B,C 중에 있을 확률 0.475 와 없을 확률 0.525의 분포입니다.</li>
<li>이 질문의 entropy 는 1.00 입니다.</li>
<li>A,B,C 중에 답이 있는 사건의 정보량은 1.07, A,B,C 중에 답이없는 사건의 정보량은 0.93 입니다.</li>
</ul>
<p><code>A: N</code></p>
<ul>
<li>이 질문을 함으로써 얻을 정보량의 기댓값이 1.00 이었으니, 예상되는 질문 후의 불확실성은 2.17 입니다.</li>
<li>그런데 답이 없었으니 정보량은 0.93밖에 얻지 못했습니다.</li>
<li>남은 고양이 털색깔, D~J 는 확률이 약 0.14인 균등분포이며, 엔트로피는 2.81 입니다. 3.17-0.93=2.24와 차이가 있습니다. 남아있는 가능성의 수가 너무 많네요!</li>
</ul>
<hr>
<div style="text-align:center"><img src="/assets/images/entropy_catcolor2-95f95ed2522411a161cb67a6239788ba.png" style="width:300px"></div>
<p><code>Q: 특별 고양이의 털 색깔이 D,E,F,G 중에 있나요?</code></p>
<ul>
<li>질문의 확률 분 포는 정답일 확률 0.57, 오답이 확률 0.43의 확률 분포이며, 엔트로피는 0.985 입니다.</li>
<li>정답이 있을 확률의 정보량은 0.8, 없을 확률의 정보량은 1.2 입니다.</li>
</ul>
<p><code>A: N</code></p>
<ul>
<li>이 질문을 함으로써 얻을 정보량의 기댓값은 0.985 였지만 실제로 얻은 정보는 1.2 입니다.</li>
<li>남은 고양이의 털 색깔 H, I, J 는 확률이 약 1/3 인 균등 분포이며 그 엔트로피는 1.58입니다.</li>
<li>2.8-1.2=1.6 입니다. 정확하게 계산하면 두 값은 같습니다. 이는 이 때부터 분포가 균등분포, 즉 엔트로피의 크기와 가능하 사건의 수가 비례하기 떄문입니다.</li>
</ul>
<hr>
<div style="text-align:center"><img src="/assets/images/entropy_catcolor3-6237500318a65177ff2eb536105a2ca4.png" style="width:300px"></div>
<p><code>Q: 특별 고양이의 털 색깔이 J인가요?</code></p>
<ul>
<li>질문의 확률 분포는 정답일 확률 0.33, 오답일 확률 0.66의 확률분포이며 엔트로피는 0.92 입니다.</li>
<li>정답인 사건의 정보량은 1.58, 오답인 사건의 정보량은 0.58입니다.
<code>A: N</code></li>
<li>정답을 맞추었다면 정보량이 1.58-1.58 로써 0이 되었겠지만 아쉽게도 오답이네요.</li>
<li>남아있는 엔트로피는 1.0 입니다. 즉, 일반적인 상황에서는 한번의 질문만으로 정답을 맞출 수 있는 상황이네요.</li>
</ul>
<hr>
<p><code>Q: 특별 고양이의 털 색깔이 H인가요?</code></p>
<p><code>A: Y</code></p>
<ul>
<li>이 부분의 설명은 생략하겠습니다!</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reference">Reference<a href="#reference" class="hash-link" aria-label="Direct link to Reference" title="Direct link to Reference">​</a></h2>
<ol>
<li><a href="https://www.youtube.com/watch?v=v68zYyaEmEA&amp;t=636s" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=v68zYyaEmEA&amp;t=636s</a></li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/information">information</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/concepts/math/information/entropy.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/concepts/math/statistics/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Statistics</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/concepts/math/information/joint_entropy"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Joint Entropy</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#information" class="table-of-contents__link toc-highlight">Information</a></li><li><a href="#entropy-1" class="table-of-contents__link toc-highlight">Entropy</a></li><li><a href="#meaning-of-entropy-1" class="table-of-contents__link toc-highlight">Meaning of Entropy 1</a></li><li><a href="#meaning-of-entropy-2" class="table-of-contents__link toc-highlight">meaning of Entropy 2</a></li><li><a href="#meaning-of-entropy-3" class="table-of-contents__link toc-highlight">meaning of Entropy 3</a></li><li><a href="#example" class="table-of-contents__link toc-highlight">Example</a></li><li><a href="#reference" class="table-of-contents__link toc-highlight">Reference</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/community/hello">Hello, Lapis</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.instagram.com/or7l_floll/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/jhpark9701/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://leetcode.com/superstone/" target="_blank" rel="noopener noreferrer" class="footer__link-item">leetcode<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>