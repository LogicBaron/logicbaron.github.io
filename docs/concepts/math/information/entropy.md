---
id: entropy
sidebar_position: 1
tags: [information]
---
import entropy_cat from './asset/entropy_cat.png';

# Entropy

정보 이론에서 다루는 정보량과 엔트로피에 대해서 알아봅시다. 헷갈리는 내용이 많아 동어반복적인 내용이 많이 나올 듯 합니다.

## Information

먼저, 정보가 무엇인지에 대해서 정의합시다.

:::note[Think about...]
고양이를 구분하는데 어떤 사실(관측)이 더 도움이 되나요?
1. 고양이의 다리 개수가 4개다.
2. 고양이의 털 색깔이 검은색이다.
:::

고양이의 다리 개수에 대한 관측결과는 전혀 도움이 되지 않습니다. 모든 고양이는 다리 개수가 4개이니까요. 반면 고양이의 털 색깔에 대한 관측 사실은 고양이를 규정하는데 도움이 됩니다. 고양이들은 서로 다른 색깔을 가지고 있기 때문입니다. 이처럼 사건(고양이)를 규정하는데 도움이 되는 사실(관측)을 **"정보"** 라고 하며, 관측이 사건을 규정하는데 도움이 되는 정도를 **"정보량"** 이라고 표현합니다. 이러한 관점에서 고양이의 다리 개수 정보 역시 정보입니다. 단지 고양이를 규정하는데 전혀 도움이 되지 않으니 정보량이 0인 정보라고 할 수 있습니다.

관측이 사건을 규정하는데 얼마나 도움이 되는지를 어떻게 정량화할 수 있을까요? 단순한 상황에 대해서 먼저 생각해봅시다. 1000마리의 고양이 중 1마리만 털 색깔이 고양이인 경우와 1000마리의 고양이 중 500마리가 검은 털을 가진 두 가지 경우 중 `고양이의 털 색깔이 검은색이다.` 라는 정보는 어떤 경우에 고양이를 규정하는데 더 도움이 될까요? 전자의 경우 털 색깔 정보는 살펴봐야할 고양이를 1/1,000 이나 줄여버립니다. 반면 두 번째 정보는 살펴봐야 하는 고양이를 절반밖에 안 줄여줍니다. 아직도 500마리나 되는 고양이를 살펴봐야 합니다. 

살펴봐야 하는 고양이의 수를 줄여주는 비율을 정보가 도움이 되는 정도, 즉 정보량을 규정하는데 사용할 수 있습니다. 이 말을 조금 더 세련되게 표현하면 사건 공간(Sample Space)의 크기를 줄여주는 정도를 정보량의 척도로 사용할 수 있습니다. 구분의 최소 단위는 yes or no. 즉 1/2 이므로 Sample space 의 크기를 절반으로 줄여주는 경우를 **정보량의 기본 단위**로 사용하며 **1bit** 라 표현합니다.

:::note[정보량의 기본 단위]
Sample space 를 1/2 크기로 줄여주는 정보를 1bit 의 정보를 가졌다고 표현한다.
:::

섀넌(정보이론의 창시자)는 밑이 2인 로그를 이용해서 정보량을 정의합니다. 여러가지 이유가 있지만 그 중 한가지는 정보의 단위가 Smaple space 를 줄여주는 **"비율"** 로써 정의되기 때문입니다. 1/4 로 줄여주는 정보를 얻은뒤, 남은 Sample space 를 1/4로 줄여주는 정보를 얻었다면 최종적으로 얻은 정보는 1/8 도 아니고 1/2 도 아니고 1/16 일 것입니다. 통계학과 확률 이론 전반에 걸쳐서 확률과 정보의 연산은 곱셈 연산으로 이루어지는 경우가 많기 때문에 대부분 덧셈 연산으로 바꾸어주기 위해 log 를 취해서 사용합니다. 

Sample space 를 p 만큼 줄여주는 관측의 정보량은 최종적으로 다음과 같이 정의합니다.

$$
I(p) := \log_2\left(\frac{1}{p}\right) = -\log_2(p)
$$

Sample space 를 p 만큼 줄여준다는 말은, 해당 관측이 일어날 확률이 p 라는 말과 같습니다. 즉 위의 정의는 **확률이 p인 관측의 정보량** 이라고 표현할 수도 있습니다.

## Entropy

불확실한 어떠한 사건을 규정하는데 도움이 되는 관측을 **"정보"** 라 정의했습니다. 엔트로피는 규정하고자 하는 사건들의 집합, 즉 Random Variable 이 얼마나 불확실한지를 나타내는 척도입니다. 엔트로피는 이 Random Variable 이 규정되면 제거되는 불확실성, 즉 Random Variable 의 정보량의 척도이기도 합니다. 한 번에 이해하기 어려운 부분이니 간단한 예시를 살펴봅시다.

<div style={{textAlign: 'center'}}>
 <img src={entropy_cat} style={{width: 800}} />
</div>

cahtGPT 에게 고양이의 털 색깔을 물어봤습니다. 고양이가 이 중 검은색, 흰색, 회색, 주황색 그리고 치즈케이크색만 가진다고 생각합시다. 고양이가 각 털 색깔을 가진 확률은 20%로 동일합니다. 

이 털 색깔 Random Variable 의 불확실성, 엔트로피는 Random vaiable 을 이루고 있는 사건들의 평균 정보량으로 정의 합니다. 

$$ 
H  = \sum_{x \in X}{p(x)I(p(x))}  = E_{X}[I(p(X))] = E_{X}[-\log_2(p(X))] 
$$

고양이 털 색깔 Random Variable 의 엔트로피를 한 번 생각해봅시다. 각 사건의 정보량은 $\log_2(p)=\log_2(0.2)\approx 2.3 \text{bit}$ 입니다. 즉, `이 고양이의 털색은 어떠한 색입니다`` 라는 관측은 살펴봐야 하는 고양이를 $1/2^{2.3} \approx 1/5$ 로 줄여줌을 의미합니다. 고양이 털 색깔 RV 의 엔트로피는 모든 사건 정보량의 기댓값인데 모든 사건이 똑같은 확률로 똑같은 정보량을 가지므로 $H(털색깔)=2.3 (bit)$ 입니다. 

## Meaning of Entropy 1

엔트로피의 의미에 대해서 조금 더 생각해봅시다. 정보량은 어떠한 관측이 Sample space 를 줄여주는 비율을 의미합니다. Random Variable 은 이러한 관측이 일어날 확률을 나타내므로 **이 Random Variable 을 관측했을 때 기대할 수 있는 Sample space 의 감소비** 입니다. 즉 $H(털색깔)=2.3 \text{bit}$ 는 털색깔에 대한 관측은 평균적으로 확인해야하는 고양이의 수를 1/5로 줄여준다는 의미입니다.

---

털색깔이 아니라, 고양이의 성격 유형도 고려해봅시다. 고양이의 성격 유형 역시 A, B, C, D, E 다섯 가지로 구별합니다. 각 성격 유형이 발생할 확률은 [0.5, 0.2, 0.2, 0.05, 0.05] 입니다. 털 색깔에 비해서 상당히 불균등한 분포죠? 성격 유형에 대해서도 정보량과 엔트로피를 계산해봅시다.

A 성격 유형의 경우 50%의 확률이므로 정보량은 $\log_2(0.5)=1 \text{bit}$ 입니다. 살펴봐야 하는 고양이의 수가 절반으로 줄어들죠. 반면 E 성격 유형의 경우 $\log_2(p)=4.3 \text{bit} $ 정보량을 가집니다. `고양이의 성격 유형은 E 입니다.` 라는 정보는 살펴봐야 하는 고양이의 수를 5% 로 줄여줌을 의미합니다 

 그렇다면 성격 유형에 대한 관측은 평균적으로 살펴봐야 하는 고양이의 수를 얼마나 줄여줄까요? 이것이 바로 엔트로피의 의미이며, 계산해보녀 $H(성격유형)=1.86 \text{bit}$ 입니다. 성격 유형을 알면 평균적으로 약 27% 정도의 고양이를 살펴봐야하네요.

 ---

 정리하자면, 엔트로피는 각 사건의 정보량과 각 사건의 분포가 전부 고려해, 기댓값으로 표현한 Random Variable 의 정보량입니다. 그리고 위의 예시에서 균등하게 분포된 털 색깔에 비해서 불균등한 성격 유형은 엔트로피가 낮은 것을 알 수 있습니다. 엔트로피는 균등한 분포일수록 높습니다. (수학적으로 증명할 수 있지만 이 글을 읽고 약간 고민해보면서 직관적으로 이해해 보는 것을 추천합니다.)

그런데 지금까지 엔트로피를 정보량의 기댓값이라고 설명했는데 글의 처음에서도, 또 많은 문서에서 정보이론 관련 글에서 엔트로피를 불확실성이라고 설명합니다. 사실 같은 의미입니다! 모든 Random Variable 은 불확실한 만큼 정보를 기대할 수 있습니다. 이 부분도 조금 더 살펴봅시다.

 ## meaning of Entropy 2

조금 다르게 표현해보면 엔트로피는 **"주어진 어떤 분포(고양이의 다리 개수 분포, 고양이의 털 색깔 분포)에서의 사건을 규정하기 위해서 평균적으로 몇 번의 yes/no 질문이 필요한가?** 를 의미합니다. 정보량의 최소 단위인 1bit 는 yes/no 질문으로 구분 가능한 50% 의 감소비로 정의한다고 했습니다. 관측 대상(Sample space)를 50% 줄여주는 것이 1번의 yes/no 질문입니다. 2.3bit 의 정보량은 2.3번의 yes/no 질문으로 관측 대상을 20% 줄일수 있음을 의미합니다. 즉, 털 색깔 RV 는 평균적으로 2.3번의 질문의 가치를 가진다는 의미이고 이는 RV 의 관측결과를 확인하기 위해서는 2.3번의 질문이 필요하다는 말과 같습니다.

확인하기 위해 필요한 질문의 수. 이 관점에서 보면 엔트로피는 불확실성이라는 말과 어울립니다.

다시 한 번 고양이의 털 색깔을 맞추는 문제를 생각해봅시다.

```
Q: 고양이의 털 색깔이 검은색 혹은 하얀색인가요?
A: N

Q: 고양이의 털 색깔이 치즈케이크 색 혹은 회 색인가요?
A: Y

Q: 고양이의 털 색깔이 회색인가요?
A: N
```

평균적으로 고양이의 털 색깔을 맞추는데 2번~3번 사이의 질문이 필요할 것입니다. 정확하게 기댓값을 구해보면 2.3, 즉 털 색깔 RV 의 엔트로피와 같을 것입니다.

---

이번에는 성격 유형의 경우를 봅시다. 어떻게 질문하는게 최적의 질문일까요? 최대한 절반씩 가능한 화률을 확인하는게 최선의 방법입니다.(yes/no 질문)
> 털색깔이 아니라, 고양이의 성격 유형도 고려해봅시다. 고양이의 성격 유형 역시 A, B, C, D, E 다섯 가지로 구별합니다. 각 성격 유형이 발생할 확률은 [0.5, 0.2, 0.2, 0.05, 0.05] 입니다. 털 색깔에 비해서 상당히 불균등한 분포죠? 성격 유형에 대해서도 정보량과 엔트로피를 계산해봅시다.


```
Q: 고양이의 털 색깔이 A(확률 50%) 인가요?
A: N

Q: 고양이의 털 색깔이 B 혹은 D 인가요?
A: N

Q: 고양이의 털 색깔이 C 인가요?
A: N (정답: E)
```

가끔씪은 훨씬 적은 질문으로 해결이 가능할 것입니다.

```
Q: 고양이의 털 색깔이 A 인가요?
A: Y
```

질문을 한다는 것은 일종의 관측을 하는 행위입니다. `고양이의 털 색깔이 A 인가요?` 라는 질문은 털 색깔이 A 이거나(50%) A가 아니거나(50%) 의 RV 로 부터 관측을 하는 행위입니다. 처음 질문이 `고양이의 털 색깔이 B 인가요?` 라는 질문은 털 색깔이 B 이거나(20%) B가 아니거나(80%) 의 RV 로 부터 관측을 하는 행위입니다.

`고양이의 털 색깔이 A 인가요?` 의 엔트로피는 1입니다. 이 질문은 평균적으로 Sample space 의 크기를 절반으로 줄여준다는 의미이며 같은 맥락에서 평균적으로 한번의 질문을 줄여줍니다.

`고양이의 털 색깔이 B 인가요?` 의 엔트로피 0.72 입니다. 평균적으로 0.72 번의 질문을 줄여준다는 의미죠. 만약 고양이의 털색깔이 B 라면 2.3번의 질문을 줄여주지만, B가 아니라면 0.3번의 질문만 줄이지 못합니다.

 아까 엔트로피는 균등할 때 최대값을 가진다고 했습니다. 우리가 각 질문에서 최대한 50% 에 가까운 확률을 확인하는 방식으로 질문을 해야하는 이유는 **질문의 정보량을 최대화**하기 위해서입니다.

다만 복잡한 분포에 있어서는 정보량-불확실성(질문수) 감소의 관계가 언제나 정확하게 일치하지 않습니다. 예를 들어서 1.5의 불확실성을 가진 분포에서, 어떤 사건을 확인했고(질문의 답을 얻었고) 이 사건의 발생 확률이 1/2 이라고 합시다. 하지만 남아 있는 불확실성은 1.5-1=0.5 보다 클수도, 작을수도 있습니다. 이는 불균등한 분포에서는 가능한 사건의 수와 엔트로피의 크기가 반드시 비례하지 않기 떄문입니다. 

예를 들어서, 고양이의 성격 유형이 A, B, C, D, E, F 6개인 상황을 상정해봅시다. A, B일 확률은 1/4 이고 B, C 일 확률은 각각 1/8 입니다.

당연히 첫 번째 질문은 `고양이의 성격이 A 또는 B 인가요?` 가 되겠죠. 만약 정답이 yes 라면 남은 후보의 갯수는 A 또는 B 입니다. 한 번의 질문만 더 하면 정답을 찾을 수 있겠네요. 하지만 정답이 no 라면 남은 후보의 갯수는 C, D, E, F 4개가 되겠죠. 적어도 두 번의 질문이 필요합니다. 이 질문이 정답인 경우와 오답인 경우는 둘다 1/2 확률로 발생할 수 있으며 그 정보량은 1로 동일하지만 두 질문의 결과에 따라서 실제로 남아 있는 질문의 수가 달라지게 됩니다. 이 부분이 불균등 분포에서 엔트로피와 정보량의 선형적인 관계를 보장할 수 없게 만듭니다.

:::note
엔트로피는 가능한 사건의 수와 각 사건들의 확률이 전부 반영된 정보량의 기댓값으로써, 둘 중 한 값에만 비례하지 않는다.
:::

:::tip
균등 분포(uniform distribution) 에서는 사건들의 확률이 전부 동일하므로 엔트로피는 질문의 정보량과 정확하게 대응하는 관계를 가진다.
:::

## Example

털색깔 A, B 의 확률이 각각 0.2 이고 나머지 C, D, E, F, G, H, I, J 의 확률이 0.075인 분포를 생각해봅시다. 초기 분포의 엔트로피는 3.17 입니다.

---

`Q: 고양이의 털 색깔이 A, B, C 중에 있나요?`
  - 질문의 확률 분포는 정답이 A,B,C 중에 있을 확률 0.475 와 없을 확률 0.525의 분포입니다.
  - 이 질문의 entropy 는 1.00 입니다.
  - A,B,C 중에 답이 있는 사건의 정보량은 1.07, A,B,C 중에 답이없는 사건의 정보량은 0.93 입니다.

`A: N`
  - 이 질문을 함으로써 얻을 정보량의 기댓값이 1.00 이었으니, 예상되는 질문 후의 불확실성은 2.17 입니다.
  - 그런데 답이 없었으니 정보량은 0.93밖에 얻지 못했습니다.
  - 남은 고양이 털색깔, D~J 는 확률이 약 0.14인 균등분포이며, 엔트로피는 2.81 입니다. 3.17-0.93=2.24와 차이가 있습니다. 남아있는 가능성의 수가 너무 많네요!

---

`Q: 고양이의 털 색깔이 D,E,F,G 중에 있나요?`
  - 질문의 확률 분포는 정답일 확률 0.57, 오답이 확률 0.43의 확률 분포이며, 엔트로피는 0.985 입니다.
  - 정답이 있을 확률의 정보량은 0.8, 없을 확률의 정보량은 1.2 입니다.

`A: N`
  - 이 질문을 함으로써 얻을 정보량의 기댓값은 0.985 였지만 실제로 얻은 정보는 1.2 입니다.
  - 남은 고양이의 털 색깔 H, I, J 는 확률이 약 1/3 인 균등 분포이며 그 엔트로피는 1.58입니다.
  - 2.8-1.2=1.6 입니다. 정확하게 계산하면 두 값은 같습니다. 이는 이 때부터 분포가 균등분포, 즉 엔트로피의 크기와 가능하 사건의 수가 비례하기 떄문입니다.

---
`Q: 고양이의 털 색깔이 G인가요?`
  - 질문의 확률 분포는 정답일 확률 0.33, 오답일 확률 0.66의 확률분포이며 엔트로피는 0.92 입니다.
  - 정답인 사건의 정보량은 1.58, 오답인 사건의 정보량은 0.58입니다.
`A: N`
  - 정답을 맞추었다면 정보량이 1.58-1.58 로써 0이 되었겠지만 아쉽게도 오답이네요.
  - 남아있는 엔트로피는 1.0 입니다. 즉, 일반적인 상황에서는 한번의 질문만으로 정답을 맞출 수 있는 상황이네요.

---
`Q: 고양이의 털 색깔이 H인가요?`

`A: Y`

- 이 부분의 설명은 생략하겠습니다!

## Reference
1. https://www.youtube.com/watch?v=v68zYyaEmEA&t=636s