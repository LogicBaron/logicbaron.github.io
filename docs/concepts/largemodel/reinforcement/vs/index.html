<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-concepts/largemodel/reinforcement/vs" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">RLHF vs GRPO vs TTRL | Logic Baron</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" name="twitter:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" property="og:url" content="https://logicbaron.github.io/docs/concepts/largemodel/reinforcement/vs"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="RLHF vs GRPO vs TTRL | Logic Baron"><meta data-rh="true" name="description" content="RLHF (Reinforcement Learning from Human Feedback, 인간 피드백 기반 강화학습)"><meta data-rh="true" property="og:description" content="RLHF (Reinforcement Learning from Human Feedback, 인간 피드백 기반 강화학습)"><link data-rh="true" rel="icon" href="/img/logicbaron_32.ico"><link data-rh="true" rel="canonical" href="https://logicbaron.github.io/docs/concepts/largemodel/reinforcement/vs"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/concepts/largemodel/reinforcement/vs" hreflang="en"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/concepts/largemodel/reinforcement/vs" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Logic Baron RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Logic Baron Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Logic Baron JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Logic Baron" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.2ce03385.css">
<script src="/assets/js/runtime~main.392722ad.js" defer="defer"></script>
<script src="/assets/js/main.8c23e9d5.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LogicBaron</b></a><a class="navbar__item navbar__link" href="/docs/community/hello">Hello, Baron</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Concept</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/concepts/math/introduction">Math</a></li><li><a class="dropdown__link" href="/docs/concepts/mlconcept/introduction">Machine Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/deeplearning/introduction">Deep Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/largemodel/introduction">Large Model</a></li><li><a class="dropdown__link" href="/docs/concepts/programming/introduction">Programming</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/data/image/introduction">Image</a></li><li><a class="dropdown__link" href="/docs/data/text/introduction">Text</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/models/mlmodel/pca">ML Models</a></li><li><a class="dropdown__link" href="/docs/models/aimodel/intro">AI Models</a></li><li><a class="dropdown__link" href="/docs/models/largemodel/introduction">Large Models</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Practice</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/practice/torch/intro">Torch</a></li><li><a class="dropdown__link" href="/docs/practice/efficienttrain/Efficient Train">Efficient Train</a></li><li><a class="dropdown__link" href="/docs/practice/mlops/intorduction">MLOPs</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Tasks</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/tasks/recommendation/hello">Recommendation</a></li><li><a class="dropdown__link" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li><a class="dropdown__link" href="/docs/tasks/retrieval/intro">Retrieval</a></li><li><a class="dropdown__link" href="/docs/tasks/knowledgegraph/knowledgegraph">Knowledge Graph</a></li><li><a class="dropdown__link" href="/docs/tasks/llm&amp;prompt/intro">LLM &amp; Prompt</a></li></ul></div></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="theme-doc-markdown markdown"><header><h1>RLHF vs GRPO vs TTRL</h1></header>
<p>RLHF (Reinforcement Learning from Human Feedback, 인간 피드백 기반 강화학습)</p>
<p>개요</p>
<p>**Reinforcement Learning from Human Feedback (RLHF)**는 인간의 선호를 학습해 에이전트를 훈련시키는 강화학습 기법이다. 구체적으로, 인간 평가자가 모델의 출력에 대해 좋고 나쁨을 피드백하거나 순위를 매기면, 이를 바탕으로 보상 모델(reward model)을 훈련한다 ￼ ￼. 이 보상 모델은 주어진 입력-출력 쌍에 대해 인간 선호도를 점수로 예측하며, 이후 실제 에이전트(정책 모델)를 강화학습으로 미세조정하는 보상 함수로 활용된다 ￼. 일반적으로 프록시mal Policy Optimization (PPO) 등의 정책 최적화 알고리즘을 사용하여, 보상 모델이 높게 평가하는 방향으로 정책 모델의 파라미터를 업데이트한다 ￼. 이를 통해 초기 지도학습이나 사전학습으로 만든 언어모델 등을 인간의 가치관이나 선호에 맞게 조율(align)할 수 있다 ￼. RLHF는 OpenAI의 GPT 계열 모델(예: InstructGPT, ChatGPT) 등 다양한 최신 **대형 언어모델(LLM)**의 안전성과 유용성을 높이기 위해 활용되어 온 방법이다 ￼.</p>
<p>장점
•	명시적 보상 설계가 어려운 문제에 적합: RLHF는 목표를 정확히 수식화하기 어려운 과제를 인간의 직관으로 해결할 수 있게 해준다. 예를 들어 “유머러스한 문장 만들기”처럼 수학적 보상 함수를 정의하기 어려운 목표도, 사람들이 웃긴 정도를 평가하여 피드백을 주면 학습이 가능하다 ￼. 이처럼 복잡하거나 모호한 기준을 가진 작업에서도 인간의 판단을 통해 모델을 개선할 수 있다.
•	높은 성능 향상: RLHF를 적용하면 모델이 인간 선호에 부합하는 방향으로 출력을 개선하여, 기존 지도학습만으로는 얻기 어려운 품질 향상을 달성할 수 있다. 실제로 요약, 대화응답 생성, 콘텐츠 필터링 등 여러 자연어 처리 작업에서 RLHF로 모델 성능이 크게 향상되었다는 보고가 있다 ￼ ￼.
•	데이터 효율성: RLHF는 비교적 적은 양의 인간 피드백 데이터로도 효과를 볼 수 있음이 확인되었다 ￼. 소수의 비교 데이터만으로도 충분한 성능 개선을 얻을 수 있고, 보상 모델의 규모를 키우는 것이 데이터 양을 늘리는 것보다 효과적이라는 연구도 있다 ￼. 즉, 한정된 피드백 데이터로 효율적인 학습이 가능하다.
•	**모델 탐색 능력 및 강건성 향상: 인간 피드백을 통한 보상은 보통 명시적 규칙보다 다양한 상황을 포괄하므로, RLHF로 학습된 에이전트는 불확실한 상황에서의 탐색이나 예기치 않은 입력에 대한 대응이 향상되는 경향이 있다 ￼. 이는 RLHF가 **모델의 탐험(exploration)**을 장려하고 보상 신호를 통해 더 풍부한 학습 신호를 제공하기 때문이다. 그 결과 RLHF는 모델의 **견고성(robustness)**도 향상시킬 수 있는 것으로 보고된다 ￼.</p>
<p>단점
•	인적 자원 비용: RLHF의 가장 큰 단점은 고품질의 인간 피드백 수집 비용이 매우 높다는 점이다 ￼. 사람들로부터 일관성 있고 공정한 선호 데이터를 얻으려면 많은 시간과 비용이 들며, 잘못 수집된 피드백은 오히려 해로울 수 있다. 예를 들어, 특정 그룹에 편향된 사람이 피드백을 주면 모델이 그 편향을 학습할 위험이 있다 ￼.
•	보상 모델의 한계: RLHF에서는 보상 모델이 곧 학습 목표가 되는데, 이 보상 모델은 인간의 선호를 완벽히 표현하지 못할 수 있다. 그 결과 에이전트가 보상 모델을 속이거나(exploiting) 비의도적 행동을 취하는 경우가 발생할 수 있는데, 이를 흔히 **“보상 해킹”**이라고 부른다. 예를 들어 문장을 다소 부자연스럽게 만들더라도 보상 모델만 만족시키면 되는 식의 부작용이 생길 수 있다. 이러한 현상은 인간 평가자가 의도한 진짜 목표와 보상 모델의 판단 사이의 괴리에서 비롯된다.
•	학습 불안정성: 강화학습 과정 자체의 불안정성도 단점이다. 언어모델에 RLHF를 적용할 때 PPO 알고리즘의 세심한 조정이 필요하며, 잘못하면 모델 출력의 다양성이 줄고 반복적이 되는 등 품질 저하가 발생할 수 있다. 또한 정책 모델, 보상 모델, (필요하다면 가치 평가 모델) 등 여러 모델을 동시에 다뤄야 하는 복잡한 학습 파이프라인이라 구현과 튜닝이 어렵다. 특히 보상 모델의 분포에 정책이 과적합(오버피팅)되지 않도록 KL 발산 패널티 등을 걸어줘야 하는 등, PPO 기반 RLHF는 많은 기술적 트릭이 요구된다.
•	편향 및 일반화 이슈: 인간 피드백 데이터가 특정 분포나 문화에 편중되면, 모델이 그 편향을 학습해 불공정하거나 치우친 거동을 보일 수 있다 ￼. 그리고 주어진 피드백 예시에 과도하게 맞추면 일반화 성능 저하(overfitting) 위험이 있다 ￼. 이는 RLHF가 데이터 수는 적지만 영향력이 크기 때문에 발생하는 문제로, 항상 다양한 상황에 대해 균형 잡힌 피드백을 제공해야 한다.</p>
<p>사용해야 하는 상황
•	명확한 자동 평가 지표가 없는 경우: 출력의 품질을 사람만이 판단할 수 있는 경우에 RLHF가 빛을 발한다 ￼. 예를 들어 대화형 AI의 예의 바름이나 유용성, 생성된 텍스트의 사실적 정확성 등은 규칙 기반으로 점수를 메기기 어려우므로 인간 피드백을 통한 강화학습이 적합하다. 실제로 ChatGPT처럼 안전하고 친절한 챗봇이나, 편향 없이 공정한 모델을 만들 때 RLHF가 적극 활용된다.
•	모델을 인간 가치에 맞추어 정렬해야 할 때: AI의 행동을 인간 의도와 align시키는 것이 중요할 때 RLHF를 사용한다. 예를 들어 콘텐츠 필터링(유해 발언 억제), 정책 준수 등 AI가 사회적 규범을 따르게 하는 작업에 인간의 선호를 반영하여 모델을 미세조정할 수 있다. 이러한 AI 안전성 및 윤리적 측면의 조율에 RLHF가 효과적이다 ￼.
•	적은 양의 피드백 데이터로 큰 효과를 내고 싶을 때: 대량의 정답 레이블 데이터를 모으기 어려운 경우, 몇 가지 비교 선택 피드백만으로 모델 성능을 개선하고자 할 때 RLHF를 고려한다. 연구에 따르면 소량의 인간 피드백으로도 상당한 성능 향상을 얻을 수 있으므로 ￼, 데이터 효율적 개선이 필요한 프로젝트에서 유용하다.
•	기존 모델의 한계 돌파: 지도학습으로 일정 성능에 도달한 대형 언어모델을 한 단계 더 향상시키고 싶을 때 RLHF를 적용한다. 예컨대 GPT-3에서 GPT-3.5(InstructGPT)로 발전할 때 인간 피드백 강화학습을 통해 사용자가 원하는 방향으로 성능을 끌어올렸다 ￼. 이처럼 모델의 응답 품질을 한층 높여야 할 때 (특히 답변의 친절도, 유용성 등) RLHF가 효과적이다.</p>
<p>GRPO (Guided Reinforcement Policy Optimization)</p>
<p>개요</p>
<p>**Guided Reinforcement Policy Optimization (GRPO)**는 강화학습 정책 최적화 알고리즘의 하나로, 기존 PPO(Proximal Policy Optimization)를 변형하여 학습 안정성과 효율을 높인 방법이다. 2023년 말 DeepSeek 연구팀이 수학적 추론 능력을 향상시키기 위해 이 알고리즘을 도입했으며, 그들은 GRPO를 **“그룹 상대 정책 최적화 (Group Relative Policy Optimization)”**로 정의하였다 ￼. GRPO의 핵심 아이디어는 기존의 가치함수(critic) 모형을 없애고, 대신 여러 후보 행동의 상대적인 품질로 이득(advantage)을 계산한다는 점이다 ￼. 예를 들어 정책이 어떤 질문에 대해 여러 개의 답변을 샘플링하면, 그 출력들 사이의 상대적 우수함을 평가하여 어떤 답이 더 나은지 판단한다 ￼. 이렇게 그룹 내 비교를 통해 얻은 보상 신호를 사용하여 정책을 업데이트함으로써, 별도의 가치망 없이도 정책의 개선 방향을 안내(guided)할 수 있다. 요약하면, GRPO는 PPO 대비 간소화된 강화학습 알고리즘으로, 다수의 샘플 비교를 통한 보상 설계를 통해 메모리 사용량을 최적화하고 복잡한 추론 과제를 더 잘 학습하도록 고안되었다 ￼.</p>
<p>장점
•	가치 네트워크 불필요로 인한 단순화: GRPO는 가치 평가 모델(critic)을 제거하여 학습 파이프라인을 단순화한다 ￼. 그 대신 정책 자체의 여러 시도(output) 간 비교로 이득(advantage)을 추정하므로, 별도의 가치망 학습과정이 없어 메모리와 계산 비용을 절감한다. PPO에서는 정책 모델 외에 동일한 크기의 가치 모델을 추가로 훈련해야 하지만, GRPO에서는 이 부분이 생략되어 훈련이 가볍고 빠를 수 있다 ￼.
•	추론 능력 향상에 유리: 여러 출력을 비교하는 GRPO의 전략은 특히 복잡한 추론이나 수학 문제 등 정답의 절대적 기준을 알기 어려운 작업에 효과적이다 ￼. DeepSeek 팀은 복잡한 수학적 추론에서 GRPO가 PPO보다 모델의 논리적 사고(chain-of-thought) 향상에 도움을 주었다고 보고하였다. 이는 여러 시도 중 상대적으로 더 나은 답을 강화하는 방식이, 단일 추론 경로만 강화하는 것보다 모델로 하여금 깊은 추론을 탐색하도록 유도하기 때문이다.
•	학습 안정성 및 수렴 속도: GRPO는 정책에 대한 가이드 역할을 해주는 참조 신호를 도입함으로써 학습을 안정화할 수 있다는 장점이 있다 ￼. 일부 구현에서는 정책의 이전 출력들을 그룹 단위로 정규화하여 보상을 계산하기 때문에, 극단적인 업데이트를 피하고 완만한 수렴을 유도한다. 이는 PPO의 KL 패널티 등과 유사한 안전장치 역할을 수행하면서도 보다 직관적인 방식으로 정책을 안내한다는 평가가 있다. 실제로 GRPO는 적은 자원으로도 실험과 실시간 조정이 용이하도록 설계되어 있어, 대규모 자원이 없는 환경에서도 실용적인 강화 미세조정을 시도할 수 있게 한다 ￼.
•	인간 피드백 없이도 사용 가능: GRPO 자체는 보상 신호만 정의되면 동작하므로, 반드시 인간 피드백이 아니더라도 적용할 수 있다. 예를 들어 DeepSeek는 지도학습 없이 순수 RL로 거대 언어모델을 훈련할 때 GRPO를 활용했다 ￼. 특히 **검증 가능 보상(verifiable reward)**이 있는 경우 (예: 수학 문제의 정답 체크, 프로그램 컴파일 성공 여부 등) 보상모델 없이 GRPO로만 학습이 가능하여, 완전히 새로운 형태의 강화학습 (예: RLVR: Reinforcement Learning with Verifiable Rewards)에도 활용되었다 ￼. 이런 유연성 덕분에, RLHF 파이프라인의 한 구성요소로서 PPO 대비 대체제로 쓰이거나, 완전히 별도의 RL 시나리오에도 응용될 수 있다.</p>
<p>단점
•	상대 평가의 한계: GRPO는 여러 샘플을 생성하여 비교해야 하므로, 각 단계마다 추가적인 생성 비용이 든다. 한 입력에 대해 하나의 출력만 평가하는 PPO와 달리, GRPO는 N개의 출력을 뽑아 그들 간 우열을 가리는 작업이 필요하므로 훈련 시간이 늘어나고 계산 자원 소모가 커질 수 있다. 또한 출력 사이의 품질 비교가 항상 명확하지 않을 수 있다. 만약 생성된 후보들이 모두 나쁘거나 모두 좋다면 상대적인 구분이 어려워 보상 신호가 애매해지고, 이로 인해 학습 안정성이 떨어질 위험이 있다.
•	보상 신호 의존도 및 설계 난이도: GRPO에서도 어떤 기준으로 출력의 상대 품질을 평가할지 보상 함수를 설계해야 한다. 일반적으로는 보상 모델 또는 규칙 기반 평가가 필요한데, 만약 이 보상 신호가 부정확하면 잘못된 방향으로 강화될 수 있다. PPO의 경우 절대적인 보상 값과 가치망의 보조로 오류를 완화하지만, GRPO는 상대적 랭킹만으로 의존하기 때문에 보상 함수 설계에 실패하면 전체 학습이 흔들릴 수 있다. 예를 들어, 잘못된 기준으로 다수결 투표를 한다면 오히려 바람직하지 않은 출력을 강화할 위험이 있다.
•	제한된 검증과 채택: GRPO는 비교적 최근 제안된 알고리즘이어서 PPO만큼 폭넓게 검증되지는 않았다. PPO는 여러 작업에 걸쳐 안정성과 성능이 입증되었지만, GRPO는 **특정한 시나리오(예: 수학 문제)**에서 주로 효과가 보고되었다. 실제 오픈소스 RLHF 프레임워크에서도 GRPO의 초기 버전은 불안정성이 지적되어 이를 개선한 **변형 알고리즘(REINFORCE++ 등)**이 나오기도 했다 ￼. 따라서 현 단계에서 GRPO를 사용할 경우 하이퍼파라미터 튜닝, 샘플 개수 N 결정 등 해결해야 할 실무적인 감각이 필요하며, 잘못 설정할 경우 PPO보다 나쁜 결과를 얻을 수도 있다.
•	적용 범위: 출력 간 상대 평가가 의미 있으려면 하나의 질문에 대한 여러 출력의 우열을 비교할 수 있어야 한다. 이는 보통 정답이 명확하거나 평가 척도가 분명한 작업에 적합하고, 열린 도메인 대화처럼 정답의 개념이 모호한 경우에는 GRPO의 설계가 까다로울 수 있다. 또한, 여러 샘플을 생성하는 방법이기 때문에 실시간 또는 상호작용 환경(예: 로봇 제어)에는 적용하기 힘들고, 주로 오프라인 배치 학습 환경에 한정된다는 한계가 있다.</p>
<p>사용해야 하는 상황
•	PPO의 대안이 필요한 RLHF 시나리오: RLHF 파이프라인에서 PPO의 복잡성이나 과도한 KL 제약 등이 문제가 될 경우, GRPO를 대안으로 고려할 수 있다 ￼. 예를 들어 소규모 모델를 인간 피드백으로 미세조정할 때 PPO보다 구현이 쉽고 튜닝이 간편한 방법을 찾는다면 GRPO가 유용하다. 실제 Hugging Face 등 오픈소스 커뮤니티에서도 RLHF를 위한 새로운 알고리즘으로 GRPO를 도입하여 실험과 교육 자료를 제공하고 있다 ￼.
•	복잡한 추론 능력 배양: 모델에게 여러 단계를 거치는 이유 있는 추론(chain-of-thought)을 학습시키고 싶을 때 GRPO가 효과적일 수 있다. DeepSeek 팀의 사례처럼, 수학 문제 풀이, 프로그래밍 문제 해결 등 정답을 맞히는 과정에서 다양한 시도를 비교하며 최적화하는 상황에서 GRPO를 활용하면 모델의 추론 깊이와 정확성을 향상시킬 수 있다 ￼. 즉, 모델 스스로 여러 해답을 모색하면서 최적해를 찾아가는 강화학습을 원한다면 GRPO가 적합하다.
•	보상모델이나 값함수를 학습시키기 어려울 때: 만약 별도의 **가치망(critic)**을 훈련시키기 힘든 환경(예: 메모리 한계)이라면 GRPO로 정책만 학습하는 것을 고려한다. 혹은 명시적 보상 모델 없이도 평가가 가능한 경우(예: 프로그램 실행 성공 여부, 산출물의 논리 검증 가능 여부 등) GRPO를 쓰면 최소한의 구성요소로 RL을 구현할 수 있다 ￼ ￼. 이처럼 학습 인프라를 간소화해야 하거나, 검증 가능 자동 피드백이 존재하는 문제라면 GRPO가 효율적이다.
•	순수 강화학습으로 LLM 훈련: 지도학습 데이터 없이 오로지 강화학습 신호만으로 LLM을 키우는 실험에도 GRPO가 활용된다. 예를 들어 중국의 DeepSeek R1 모델은 지도 미세조정 없이 GRPO 기반 RL로만 학습되어 공개되었는데 ￼, 이처럼 인간 레이블 없이 에이전트를 훈련해야 하는 상황에서 GRPO와 적절한 보상설계(예: 정답 검증기)를 조합하면 돌파구를 마련할 수 있다 ￼. 새로운 환경에 적응하거나 자체적으로 발전하는 에이전트를 만들고자 할 때 이러한 접근이 탐구된다.</p>
<p>TTRL (Test-Time Reinforcement Learning, 테스트 시 강화학습)</p>
<p>개요</p>
<p>**Test-Time Reinforcement Learning (TTRL)**은 모델이 테스트 단계에서 자체적으로 강화학습을 통해 적응하도록 하는 새로운 개념의 방법이다. 일반적으로 모델은 **훈련(Train)**이 끝나면 테스트(Test) 시에는 파라미터가 고정되지만, TTRL에서는 테스트 데이터를 이용해 모델 파라미터를 온라인으로 업데이트한다는 특징이 있다 ￼ ￼. 특히 레이블이 없는(test) 데이터에 대해 모델이 스스로 보상을 추론하여 학습하는 것이 핵심인데, PRIME-RL 팀은 그 해결책으로 다수결 투표 기반 보상 추정을 제시하였다 ￼. 구체적으로, 모델이 주어진 테스트 문제에 대해 여러 개의 출력을 생성한 뒤, 그 중 가장 많이 등장한 답을 잠정 정답으로 간주한다. 그리고 각 출력이 그 다수결 정답과 얼마나 일치하는지를 기반으로 보상(rule-based reward)을 계산하여 모델을 업데이트한다 ￼ ￼. 이렇게 하면 정답 레이블 없이도 모델이 자신의 출력을 평가하여 학습할 수 있다. TTRL은 본질적으로 사전 학습된 LLM의 내재된 지식(prior)을 활용하여, 추론 일관성을 보상으로 삼음으로써 **자기 진화(self-evolution)**를 가능케 한다 ￼. 그 결과, TTRL을 적용한 모델은 처음 주어진 성능 한계를 넘어 지속적으로 향상될 수 있다. 예를 들어, TTRL 기법은 Qwen-2.5-Math-7B 모델의 수학 문제 정확도를 159% 향상시켜, 레이블 없이도 레이블이 있는 경우에 버금가는 성능까지 끌어올렸다 ￼.</p>
<p>TTRL의 개요: 테스트 데이터(예: 질문 q)에 대해 LLM이 여러 답안을 생성(ŷ1, ŷ2, … ŷM). 다수결 투표로 가장 빈도가 높은 예측 y를 **추정 정답(label)**으로 삼는다. 각 출력 ŷ와 추정 정답 y의 일치도를 기반으로 보상 *R(ŷ, y)를 계산한 뒤, 이 보상으로 정책 최적화를 수행하여 LLM의 파라미터를 업데이트한다 ￼ ￼.</p>
<p>장점
•	레이블 없이 성능 향상: TTRL의 가장 큰 장점은 별도의 정답 레이블이나 인간 피드백 없이도 모델 성능을 높일 수 있다는 것이다. 기존에는 새로운 데이터에 대응하려면 추가 라벨링이나 미세조정이 필요했지만, TTRL은 모델 스스로 평가한 보상만으로 향상되므로 비용이 들지 않는다. 이는 **사후학습(post-training)**의 패러다임 전환으로, 테스트 단계에서도 러닝을 지속함으로써 배포 후 모델 성능 저하를 완화하고 지속적으로 개선시킬 수 있다.
•	극적인 성능 개선 및 상한 돌파: 연구 결과에 따르면 TTRL 적용 시 초기 모델의 한계를 뛰어넘는 성능 향상이 관찰되었다 ￼. 예를 들어 AIME 2024 벤치마크에서 TTRL은 초기 모델을 크게 능가하여, 마치 테스트 데이터에 레이블을 달아 직접 학습시킨 것에 근접한 성과를 냈다 ￼. 이는 모델이 테스트 분포에 직접 적응하기 때문에 가능한 것으로, distribution shift가 큰 상황에서도 높은 성능을 낼 수 있는 잠재력을 보여준다.
•	일반성 및 범용성: TTRL은 특정 작업에 국한되지 않고 다양한 작업과 모델에 걸쳐 일관된 개선을 보였다고 보고된다 ￼. 보상 신호만 적절히 설계하면, 자연어 문제 풀이, 코드 생성, 수리적 추론 등 여러 도메인에서 적용 가능하다. 또한 사전학습된 어떤 LLM이라도 테스트 시 추가 학습이 가능하므로, 향후 모델 배포 및 업그레이드 방식에 폭넓은 응용이 기대된다. 모델이 새로운 도메인이나 사용자 데이터에 알아서 학습하도록 할 수 있다는 점에서, **연속학습(continual learning)**의 한 형태로 볼 수도 있다 ￼ ￼.
•	모델의 자기 평가 능력 활용: 다수결 투표, 자가검증 등 TTRL의 보상 기법은 모델이 자신의 불확실성을 감소시키도록 유도한다. 모델이 여러 번 시도해서 일관된 답을 내놓는 방향으로 강화되므로, 최종적으로 답의 신뢰도가 높아진다. 이는 곧 모델 내부에 암묵적으로 존재하는 지식을 끌어내는 효과로 이어진다. 예를 들어, 모델이 알고는 있지만 한 번의 샘플링에 드러나지 않았던 정답도 반복 시도 중 표출되어 다수결로 채택될 수 있다. 이런 방식으로 TTRL은 모델 내재 지식의 활용도를 극대화한다는 장점이 있다 ￼.
•	온라인 학습으로 인한 적응력: TTRL은 테스트 시점을 학습 기회로 활용하므로, 모델이 실시간으로 환경 변화에 적응할 수 있다. 예를 들어 시간이 지남에 따라 문제 스타일이 바뀌거나 새로운 유형의 질문이 등장해도, 모델이 테스트 과정에서 이에 맞추어 파라미터를 업데이트하면 추가 재훈련 없이도 대응할 수 있다 ￼ ￼. 이처럼 학습과 추론의 경계를 허물어 모델이 자율적으로 진화하게 함으로써, 고정된 모델보다 환경 변화에 훨씬 강인한 시스템을 구축할 수 있다.</p>
<p>단점
•	추론 비용 증가: TTRL은 한 번의 질문에 대해 다수의 출력 생성 및 반복적 학습 업데이트를 동반하므로, 기존 고정 모델에 비해 추론 비용(시간, 메모리)이 크게 증가한다. 예를 들어 다수결을 위해 한 질의에 대해 5~10번의 답변을 생성하고, 그에 따른 보상 계산과 그래디언트 업데이트까지 해야 한다면, 실시간 응답이 필요한 애플리케이션에는 부적합할 수 있다. 특히 대형 모델의 경우 테스트 시의 이런 추가 연산이 부담되므로, 실용성을 위해 경량화나 간헐적 적용 등의 전략이 필요하다.
•	보상 신호 신뢰도 문제: TTRL에서 사용하는 **자가 보상(self-derived reward)**은 어디까지나 추정치이므로, 잘못된 방향으로 모델을 이끌 위험이 있다. 예를 들어 모델의 지식 한계로 다수결 자체가 틀린 답을 합의해버린다면, 오히려 그 오답을 강화하는 결과가 된다. 초기 모델이 충분한 성능을 발휘하지 못하는 경우 이러한 자기강화 편향이 발생할 수 있다. 따라서 TTRL 보상이 항상 옳다는 보장이 없으며, 잘못된 합의에 빠지지 않도록 때로는 인간 검수나 외부 신호와 병행하는 것이 안전할 수 있다.
•	특정 과제에의 한계: TTRL이 효과를 발휘하려면 다수결 등으로 추정 정답을 얻을 수 있는 설정이어야 한다. 하지만 모든 문제가 다수결로 풀리지는 않는다. 예를 들어 창의적인 문장 생성이나 개방형 대화 응답의 품질은 동일한 질문에 반복 생성한다고 해서 객관적 정답이 나오는 것이 아니다. 이런 경우 TTRL의 보상 개념을 적용하기 어렵다. TTRL은 주로 객관식 문제나 정답이 존재하는 과제(수학 문제, 퀴즈 등)에 적합하며, 주관적이거나 열려있는 생성 과제에는 바로 적용하기 힘들다.
•	테스트-훈련 경계 모호로 인한 검증 어려움: 모델이 테스트 시에도 업데이트를 한다는 것은, 전통적인 의미의 테스트 셋에 대한 평가 개념이 흐려질 수 있다는 의미다. 예컨대 TTRL을 적용하면 모델이 테스트 데이터를 학습해버리므로, 일반적인 기준의 일반화 성능 측정이 어려워진다. 이는 연구나 대회에서 엄밀한 비교를 위해서는 사용하기 어렵다는 뜻이며, 과적합의 개념도 재고해야 한다. 또한 배포 중인 모델이 지속 변경되므로, 버그나 악성 업데이트 발생 시 추적 및 롤백이 복잡해지는 운영상의 단점도 있다.
•	구현 복잡성: TTRL을 실제 시스템에 도입하려면, 온라인 학습 인프라가 뒷받침되어야 한다. 즉, 모델 서비스 중에 사용자 입력을 모아 실시간으로 또는 주기적으로 모델을 업데이트하는 파이프라인을 구축해야 하는데, 이는 기존 일괄 배포 방식보다 복잡하고 위험도 관리해야 한다. 또한 계속 학습하는 모델의 학습률, 업데이트 빈도 등을 어떻게 설정할지에 대한 노하우도 아직 부족하다.</p>
<p>사용해야 하는 상황
•	레이블 없이 모델을 개선해야 하는 경우: 대규모의 미라벨(test) 데이터가 있으나 레이블링이 불가능하거나 너무 비쌀 때, TTRL은 그 데이터를 활용하여 모델 성능을 높이는 훌륭한 방법이 된다 ￼ ￼. 예컨대 실사용 중에 쌓이는 대화 로그, 유저 피드백이 없는 검색 질의 등에서 모델이 스스로 학습하도록 하여 지속적인 품질 개선을 추구할 수 있다. 이런 상황에서 TTRL은 데이터 활용 효율을 극대화하는 방법이다.
•	테스트 분포가 학습 분포와 많이 다른 경우: 모델이 훈련 때 보지 못한 배포 후의 실제 데이터 분포가 사뭇 다르면, TTRL을 통해 배포 시점에서 추가 학습을 함으로써 도메인 적응을 수행할 수 있다 ￼ ￼. 이를테면, 새로운 유형의 질문이 등장하는 시험 대회에 참가하는 모델이라면, 초기 몇 문제를 풀면서 TTRL로 자체 개선하여 이후 문제들을 더 잘 풀도록 할 수 있다. 즉, 훈련-테스트 분포 차이를 온라인 학습으로 메꾸는 용도로 TTRL을 활용한다.
•	모델의 지속적 업데이트가 필요한 서비스: AI 모델을 한 번 배포하고 끝내지 않고, 지속적으로 향상시키는 제품 전략을 취하는 경우 TTRL이 적합하다. 예를 들어 맞춤형 개인비서 AI가 사용자의 취향을 점점 더 잘 학습하도록 하거나, 자율 주행 AI가 주행 중 만나는 새로운 상황에 적응하도록 하는 등 라이브 환경에서 러닝이 필요한 경우이다. 이러한 맥락에서 TTRL은 연속 학습 프레임워크로서, 서비스 운영 중에 발생하는 피드백(설령 그것이 명시적 보상은 아니더라도)을 바로바로 모델 개선에 활용하는 기반을 제공한다.
•	대규모 사전학습 후 파인튜닝 데이터가 부족할 때: 최근 LLM들은 거대한 사전학습을 거친 후 비교적 적은 양의 지도 데이터로 미세조정되는데, 만약 그 지도데이터가 충분치 않다면 테스트 단계의 추가 학습으로 보완할 수 있다. TTRL은 사전학습으로 형성된 모델의 잠재력을 최대로 끌어내는 보조 수단이 될 수 있다. 예를 들어 수학능력이 뛰어나게 잠재지식을 쌓은 모델이 있다고 할 때, 이에 대한 레이블된 파인튜닝 데이터가 부족하더라도 테스트 시 다수결 보상으로 능력을 끌어올릴 수 있다 ￼. 이처럼 학습 부족을 현장에서 메꾸는 상황에서 TTRL이 유용하다.</p>
<p>비교: RLHF vs GRPO vs TTRL</p>
<p>각 접근법의 특징과 장단점을 한눈에 볼 수 있도록 정리하면 다음과 같다.</p>
<p>항목	RLHF (인간 피드백 강화학습)	GRPO (Guided/Group Reinforcement Policy Optimization)	TTRL (Test-Time Reinforcement Learning)
핵심 아이디어	- 인간 평가자를 통해 모델 출력의 선호도를 수집하고, 이를 바탕으로 보상 모델을 학습한 후 정책을 강화학습으로 조정함. 인간 선호도를 직접 학습에 활용하여 모델을 인간 가치에 align시킴 ￼ ￼.- 전형적으로 PPO 알고리즘 등을 사용하여, 사람의 랭킹에서 파생된 보상 함수를 극대화하도록 모델을 업데이트함 ￼.	- PPO를 변형한 정책 최적화 알고리즘으로, 여러 샘플의 상대적인 품질 비교를 통해 정책을 업데이트함. 가치망(critic)을 제거하고 정책의 자체 출력을 그룹으로 평가하여 이득(advantage)을 계산함 ￼.- DeepSeek 등에서 제안되어 수학적 추론 향상에 활용. **“Guided”**라는 이름처럼, 정책이 스스로 산출한 최선의 행동을 지향하도록 안내하는 RL 기법 (그룹 내 다수의 출력 비교로 보상 설계).	- 테스트 단계에서 모델을 계속 학습시키는 방법론. 레이블 없는 테스트 데이터에 대해 모델이 여러 번 시도해보고, 다수결 등 규칙으로 보상을 얻어 온라인으로 파라미터를 업데이트함 ￼.- **자기 강화(self-training)**의 일종으로, 모델이 추론 단계에서도 학습하여 배포 후 스스로 진화하도록 하는 프레임워크. 특히 대형 LLM의 추론 일관성을 활용해 보상 신호를 생성함 ￼.
장점	- 명시적 지시 어려운 목표(유용성, 유머 등)에 인간 직관으로 대응 가능 ￼. 모델을 인간 선호에 맞춰 안전하고 유용하게 만들 수 있음.- 적은 피드백 데이터로도 효과적으로 성능 개선 ￼. 인간 평가 몇 백건으로도 큰 개선 가능.- 다양한 NLP 작업에 적용 가능 (요약, 대화, 이미지 생성 등). 이미 많은 최신 모델에 적용되어 검증되었음 ￼.- 모델의 탐색 능력 향상 및 강건성 증가 (인간 피드백이 포괄적인 보상으로 작용하여 불확실한 상황 대처 향상) ￼.	- 가치망 불필요로 학습 파이프라인이 단순하고 메모리 효율적 ￼. 동일 자원으로 더 큰 모델 또는 더 빠른 학습 가능.- 상대 평가를 통해 복잡한 추론에 강함. 여러 시도를 비교하므로 모델이 깊이 있는 사고 과정을 거치도록 유도 ￼.- PPO 대비 하이퍼파라미터 튜닝이 쉬울 가능성 (KL 패널티 등 복잡성 감소). 오픈소스 구현이 등장하여 실험과 활용이 확산됨 ￼.- 보상 모델 없이도 사용 가능한 유연성. 검증 가능한 환경에서는 순수 RL로도 대안이 됨 (DeepSeek-R1-Zero 사례) ￼ ￼.	- 레이블 없이 성능 개선 가능. 추가 데이터 라벨링 비용 없이 모델을 테스트 데이터에 맞춰 향상 ￼ ￼.- 배포 후 지속 학습으로 도메인 변화에 적응 가능. 새로운 유형의 입력이나 환경 변화에 실시간 대응 ￼ ￼.- 기존 모델 성능 상한 돌파: 초기 모델의 한계를 넘는 극적인 향상 (자기 진화 효과) ￼. 특히 추론 일관성 향상을 통해 정확도 대폭 증대.- 다양한 모델/작업에 적용되는 일반성. 규칙만 잘 정하면 어떤 LLM에도 응용 가능하고, 연속학습 프레임워크로 활용되어 AI 서비스의 업그레이드 방법 혁신.
단점	- 인간 피드백 수집 비용이 큼. 전문 지식이 필요한 평가일수록 비용 상승 ￼. 잘못된 피드백시 편향 학습 위험 ￼.- 보상 모델의 한계로 인한 위험. 보상 모델을 만족시키려다 본래 의도와 다른 부작용(유도된 어색한 문장 등) 발생 가능.- RL 학습의 불안정성: 잘못 설정하면 모델 품질 저하나 모놀리틱한 답변 가능. 정책모델+보상모델(+가치모델) 등 구현 복잡.- 편향 및 일반화 문제: 피드백이 편향될 경우 모델이 치우치게 되고, 소수 피드백에 과적합 우려 ￼.	- 다수 샘플 생성으로 인한 비용: 한 입력당 여러 출력을 평가해야 하므로 훈련 계산량 증가. 실시간/대화 상황엔 부적합.- 보상 함수 설계 난이도: 출력들간 우열을 판단할 기준(보상모델 혹은 heuristic)이 부정확하면 잘못된 강화 위험. 보상신호의 신뢰성이 관건.- 검증 사례 축적 부족: 새로운 기법이라 PPO만큼 일반적 성공 보장 어렵고, 안정성 이슈가 있을 수 있음. 최적 성능 위해선 세심한 튜닝 필요.- 적용 한계: 답의 상대적 우수성 평가가 힘든 개방형 생성 작업에는 적용 어려움. 주로 객관적 평가 지표가 있는 과제에 한정.	- 추론 지연 및 비용 증가: 테스트 때 여러 번 생성+업데이트하여 응답시간 상승. 대형모델 서비스에 부하.- 자기 보상 신뢰도: 모델이 잘못 판단한 결과를 다수결로 강화하면 오히려 오답 학습. 초기 모델 성능이 낮으면 자기강화 편향 우려.- 적용 도메인 제약: 정답 검증이 어려운 창의적/주관적 작업에는 보상 설계가 곤란. 다수결이 통하지 않는 경우 한계.- 평가 및 운영 복잡: 테스트 셋을 학습해버리므로 일반화 평가 모호. 배포 중 모델이 계속 변해 검증 및 버전관리 어려움. 온라인 학습 인프라 요구.
언제 활용하는가 (적합한 상황)	- 모델 정렬(alignment) 필요할 때: AI를 인간의 가치관/선호와 부합시키고자 할 때 필수적인 기법. 예: 대화봇의 유해발언 제어, 콘텐츠 추천의 개인화 등 인간 판단이 중요한 경우.- 자동 평가 어려운 과제: 정답을 공식화하긴 어렵지만 사람은 쉽게 평가하는 과제(유머, 창의성 등) ￼. 이런 감성적/주관적 기준의 작업에 RLHF가 효과적.- 피드백 데이터가 적을 때: 큰 라벨링 없이도 일부 비교 데이터로 성능을 높이고자 할 때. 초기 모델 성능을 끌어올리는 후처리 미세조정 단계로 많이 사용. 예: ChatGPT, InstructGPT 개발시.	- PPO의 대안이 필요할 때: PPO의 KL 제한 등 복잡성을 피하고 싶거나, PPO로 안정적 학습이 어려울 때 실험적으로 도입 ￼. 오픈소스 RLHF에서 경량 대체 알고리즘으로 각광.- 복잡한 문제의 강화학습: 체스, 수학 풀이처럼 여러 시도 중 최선 찾기 문제가 있을 때. 모델이 자체 생성 평가를 통해 깊이 있는 답을 찾도록 유도하고 싶을 때.- 가치망/보상모델 없이 학습해야 할 때: 메모리나 데이터 제약으로 추가 모델 학습이 힘든 경우. 혹은 정답 검증 함수가 존재하는 환경(컴파일 성공 등)에서 최소한의 구성으로 RL 구현 시. ￼ ￼- 순수 RL로 LLM 훈련 실험: 지도데이터 없이 RL만으로 언어모델을 키우는 연구에 활용. (예: DeepSeek-R1-Zero 모델을 GRPO로만 학습) ￼.	- 실서비스에서 모델 개선을 이어가야 할 때: 배포 후 사용자 데이터가 쌓이는 온라인 서비스에서, 모델을 계속 업데이트해 품질을 높이고자 할 때. (예: 개인화 비서가 사용자와 대화하며 학습)- 도메인/환경 변화가 잦을 때: 학습 때 보지 못한 새로운 유형의 입력이나 문제가 나타나는 상황. TTRL로 현장에서 도메인 적응 수행 ￼ ￼.- 라벨링 불가한 테스트 데이터 활용: 대량의 미라벨 데이터를 그냥 두지 않고 모델 향상에 쓰고 싶을 때. (예: 대회 테스트 세트, 운영 중 수집 데이터 등에서 자체 학습) ￼.- 모델 성능 한계 타파 필요: 추가 지도데이터는 없지만 더 높은 성능이 요구될 때, 모델 자체 지식을 최대 활용하는 수단으로 TTRL 시도.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/concepts/largemodel/reinforcement/vs.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"></nav></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/community/hello">Hello, Lapis</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.instagram.com/or7l_floll/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/jhpark9701/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://leetcode.com/superstone/" target="_blank" rel="noopener noreferrer" class="footer__link-item">leetcode<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>