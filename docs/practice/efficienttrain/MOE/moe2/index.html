<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-practice/efficienttrain/MOE/moe2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Adaptive Attention Span in Transformer | Logic Baron</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" name="twitter:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" property="og:url" content="https://logicbaron.github.io/docs/practice/efficienttrain/MOE/moe2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Adaptive Attention Span in Transformer | Logic Baron"><meta data-rh="true" name="description" content="Transformer 계열 모델의 구조는 사실상 Mixture of Experts 를 내포하고 있습니다. MHA 모듈의 각 head 는 concat 된 뒤 linear layer 를 거치면서 서로 다르게 활성화됩니다. 엄밀하게 MOE와 정확히 같지는 않고, 보다 복잡하고 세세하게 구성된 MOE 라고 볼 수 있습니다. 사실 Transformer 내부에는 다양한 &quot;전문가&quot; 를 만들어내려는 철학이 많이 녹아있어요."><meta data-rh="true" property="og:description" content="Transformer 계열 모델의 구조는 사실상 Mixture of Experts 를 내포하고 있습니다. MHA 모듈의 각 head 는 concat 된 뒤 linear layer 를 거치면서 서로 다르게 활성화됩니다. 엄밀하게 MOE와 정확히 같지는 않고, 보다 복잡하고 세세하게 구성된 MOE 라고 볼 수 있습니다. 사실 Transformer 내부에는 다양한 &quot;전문가&quot; 를 만들어내려는 철학이 많이 녹아있어요."><link data-rh="true" rel="icon" href="/img/logicbaron_32.ico"><link data-rh="true" rel="canonical" href="https://logicbaron.github.io/docs/practice/efficienttrain/MOE/moe2"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/practice/efficienttrain/MOE/moe2" hreflang="en"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/docs/practice/efficienttrain/MOE/moe2" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Logic Baron RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Logic Baron Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Logic Baron JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Logic Baron" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.26e27a6e.css">
<script src="/assets/js/runtime~main.f7073dba.js" defer="defer"></script>
<script src="/assets/js/main.fe665110.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LogicBaron</b></a><a class="navbar__item navbar__link" href="/docs/community/hello">Hello, Baron</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Concept</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/concepts/math/introduction">Math</a></li><li><a class="dropdown__link" href="/docs/concepts/mlconcept/introduction">Machine Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/deeplearning/introduction">Deep Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/largemodel/introduction">Large Model</a></li><li><a class="dropdown__link" href="/docs/concepts/programming/introduction">Programming</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/data/image/introduction">Image</a></li><li><a class="dropdown__link" href="/docs/data/text/introduction">Text</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/models/mlmodel/pca">ML Models</a></li><li><a class="dropdown__link" href="/docs/models/aimodel/intro">AI Models</a></li><li><a class="dropdown__link" href="/docs/models/largemodel/introduction">Large Models</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Practice</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/practice/torch/intro">Torch</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/practice/efficienttrain/Efficient Train">Efficient Train</a></li><li><a class="dropdown__link" href="/docs/practice/mlops/intorduction">MLOPs</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Tasks</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/tasks/recommendation/hello">Recommendation</a></li><li><a class="dropdown__link" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li><a class="dropdown__link" href="/docs/tasks/retrieval/intro">Retrieval</a></li><li><a class="dropdown__link" href="/docs/tasks/knowledgegraph/knowledgegraph">Knowledge Graph</a></li><li><a class="dropdown__link" href="/docs/tasks/llm&amp;prompt/intro">LLM &amp; Prompt</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/practice/efficienttrain/Efficient Train">Efficient Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/practice/efficienttrain/1">ZERO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/practice/efficienttrain/zero_infinity">ZERO INFINITY</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/practice/efficienttrain/MOE/moe1">MOE</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/practice/efficienttrain/MOE/moe1">Sparsely-Gated MOE Layer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/practice/efficienttrain/MOE/moe2">Adaptive Attention Span in Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/practice/efficienttrain/MOE/gshard">GShard</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/practice/efficienttrain/MOE/switchformer">Switch Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/practice/efficienttrain/MOE/moe3">Efficient MOE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/practice/efficienttrain/MOE/mistralmoe">Mistral MOE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/practice/efficienttrain/MOE/deepseekmoe">DeepSeek MOE</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">MOE</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Adaptive Attention Span in Transformer</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Adaptive Attention Span in Transformer</h1></header>
<p>Transformer 계열 모델의 구조는 사실상 Mixture of Experts 를 내포하고 있습니다. MHA 모듈의 각 head 는 concat 된 뒤 linear layer 를 거치면서 서로 다르게 활성화됩니다. 엄밀하게 MOE와 정확히 같지는 않고, 보다 복잡하고 세세하게 구성된 MOE 라고 볼 수 있습니다. 사실 Transformer 내부에는 다양한 &quot;전문가&quot; 를 만들어내려는 철학이 많이 녹아있어요.</p>
<p>다음 글도 도움이 됩니다. <a href="/docs/models/aimodel/Transformer/mhainitialization">MHA INITIALIZATION</a></p>
<p>Transformer 계열에서 MOE 를 적용하는 것을 이해하기 위해서는 먼저, 각각의 Layer 와 Attention 모듈이 어떻게 동작하는지 이해하는 것이 중요합니다. Transformer-MOE 연구 초기는 MOE 구조를 Transformer 내부로 이동시키는 것에 집중했다면, 후기 연구들은 <strong>Attention 모듈의 특성을 MOE 구조에 보다 엄밀하게 반영</strong>하려고 노력합니다</p>
<p>본 논문은 Attention Module 의 동작을 분석했는데 MOE 논문을 읽으며 동작을 이해하는 데 많은 도움이 됩니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-attention-span">Adaptive Attention Span<a href="#adaptive-attention-span" class="hash-link" aria-label="Direct link to Adaptive Attention Span" title="Direct link to Adaptive Attention Span">​</a></h2>
<p>Attention Head 는 각 토큰을 다른 토큰과 얼마나 연관지을지 결정합니다. 본 논문의 첫 관찰은, 각 Attention Head 가 참조하는 토큰의 범위가 크게 차이가 난다는 것입니다.</p>
<div style="text-align:Center"><img src="/assets/images/adaptive_attention_span-cd07f4f862bd2dbd8b113f07e03aeb62.png" style="border:solid;width:600px"></div>
<p>위 그림은 두 개의 attention head 에서 거리에 따른 attention 크기를 plot한 그래프입니다. Head B 는 Head A 에 비해 훨씬 먼 거리의 token 까지도 attention 값이 크게 유지되는 것을 확인할 수 있습니다. Head A 는 보다 가까운 단어들을 활용해서 문맥을 해석하고, Head B 는 보다 먼 단어들까지 활용해서 문맥을 해석합니다.</p>
<p>Attention 모듈의 구조 상, Head A 는 가까운 단어들에 집중을 하지만 여전히 먼 단어들까지 고려된 forward 그리고 backward-propagating 이 이루어집니다. 저자들은 이 부분에서 발생하는 비효율에 집중해서 Attention Module 에서 참조하는 크기를 제한 합니다. Attention Window 를 attention head 마다 다르게 적용하는 개념이라고 이해할 수 있습니다.</p>
<p>저자들은 아래와 같은 Span Filter 를 적용합니다. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> 는 attention head 마다 설정되는 parameter 이며 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span></span></span></span> 은 일괄적으로 적용되는 smoothing parameter 입니다.</p>
<div style="text-align:Center"><img src="/assets/images/span_filter-767f1e1539c6f3be6029d731296d4b44.png" style="border:solid;width:600px"></div>
<p>예를 들어서, 100번째 토큰을 처리할 때 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>−</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">100-z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> ~ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>+</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">100+z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> 까지의 attention 은 그대로 유지되어 활성화됩니다. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>+</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">100+z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> ~ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>+</mo><mi>z</mi><mo>+</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">100+z+R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span></span></span></span> 와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>−</mo><mi>z</mi><mo>−</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">100-z-R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span></span></span></span> ~ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>−</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">100-z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> 범위의 attention 역시 filter 값을 곱한 크기만큼 활성화됩니다. 그 외 범위의 attention 은 masking 되어 계산에 사용되지 않습니다.</p>
<p>참고로, 구현에 관심있으신 분들은 masking 을 하는 것은 연산 효율에는 도움이 안 된다고 생각하실 것 같습니다. 저자들의 원 코드를 찾아보니, masking 되어야 하는 부분들을 실제로 tensor 에서 제외하고 처리합니다. 결과적으로 실제로 연산량을 줄이도록 만들었습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-adaptive-attention-span">Dynamic Adaptive Attention Span<a href="#dynamic-adaptive-attention-span" class="hash-link" aria-label="Direct link to Dynamic Adaptive Attention Span" title="Direct link to Dynamic Adaptive Attention Span">​</a></h2>
<p>이전 실험에서는 adaptive attention span 크기, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> 값을 사람이 직접 설정했습니다. dynamic adaptive attention span 실험에서는 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> 값을 학습할 있도록 linear layer 를 사용합니다.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mi>S</mi><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>v</mi><mi>T</mi></msup><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z_t = S \sigma ( v^T x_t + b )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span></span>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="result">Result<a href="#result" class="hash-link" aria-label="Direct link to Result" title="Direct link to Result">​</a></h2>
<p>본 논문의 실험 결과는 attention module 의 특징 분석, 그리고 효율 개선 두 가지로 나뉘어집니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="attention-분석">Attention 분석<a href="#attention-분석" class="hash-link" aria-label="Direct link to Attention 분석" title="Direct link to Attention 분석">​</a></h3>
<div style="text-align:Center"><img src="/assets/images/attention_result-db78138fcbf0953f231a4091a22c0062.png" style="border:solid;width:600px"></div>
<p>저자들은 하위 5개 Layer 에서는 attention span 의 크기가 매우 작다고 말합니다. 즉 하위 layer 에서는 굳이 full-attention 을 활용할 필요가 없다는 점을 시사합니다.</p>
<p>반면 상위 layer 의 일부 어테션 헤드에서는 매우 큰, 몇 천 크기의 attnetion span 이 확인되었다고 이야기합니다.</p>
<p>이는 CNN 의 설계에서 의도한 결과와 동일합니다. 두 방식 모두 초기에는 가까운 feature 위주로 분석하고, 후반에는 전체적인 의미를 해석하도록 동작합니다.</p>
<p>두 번째 분석은 <strong>input sequence 에 따른 dynamic span</strong> 변화입니다.</p>
<p>아래 그림은 input sequence 에 따른 평균적인 dynamic span 을 나타내는데, 단어의 앞과 끝에서는 비교적 짧은 span이, 단어의 중간에서는 비교적 긴 span이 적용되는 패턴이 있음을 알 수 있습니다.</p>
<div style="text-align:Center"><img src="/assets/images/adaptive_sequence_span-984fe49c7fad198de8d6feff1cc125f1.png" style="border:solid;width:600px"></div>
<p>참고로 Decoder-Only 모델에서는 재밌는 결과가 관찰됩니다. Decoder-only 모델은 input text, output text 에 가장 가까운 초기 그리고 마지막 layer 에서 문법적인 해석에 집중하는 모습을 보여줍니다. 문법은 보다 전체적인 문장 구조를 이해해야하는 과제인데, 이는 encoder 구조와 decoder-only 구조가 그 동작의 유사성과 별개로 과정은 완전히 다를 수 있다는 점을 보여줍니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="효율-개선">효율 개선<a href="#효율-개선" class="hash-link" aria-label="Direct link to 효율 개선" title="Direct link to 효율 개선">​</a></h3>
<p>본 논문에서는 Adaptive Attention Span 적용이 모델의 효율성 뿐 아니라, 모델의 성능 개선 역시 달성했다고 서술합니다.</p>
<p>비교를 위해 baseline 모델: Fixed span - 의 attention width 역시 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span></span></span></span> 로 제한합니다. attention window 를 상정하면 될 것 같습니다.</p>
<div style="text-align:Center"><img src="/assets/images/moe2_result-be767dbaf2fbb9ded2923caf3fe6ff11.png" style="border:solid;width:600px"></div>
<p>가장 왼쪽 그래프는 Adaptive Span 을 적용한 모델과 Fixed Span 모델의 BPC 성능을 보여줍니다. 낮은 span 크기에서는 모델의 성능이 비슷이 비슷한데 1000이 넘어가는 범위의 span 크기에서는 adaptive span 모델의 성능이 더 좋아집니다. 하지만 이 결과는 실제로는 모델 성능이 아니라 모델 효용의 문제라고 저는 해석합니다. 큰 모델을 학습하기 위해서는 충분히 많은 데이터와 컴퓨팅 리소스가 필요합니다. 해당 논문에서 사용할 수 있는 데이터와 컴퓨팅 리소스의 크기가 충분하지 않았으며, 만약 충분했다면 Fixed span 의 성능이 더 좋았을 것입니다.</p>
<p><strong>사실상 adaptive span 모델은 fixed span 모델의 parameter 가 조정되어 학습될 수 있는 구조이므로 당연합니다.</strong></p>
<p>이는 SOTA 레벨에서 모델의 성능과 모델의 효용이 사실상 같은 의미임을 나타냅니다.</p>
<p>중간, 오른쪽 그래프는 모델의 효용을 보다 직접적으로 나타냅니다. adaptive span 의 크기가 fixed span 의 크기보다 매우 적으며 이로 인해 FLOPS 가 크게 감소한다고 이야기 합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="limits">Limits<a href="#limits" class="hash-link" aria-label="Direct link to Limits" title="Direct link to Limits">​</a></h2>
<p>adaptive span 방식은 consecutive span 밖에 설정하지 못한다는 단점을 가지고 있습니다.</p>
<p>또한 Transformer 구조에서 FLOPS 의 또 다른 큰 축인 FFN 는 효율화되지 않았습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reference">Reference<a href="#reference" class="hash-link" aria-label="Direct link to Reference" title="Direct link to Reference">​</a></h2>
<p>[1] Adaptive Attention Span in Transforer, <a href="https://arxiv.org/pdf/1905.07799" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1905.07799</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/moe">MOE</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/transformer">Transformer</a></li></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/practice/efficienttrain/MOE/moe2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/practice/efficienttrain/MOE/moe1"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Sparsely-Gated MOE Layer</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/practice/efficienttrain/MOE/gshard"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">GShard</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#adaptive-attention-span" class="table-of-contents__link toc-highlight">Adaptive Attention Span</a></li><li><a href="#dynamic-adaptive-attention-span" class="table-of-contents__link toc-highlight">Dynamic Adaptive Attention Span</a></li><li><a href="#result" class="table-of-contents__link toc-highlight">Result</a><ul><li><a href="#attention-분석" class="table-of-contents__link toc-highlight">Attention 분석</a></li><li><a href="#효율-개선" class="table-of-contents__link toc-highlight">효율 개선</a></li></ul></li><li><a href="#limits" class="table-of-contents__link toc-highlight">Limits</a></li><li><a href="#reference" class="table-of-contents__link toc-highlight">Reference</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/community/hello">Hello, Lapis</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.instagram.com/or7l_floll/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/jhpark9701/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://leetcode.com/superstone/" target="_blank" rel="noopener noreferrer" class="footer__link-item">leetcode<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>