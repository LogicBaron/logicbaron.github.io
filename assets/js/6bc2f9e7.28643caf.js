"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[5103],{3946:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>x,frontMatter:()=>d,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"papers/y2025/oct/20251022","title":"10-22 papers summary","description":"1. MoGA bytedance","source":"@site/docs/papers/y2025/oct/20251022.md","sourceDirName":"papers/y2025/oct","slug":"/papers/y2025/oct/20251022","permalink":"/docs/papers/y2025/oct/20251022","draft":false,"unlisted":false,"editUrl":"https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/papers/y2025/oct/20251022.md","tags":[],"version":"current","frontMatter":{},"sidebar":"Y2025Sidebar","previous":{"title":"10-21 papers summary","permalink":"/docs/papers/y2025/oct/20251021"},"next":{"title":"2025-10-23 papers review","permalink":"/docs/papers/y2025/oct/20251023"}}');var o=r(4848),t=r(8453);const s=r.p+"assets/images/blog_20251022_img0-175f7345994cdf233c8042ce694816bd.png",a=r.p+"assets/images/blog_20251022_img1-9a6f1ec24a01af78a0ebfde62e051ce7.png",d={},l="10-22 papers summary",c={},p=[{value:"1. MoGA <code>bytedance</code>",id:"1-moga-bytedance",level:2},{value:"Mixture-of-Groups Attention for End-to-End Long Video Generation",id:"mixture-of-groups-attention-for-end-to-end-long-video-generation",level:3},{value:"2. Grasp Any Region <code>bytedance</code>",id:"2-grasp-any-region-bytedance",level:2},{value:"Towards Precise, Contextual Pixel Understanding for Multimodal LLMs",id:"towards-precise-contextual-pixel-understanding-for-multimodal-llms",level:3},{value:"3. ProCLIP",id:"3-proclip",level:2},{value:"Progressive Vision-Language Alignment via LLM-based Embedder",id:"progressive-vision-language-alignment-via-llm-based-embedder",level:3},{value:"4. DSI-BENCH",id:"4-dsi-bench",level:2},{value:"A BENCHMARK FOR DYNAMIC SPATIAL INTELLIGENCE",id:"a-benchmark-for-dynamic-spatial-intelligence",level:3},{value:"5. VIDEO REASONING WITHOUT TRAINING <code>qualcomm</code>",id:"5-video-reasoning-without-training-qualcomm",level:2},{value:"6. Extracting alignment data in open models <code>google</code>",id:"6-extracting-alignment-data-in-open-models-google",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"10-22-papers-summary",children:"10-22 papers summary"})}),"\n",(0,o.jsxs)(n.h2,{id:"1-moga-bytedance",children:["1. ",(0,o.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18692",children:"MoGA"})," ",(0,o.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,o.jsx)(n.h3,{id:"mixture-of-groups-attention-for-end-to-end-long-video-generation",children:"Mixture-of-Groups Attention for End-to-End Long Video Generation"}),"\n",(0,o.jsxs)(n.h2,{id:"2-grasp-any-region-bytedance",children:["2. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18876",children:"Grasp Any Region"})," ",(0,o.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,o.jsx)(n.h3,{id:"towards-precise-contextual-pixel-understanding-for-multimodal-llms",children:"Towards Precise, Contextual Pixel Understanding for Multimodal LLMs"}),"\n",(0,o.jsx)(n.p,{children:"MLLM \ub17c\ubb38\uc758 \uc804\uccb4 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825 -> \ubcf5\uc7a1\ud55c \uc7a5\uba74\uc758 dense understanding + \ubbf8\uc138\ud55c pixel understanding \uc744 \ub2ec\uc131\ud560 \uc218 \uc788\ub3c4\ub85d \ub3de\ub294 \uc0c8\ub85c\uc6b4 \ud504\ub808\uc784 \uc6cc\ud06c\uc778 Grasp Any Region \uc744 \uc81c\uc548."}),"\n",(0,o.jsx)(n.p,{children:"\uc804\uccb4 \uc774\ubbf8\uc9c0 feature map \uc77c\ub2e8 \ud655\ubcf4\ud574\ub450\uace0, Region base featrure \ub3c4 \ucd94\ucd9c\ud558\ub294 \ubc29\uc2dd."}),"\n",(0,o.jsxs)(n.h2,{id:"3-proclip",children:["3. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18795",children:"ProCLIP"})]}),"\n",(0,o.jsx)(n.h3,{id:"progressive-vision-language-alignment-via-llm-based-embedder",children:"Progressive Vision-Language Alignment via LLM-based Embedder"}),"\n",(0,o.jsx)(n.p,{children:"CLIP text encdoer \ub97c BERT -> LLM-based embedder \ub85c \ub300\uccb4\ud558\ub294 \ub17c\ubb38. \uad73\uc774 \uc65c \ub300\uccb4\ud558\ub0d0?"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"\uc784\ubca0\ub529\uc758 \ud45c\ud604\ub825\uc740 contrastive learning \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc744 \ub584 \ub354 \ub192\ub2e4.\n-> \uc784\ubca0\ub529 \uc790\uccb4\uc758 \ud65c\uc6a9\ub3c4\uac00 \ub192\uc74c."}),"\n",(0,o.jsx)(n.li,{children:"\ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc784\ubca0\ub529 alignment \ub2a5\ub825\uc774 \ub9e4\uc6b0 \uc720\uc6a9\ud568."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"\uadf8\ub7ec\ubbc0\ub85c \uc800 \ub450 \ub2a5\ub825\uc744 \uc720\uc9c0\ub418\ub3c4\ub85d lm-based embedder \ud29c\ub2dd\ud558\ub294 \ub17c\ubb38\uc784."}),"\n",(0,o.jsx)("div",{style:{textAlign:"center"},children:(0,o.jsx)("img",{src:s,style:{width:500}})}),"\n",(0,o.jsx)(n.p,{children:"llm-based embedder \uac00 \uc544\ub2c8\ub77c MLP \ub97c \ud558\ub098 \ubd99\uc5ec\uc11c \uc598\ub9cc \ud559\uc2b5\uc2dc\ud0b4."}),"\n",(0,o.jsx)(n.p,{children:"1\ub2e8\uacc4\ub294 CLIP-text-encdoer \uc640 llm-based embedder \uc758 alignment \ud29c\ub2dd \uc791\uc5c5."}),"\n",(0,o.jsx)(n.p,{children:"2\ub2e8\uacc4\ub294 text-image alignment \uac00 \ub9de\ub3c4\ub85d CLIP \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ud568."}),"\n",(0,o.jsxs)(n.h2,{id:"4-dsi-bench",children:["4. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18873",children:"DSI-BENCH"})]}),"\n",(0,o.jsx)(n.h3,{id:"a-benchmark-for-dynamic-spatial-intelligence",children:"A BENCHMARK FOR DYNAMIC SPATIAL INTELLIGENCE"}),"\n",(0,o.jsx)("div",{style:{textAlign:"center"},children:(0,o.jsx)("img",{src:a,style:{width:500}})}),"\n",(0,o.jsx)(n.p,{children:"Observer \uc640 Object \uac00 \ub3d9\uc2dc\uc5d0 \uc6c0\uc9c1\uc774\ub294 \ud604\uc2e4\uc801\uc778 3D \uc2dc\ub098\ub9ac\uc624 \ub370\uc774\ud130 \ubca4\uce58\ub9c8\ud06c."}),"\n",(0,o.jsxs)(n.h2,{id:"5-video-reasoning-without-training-qualcomm",children:["5. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.17045",children:"VIDEO REASONING WITHOUT TRAINING"})," ",(0,o.jsx)(n.code,{children:"qualcomm"})]}),"\n",(0,o.jsx)(n.p,{children:"\ud004\ucef4\uc774 \uac11\uc790\uae30 \uc660 \ub17c\ubb38\uc744."}),"\n",(0,o.jsx)(n.p,{children:"\ud6c8\ub828 \uc5c6\uc774 Video reasoning \ud488\uc9c8\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\ub860. \ud55c \uac00\uc9c0 \uad00\ucc30\uc5d0\uc11c \uc544\uc774\ub514\uc5b4\ub97c \uc5bb\uc74c."}),"\n",(0,o.jsx)(n.p,{children:"\ubaa8\ub378\uc758 \ud1a0\ud070 \uc0dd\uc131 \uacfc\uc815\uc5d0\uc11c\ub294 \ub450 \uac1c\uc758 \ud070 \ud750\ub984\uc774 \uc788\uc74c."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"macro-exploration \ub2e8\uacc4 : \ubaa8\ub378\uc774 \ud574\ub2f5\uc5d0 \ub300\ud55c \ubd88\ud655\uc2e4\uc131\uc744 \ub290\ub07c\uba70 \uc5ec\ub7ec \ub300\ub2f5\uc744 \uac80\ud1a0\ud558\ub294 \ub2e8\uacc4. \uc5d4\ud2b8\ub85c\ud53c\uac00 \uc810\uc9c4\uc801 \uc99d\uac00."}),"\n",(0,o.jsx)(n.li,{children:"micro-exploitation \ub2e8\uacc4 : \ubaa8\ub378\uc774 \ucd5c\uc885 \ub300\ub2f5\uc5d0 \ub300\ud55c \ud655\uc2e0\uc744 \uac00\uc9c0\uace0 \ub300\ub2f5\uc744 \uc0dd\uc131\ud558\ub294 \ub2e8\uaf10. \uc5d4\ud2b8\ub85c\ud53c\uac00 \uc810\uc9c4\uc801 \uac10\uc18c."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:':::note:::\n"We discover that more accurate models have an entropy peak that is both lower and delayed during Macro-Exploration and an Exploitation Phase that converges to a lower final entropy."\n:::'}),"\n",(0,o.jsx)(n.p,{children:"\uc88b\uc740 \ubaa8\ub378\uc77c\uc218\ub85d macro-exploration \ub2e8\uacc4\uc5d0\uc11c \uc5d4\ud2b8\ub85c\ud53c \ud53c\ud06c\uac00 \ub0ae\uace0 \ub2a6\uac8c \ub098\ud0c0\ub098\uba70, micro-exploitation \uc5d0\uc11c \ucd5c\uc885\uc801\uc73c\ub85c \ub354 \ub0ae\uc740 \uc5d4\ud2b8\ub85c\ud53c\ub85c \uc218\ub834\ud568."}),"\n",(0,o.jsxs)(n.p,{children:["\uad00\ucc30 \uacb0\uacfc\uac00 ",(0,o.jsx)(n.a,{href:"/docs/papers/y2025/oct/20251007#swireasoning-microsoft",children:"SWIREASONGING"})," \uacfc\ub3c4 \uad00\ub828\uc774 \uc788\uc5b4\ubcf4\uc778\ub2e4."]}),"\n",(0,o.jsx)(n.p,{children:"\uc774\ub97c \ubc14\ud0d5\uc73c\ub85c, LLM \uc758 \ud1a0\ud070 \uc0dd\uc131 \uc9c1\uc804\uc5d0 \uc791\uc740 trainable vector \ub97c \ucd94\uac00\ud55c\ub2e4. \uc774 trainable vector \ub97c controller \ub77c\uace0 \ubd80\ub974\uaca0\uc74c. \uc774 controller \ub294 \ud1a0\ud070\uc774 \uc0dd\uc131\ub420\ub584\ub9c8\ub2e4 \ud559\uc2b5\ub418\uba74\uc11c \uac12\uc774 \uc5c5\ub370\uc774\ud2b8\ub428."}),"\n",(0,o.jsx)(n.p,{children:"\ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub418\ub294 loss \ub294"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"entropy EMA \uac00 \uc99d\uac00\ud558\ub294 \ucd94\uc138\uc77c \ub54c -> entropy EMA \uac00 \uc99d\uac00\ud558\ub3c4\ub85d \uc7a5\ub824"}),"\n",(0,o.jsx)(n.li,{children:"entropy EMA \uac00 \uac10\uc18c\ud558\ub294 \ucd94\uc138\uc77c \ub54c -> entropy EMA \uac00 \uac10\uc18c\ud558\ub3c4\ub85d \uc7a5\ub824"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"\uc989, \uc88b\uc740 \ubaa8\ub378\uc5d0\uc11c \ubcf4\uc774\ub294 \ucd94\uc138\ub97c \uc798 \ub530\ub77c\uac00\ub3c4\ub85d \uac00\uc774\ub358\uc2a4\ub97c \uc8fc\ub294 controller \ub97c \ucd94\uac00\ud558\ub294 \uac70\uc784."}),"\n",(0,o.jsx)(n.p,{children:"\ud6c8\ub828\ud55c \ubaa8\ub378\ub9cc\ud07c \uc131\ub2a5\uc774 \ub098\uc624\uc9c0\ub294 \uc54a\uc558\uc9c0\ub9cc, \ud559\uc2b5 \uc2dc\uac04 \ub4f1\uc758 \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654 \uc2dc\ud0a4\uba74\uc11c \uac70\uc758 \ud6c8\ub828\ub41c \ubaa8\ub378 \uc131\ub2a5\uc744 \ub530\ub77c\uc7a1\uc558\ub2e4\uace0 \ud568. \ud070 \uc7a5\uc810\uc73c\ub85c \uc774\uc57c\uae30\ud55c \uac83\uc911 \ud558\ub098\uac00 \ud655\uc7a5\uc131. \ubcf5\uc7a1\ud558\uc9c0 \uc54a\uc740 \uad6c\uc870\ub85c \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc5d0 \uc27d\uac8c \uc801\uc6a9\uac00\ub2a5\ud558\ub2e4\uace0 \ud568."}),"\n",(0,o.jsxs)(n.h2,{id:"6-extracting-alignment-data-in-open-models-google",children:["6. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18554",children:"Extracting alignment data in open models"})," ",(0,o.jsx)(n.code,{children:"google"})]}),"\n",(0,o.jsx)(n.p,{children:"open-source model \uc758 \uacf5\uac1c\ub418\uc9c0 \uc54a\uc740 \ud6c8\ub828 \ub370\uc774\ud130\ub97c \ubf51\uc544\ub0bc \uc218 \uc788\ub2e4\ub294 \ub17c\ubb38. \uc704\ud5d8\uc131 \uc54c\ub9ac\ub824\ub294\uac8c \ub2f9\uc5f0\ud788 \ubaa9\ud45c\uc77c\uac70\ub2e4."}),"\n",(0,o.jsxs)(n.p,{children:["open-source model \uc774 ",(0,o.jsx)(n.code,{children:"<|user|>"})," \uc640 \uac19\uc740 \uc2dc\uc791 \ud1a0\ud070\uc744 post-train \ub2e8\uacc4\uc5d0\uc11c\ub9cc \ub3c4\uc785\ub41c\ub2e4\ub294 \ubc94\uc744 \uc774\uc6a9\ud574\uc11c, \uc774 prefix \ub97c \uc0ac\uc6a9\ud55c prompt \ub97c alignment data \uc640 \uc720\uc0ac\ud55c \ud14d\uc2a4\ud2b8\ub97c \uc0dd\uc131\ud558\ub3c4\ub85d \uc720\ub3c4\ud568."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"<|user|>"})," -> \ud6c8\ub828 \ub370\uc774\ud130\ub97c \uc904\uc904 \ubc49\ub294\ub2e4\ub294 \ub9d0.."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Verbatim : \uc644\ubcbd\ud788 \ub611\uac19\uc774 \uc77c\uce58\ud558\ub294 \ubcf5\uc81c\ud55c \ubb38\uc790\uc5f4. (\ub9d0\uadf8\ub300\ub85c, \ubb38\uc790\uadf8\ub300\ub85c)"}),"\n"]})]})}function x(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(6540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);