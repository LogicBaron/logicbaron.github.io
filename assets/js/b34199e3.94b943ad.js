"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2769],{5341:e=>{e.exports=JSON.parse('{"tag":{"label":"MOE","permalink":"/docs/tags/moe","allTagsPath":"/docs/tags","count":7,"items":[{"id":"practice/efficienttrain/MOE/moe2","title":"Adaptive Attention Span in Transformer","description":"Transformer \uacc4\uc5f4 \ubaa8\ub378\uc758 \uad6c\uc870\ub294 \uc0ac\uc2e4\uc0c1 Mixture of Experts \ub97c \ub0b4\ud3ec\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. MHA \ubaa8\ub4c8\uc758 \uac01 head \ub294 concat \ub41c \ub4a4 linear layer \ub97c \uac70\uce58\uba74\uc11c \uc11c\ub85c \ub2e4\ub974\uac8c \ud65c\uc131\ud654\ub429\ub2c8\ub2e4. \uc5c4\ubc00\ud558\uac8c MOE\uc640 \uc815\ud655\ud788 \uac19\uc9c0\ub294 \uc54a\uace0, \ubcf4\ub2e4 \ubcf5\uc7a1\ud558\uace0 \uc138\uc138\ud558\uac8c \uad6c\uc131\ub41c MOE \ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0ac\uc2e4 Transformer \ub0b4\ubd80\uc5d0\ub294 \ub2e4\uc591\ud55c \\"\uc804\ubb38\uac00\\" \ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub824\ub294 \ucca0\ud559\uc774 \ub9ce\uc774 \ub179\uc544\uc788\uc5b4\uc694.","permalink":"/docs/practice/efficienttrain/MOE/moe2"},{"id":"practice/efficienttrain/MOE/deepseekmoe","title":"DeepSeek MOE","description":"","permalink":"/docs/practice/efficienttrain/MOE/deepseekmoe"},{"id":"practice/efficienttrain/MOE/moe3","title":"Efficient MOE","description":"Large Model \uc5d0\uc11c MOE \uc758 \uc801\uc6a9\uc5d0 \ub530\ub978 pros / cons \ub97c \uc798 \ubd84\uc11d\ud55c \ub17c\ubb38. \uc8fc\uc694 \ub17c\uc810\uc774 \ub9e4\uc6b0 \uc911\uc694\ud55c\ub370, \uc8fc\uc694 \ub17c\uc810\ub9cc \ud30c\uc545\ud558\uba74 \ub418\ub294 \ub17c\ubb38\uc774\ub77c\uace0 \uc0dd\uac01\ud574\uc11c \uac04\ub2e8\ud558\uac8c \uc694\uc57d\ud558\ub824 \ud568.","permalink":"/docs/practice/efficienttrain/MOE/moe3"},{"id":"practice/efficienttrain/MOE/gshard","title":"GShard","description":"Motivation","permalink":"/docs/practice/efficienttrain/MOE/gshard"},{"id":"practice/efficienttrain/MOE/mistralmoe","title":"Mixtral of Experts","description":"mistral \uc5d0\uc11c \uc81c\uc548\ud558\ub294 MOE LLM. decoder-only transformer architecture\uc5d0 MOE\ub97c \uc0ac\uc6a9\ud588\ub2e4. \uac01 layer\uc5d0\uc11c \uac01 \ud1a0\ud070\uc740 2\uac1c\uc758 expert\ub85c \ub77c\uc6b0\ud305 (Top-2 Routing)\ub41c\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c mixtral 8x7B \ubaa8\ub378\uc5d0\uc11c \uac01 \ud1a0\ud070\uc740 47B\uac1c\uc758 parameter\uc5d0 \uc811\uadfc\uac00\ub2a5\ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c \ud65c\uc131\ud654\ub418\ub294 parameter\ub294 13B \uc218\uc900\uc774 \ub41c\ub2e4.","permalink":"/docs/practice/efficienttrain/MOE/mistralmoe"},{"id":"practice/efficienttrain/MOE/moe1","title":"Sparsely-Gated MOE Layer","description":"Mixture of Experts \ub97c \uc815\ub9ac\ud558\ub294 \uccab \ubc88\uc9f8 \uae00\uc774\ub2e4. \uccab \ubc88\uc9f8 \uae00\uc744 \uaf64 \uace0\ubbfc\ud588\ub294\ub370, Transformer \uc774\uc804 \ub17c\ubb38 \uc911 \uac00\uc7a5 \uc758\ubbf8\uc788\ub2e4\uace0 \uc0dd\uac01\ub418\ub294 \ub17c\ubb38\uc774\ub2e4.","permalink":"/docs/practice/efficienttrain/MOE/moe1"},{"id":"practice/efficienttrain/MOE/switchformer","title":"Switch Transformers","description":"Introduction","permalink":"/docs/practice/efficienttrain/MOE/switchformer"}],"unlisted":false}}')}}]);