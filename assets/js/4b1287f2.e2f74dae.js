"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[7135],{8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var t=i(6540);const a={},s=t.createContext(a);function l(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),t.createElement(s.Provider,{value:n},e.children)}},8497:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>u,contentTitle:()=>g,default:()=>m,frontMatter:()=>d,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"papers/y2025/oct/20251029","title":"2025-10-29 papers review","description":"1. FunReason-MT Technical Report: Multi-Turn Function Callingg","source":"@site/docs/papers/y2025/oct/20251029.md","sourceDirName":"papers/y2025/oct","slug":"/papers/y2025/oct/20251029","permalink":"/docs/papers/y2025/oct/20251029","draft":false,"unlisted":false,"editUrl":"https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/papers/y2025/oct/20251029.md","tags":[],"version":"current","frontMatter":{},"sidebar":"Y2025Sidebar","previous":{"title":"2025-10-27 papers review","permalink":"/docs/papers/y2025/oct/20251027"}}');var a=i(4848),s=i(8453);const l=i.p+"assets/images/blog_20251029_img0-50657d564662f1d54ccb3816ab8f3591.png",o=i.p+"assets/images/blog_20251029_img1-2a423472a128739e2417d900dbcdd9ae.png",r=i.p+"assets/images/blog_20251029_img2-7ff2474d3efbd18982d203cc56050e97.png",c=i.p+"assets/images/blog_20251029_img3-d418d0ce07133a785afda85fdb51d115.png",d={},g="2025-10-29 papers review",u={},h=[{value:"1. FunReason-MT Technical Report: Multi-Turn Function Callingg",id:"1-funreason-mt-technical-report-multi-turn-function-callingg",level:2},{value:"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs <code>microsoft</code>",id:"latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms-microsoft",level:2},{value:"2. ATLAS <code>google</code>",id:"2-atlas-google",level:2},{value:"ADAPTIVE TRANSFER SCALING LAWS FOR MULTILINGUAL PRETRAINING, FINETUNING, AND DECODING THE CURSE OF MULTILINGUALITY",id:"adaptive-transfer-scaling-laws-for-multilingual-pretraining-finetuning-and-decoding-the-curse-of-multilinguality",level:3},{value:"3. Game-TARS <code>bytedance</code>",id:"3-game-tars-bytedance",level:2},{value:"Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents",id:"pretrained-foundation-models-for-scalable-generalist-multimodal-game-agents",level:3},{value:"4. From Spatial to Actions <code>bytedance</code>",id:"4-from-spatial-to-actions-bytedance",level:2},{value:"Grounding Vision-Language-Action Model in Spatial Foundation Priors",id:"grounding-vision-language-action-model-in-spatial-foundation-priors",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"2025-10-29-papers-review",children:"2025-10-29 papers review"})}),"\n",(0,a.jsxs)(n.h2,{id:"1-funreason-mt-technical-report-multi-turn-function-callingg",children:["1. ",(0,a.jsx)(n.a,{href:"https://huggingface.co/papers/2510.24448",children:"FunReason-MT Technical Report: Multi-Turn Function Callingg"})]}),"\n",(0,a.jsx)(n.p,{children:"LLM \ub3c4\uad6c multi-turn function calling \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \ubc29\ubc95 \uc81c\uc548."}),"\n",(0,a.jsx)(n.p,{children:"matser \ub97c \ub450\ub294 \ubc29\uc2dd. top-down \uc2e4\ud589."}),"\n",(0,a.jsxs)(n.p,{children:["\uba3c\uc800 \ub3c4\uad6c\ub4e4\uc758 dependency graph \uadf8\ub9b0\ub2e4. \uadf8 \ud6c4 \ubaa9\ud45c\ub97c \uc774\ub8e8\uae30\uc704\ud55c execution trace \ub97c \uad6c\uc0c1. execution trace \ub97c \ucd94\uc0c1\ud654\ud574\uc11c \uc5b4\ub824\uc6b4 \ucffc\ub9ac\ub97c \uad6c\uc0c1. \uc774 \uacfc\uc815\uc740 ",(0,a.jsx)(n.strong,{children:"\uc0c1\uc704 \ub808\ubca8 \uc5f0\uc0b0 \ud234 \ud569\uc131"})," -> \uc0c1\uc704 \ub808\ubca8 \uc5f0\uc0b0 \ud234\ub85c \ud480 \uc218 \uc788\ub294 \uc0c1\uc704 \ub808\ubca8 \ucffc\ub9ac \uad6c\uc0c1. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc624\ub958 \ud53c\ub4dc\ubc31. \uac01 \uacfc\uc815\uc774 \uc758\ubbf8 \uc788\ub294\ub4ef."]}),"\n",(0,a.jsxs)(n.h2,{id:"latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms-microsoft",children:[(0,a.jsx)(n.a,{href:"https://huggingface.co/papers/2510.24514",children:"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"})," ",(0,a.jsx)(n.code,{children:"microsoft"})]}),"\n",(0,a.jsx)(n.p,{children:"\ud654.. \ub611\ub611\ud558\ub2e4..."}),"\n",(0,a.jsx)(n.p,{children:'MLLM \ubaa8\ub378\uc740 \uc2dc\uac01 \uc694\uc18c \uc774\ud574 \ub2a5\ub825\uc740 \ub6f0\uc5b4\ub098\uc9c0\ub9cc visual reasoning (multimodal reasoning) \ub2a5\ub825\uc740 \ub5a8\uc5b4\uc9d0. \uc774\ub294 \ubaa8\ub378\uc774 "\uc0c1\uc0c1\ub825", \uadf8\ub7ec\ub2c8\uae4c \uc774\ubbf8\uc9c0 \uae30\ubc18 \uc0dd\uac01 \ub2a5\ub825\uc774 \ubd80\uc871\ud574\uc11c \uadf8\ub7fc. \uc0ac\ub78c\uc740 \uc774\ubbf8\uc9c0 \uad00\ub828 \ubb38\uc81c\ub97c "\uc0c1\uc0c1" \ud574\uc11c \ud574\uacb0\ud568. \uc608\ub97c \ub4e4\uc5b4\uc11c \ubbf8\ub85c. \ubbf8\ub85c\ub97c \uae00\ub85c \uc0dd\uac01\ud574\uc11c \ud574\uacb0\ud558\ub294 \uc0ac\ub78c\uc774 \uc788\uc744\uae4c?'}),"\n",(0,a.jsx)(n.p,{children:"Latent sketchpad\ub97c \ub3c4\uc785. native autoregressive reasoning process \ub0b4\uc5d0\uc11c textual reasoning\uacfc visual latents \uc0dd\uc131\uc744 interleave."}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)("img",{src:l,style:{width:500}})}),"\n",(0,a.jsx)(n.p,{children:"\uc9c4\uc9dc \ub9d0\uadf8\ub300\ub85c visual latent \ub97c \uc0dd\uc131\ud55c\ub2e4. \uc774 \ubc29\uc2dd\uc758 \uac00\uc7a5 \ud070 \uc7a5\uc810\uc740 plug-and-play. \ubaa8\ub4c8 \ub0b4\ubd80\uc5d0 \uc0dd\uc131\ub418\ub294 \uac83\uc774 \uc544\ub2c8\ub77c autoregressive \uacfc\uc815\uc5d0\uc11c \uc774\ubbf8\uc9c0 latent \ub97c \uc0dd\uc131\ud574\uc11c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc774\uae30 \ub54c\ubb38\uc5d0 MLLM \ub4a4\uc5d0 \ubd99\uc774\uba74 \ub428. \uc2e4\uc81c\ub85c Qwen3 \ubaa8\ub378\ub4e4\uc5d0 \ubd99\uc5ec\uc11c \uc2e4\ud5d8 \uc9c4\ud589\ud568."}),"\n",(0,a.jsx)(n.p,{children:"visual latent \ub97c \ub514\ucf54\ub529\ud560 \uc218 \uc788\ub294 sketch decoder \ub3c4 \ud559\uc2b5\uc2dc\ud0b4."}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)("img",{src:o,style:{width:500}})}),"\n",(0,a.jsx)(n.p,{children:"\ud559\uc2b5\uc740 backbone model \uc744 frozen \ud558\uace0 \uc9c4\ud589\ub428."}),"\n",(0,a.jsxs)(n.h2,{id:"2-atlas-google",children:["2. ",(0,a.jsx)(n.a,{href:"https://huggingface.co/papers/2510.22037",children:"ATLAS"})," ",(0,a.jsx)(n.code,{children:"google"})]}),"\n",(0,a.jsx)(n.h3,{id:"adaptive-transfer-scaling-laws-for-multilingual-pretraining-finetuning-and-decoding-the-curse-of-multilinguality",children:"ADAPTIVE TRANSFER SCALING LAWS FOR MULTILINGUAL PRETRAINING, FINETUNING, AND DECODING THE CURSE OF MULTILINGUALITY"}),"\n",(0,a.jsx)(n.p,{children:"multi-language \uc5d0\ub3c4 scailing law \uac00 \uc788\uc74c. curse of dimension \uc5d0 \uac00\uae5d\ub098?"}),"\n",(0,a.jsx)(n.p,{children:"multi-language train \uc758 \ubb38\uc81c \uc911 \ud558\ub098\uc778 **\ub2e4\uad6d\uc5b4 \uc800\uc8fc (Curse of Multilinguality)**\ub294 \ud558\ub098\uc758 \ubaa8\ub378\uc744 \uc5ec\ub7ec \uc5b8\uc5b4\ub85c \ub3d9\uc2dc\uc5d0 \ud559\uc2b5(\ub2e4\uad6d\uc5b4 \uc0ac\uc804 \ud559\uc2b5)\ud560 \ub54c \ubc1c\uc0dd\ud558\ub294 \uc131\ub2a5 \uc800\ud558 \ud604\uc0c1\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \ub610\ud55c \uc5b8\uc5b4\uac04 \uc804\uc774 \ud604\uc0c1. \uae0d\uc815\uc801\uc778 \uc804\uc774\uc640 \ubd80\uc815\uc801\uc778 \uc804\uc774\uac00 \uc804\ubd80 \uc788\uc5b4 \uc608\uce21\ud558\uae30 \uc5b4\ub835\ub2e5."}),"\n",(0,a.jsx)(n.p,{children:"\ubcf8 \ub17c\ubb38\uc740 \uc774\ub7f0 \ubb38\uc81c\ub97c \uace0\ub824\ud574\uc11c multi-language train \uc5d0 \uc788\uc5b4\uc11c scailing law \ud6a8\uacfc\ub97c \uac80\uc99d\ud568."}),"\n",(0,a.jsxs)(n.p,{children:["\uc77c\ub2e8 \uc5b8\uc5b4\uc218\uac00 \ub298\uc5b4\ub098\uba74 \ubaa8\ub378 \ud06c\uae30\ub3c4 \ud655\uc7a5\uc2dc\ucf1c\uc57c \ud55c\ub2e4. \ud2b9\ud788 \uc131\ub2a5 \uc720\uc9c0\uc5d0 \uc788\uc5b4\uc11c\ub294 ",(0,a.jsx)(n.strong,{children:"\ub370\uc774\ud130 \ud06c\uae30\ubcf4\ub2e4 \ubaa8\ub378 \ud06c\uae30 \uc99d\uac00"}),"\uac00 \ub354 \ud070 \uc774\ub4dd\uc744 \uac00\uc838\uc635\ub2c8\ub2e4."]}),"\n",(0,a.jsxs)(n.p,{children:["multi-language \uc774\uc288\ub294 \uc5b8\uc5b4 \uc218\uac00 \uc99d\uac00\ud560 \ub54c\ub9c8\ub2e4 \ubaa8\ub378 \ud06c\uae30\uc640 \ub370\uc774\ud130 \ud06c\uae30\uac00 ",(0,a.jsx)(n.strong,{children:"\uc9c0\uc218\uc801"}),"\uc73c\ub85c \ud655\uc7a5\ub418\uc5b4\uc57c \ud55c\ub2e4\uace0 \ud568."]}),"\n",(0,a.jsxs)(n.h2,{id:"3-game-tars-bytedance",children:["3. ",(0,a.jsx)(n.a,{href:"https://huggingface.co/papers/2510.23691",children:"Game-TARS"})," ",(0,a.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,a.jsx)(n.h3,{id:"pretrained-foundation-models-for-scalable-generalist-multimodal-game-agents",children:"Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents"}),"\n",(0,a.jsx)(n.p,{children:"\ubc14\uc774\ud2b8 \ub304\uc2a4\uc758 \uac8c\uc784 \ub17c\ubb38. \ubc14\uc774\ud2b8 \ub304\uc2a4\uac00 \uc601\uc0c1AI\ub97c \uc5c4\uccad \uc5f0\uad6c\ud558\ub294 \ub9cc\ud07c \uac8c\uc784 \uc5d0\uc774\uc804\ud2b8 \ucabd \uc5f0\uad6c\uac00 \ud569\uccd0\uc9c0\uba74 \uc2dc\ub108\uc9c0\uac00 \uc5c4\uccad\ub0a0 \uac83\uc774\ub77c\uace0 \uc0dd\uac01\ud568."}),"\n",(0,a.jsx)(n.p,{children:"\ud1b5\ud569\uc801\uc778 \uac8c\uc784 \uc5d0\uc774\uc804\ud2b8\uc5d0 \ub300\ud55c \ub17c\ubb38. \uae30\uc874 \uac8c\uc784 \uc5d0\uc774\uc804\ud2b8 \uc5f0\uad6c\ub4e4\uc740 \ud55c\uc815\ub41c \ud589\ub3d9\uc5d0 \ubc18\uc751\ud558\ub294 \uc218\uc900\uc774\uc5c8\uc9c0\ub9cc \uc804\ubc18\uc801\uc778 \ub9c8\uc6b0\uc2a4 \ud0a4\ubcf4\ub4dc \uc785\ub825\uc5d0 \ub300\ud574 \ubc18\uc751\ud560\uc218 \uc788\ub294 \uac8c\uc784 \uc5d0\uc774\uc804\ud2b8\ub97c \ud559\uc2b5\uc2dc\ud0b4."}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)("img",{src:r,style:{width:500}})}),"\n",(0,a.jsxs)(n.h2,{id:"4-from-spatial-to-actions-bytedance",children:["4. ",(0,a.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17439",children:"From Spatial to Actions"})," ",(0,a.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,a.jsx)(n.h3,{id:"grounding-vision-language-action-model-in-spatial-foundation-priors",children:"Grounding Vision-Language-Action Model in Spatial Foundation Priors"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\uae30\uc874 VLA(Vision-Language-Action) \ub85c\ubd07 \ubaa8\ub378\uc740 3D \uc2e4\uc81c \ud658\uacbd\uc5d0\uc11c \uc791\ub3d9\ud568\uc5d0\ub3c4 2D \uc778\ucf54\ub354 \uae30\ubc18\uc73c\ub85c \uad6c\ucd95\ub418\uc5b4 3D \uacf5\uac04 \ucd94\ub860(Spatial Reasoning)\uc758 \uacf5\ubc31\uc774 \ubc1c\uc0dd"}),"\n",(0,a.jsx)(n.li,{children:"FALCON\uc740 \ud48d\ubd80\ud55c 3D \uacf5\uac04 \uc815\ubcf4\ub97c **\uc561\uc158 \ud5e4\ub4dc(Action Head)**\uc5d0 \uc9c1\uc811 \uc8fc\uc785"}),"\n",(0,a.jsx)(n.li,{children:"FALCON\uc740 \uc138 \uac00\uc9c0 \uc2dc\ubbac\ub808\uc774\uc158 \ubca4\uce58\ub9c8\ud06c (CALVIN, SimplerEnv) \ubc0f 11\uac00\uc9c0 \uc2e4\uc81c \ud0dc\uc2a4\ud06c\uc5d0\uc11c \uc77c\uad00\ub418\uac8c SOTA \uc131\ub2a5\uc744 \ub2ec\uc131"}),"\n"]}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)("img",{src:c,style:{width:500}})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}}}]);