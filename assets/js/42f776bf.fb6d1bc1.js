"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3799],{2111:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>g,frontMatter:()=>o,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"papers/y2025/20251022","title":"10-22 papers summary","description":"1. MoGA bytedance","source":"@site/docs/papers/y2025/20251022.md","sourceDirName":"papers/y2025","slug":"/papers/y2025/20251022","permalink":"/docs/papers/y2025/20251022","draft":false,"unlisted":false,"editUrl":"https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/papers/y2025/20251022.md","tags":[],"version":"current","frontMatter":{},"sidebar":"Y2025Sidebar","previous":{"title":"10-21 papers summary","permalink":"/docs/papers/y2025/20251021"}}');var s=i(4848),d=i(8453);const t=i.p+"assets/images/blog_20251022_img0-175f7345994cdf233c8042ce694816bd.png",a=i.p+"assets/images/blog_20251022_img1-9a6f1ec24a01af78a0ebfde62e051ce7.png",o={},l="10-22 papers summary",c={},p=[{value:"1. MoGA <code>bytedance</code>",id:"1-moga-bytedance",level:2},{value:"Mixture-of-Groups Attention for End-to-End Long Video Generation",id:"mixture-of-groups-attention-for-end-to-end-long-video-generation",level:3},{value:"2. Grasp Any Region <code>bytedance</code>",id:"2-grasp-any-region-bytedance",level:2},{value:"Towards Precise, Contextual Pixel Understanding for Multimodal LLMs",id:"towards-precise-contextual-pixel-understanding-for-multimodal-llms",level:3},{value:"3. ProCLIP",id:"3-proclip",level:2},{value:"Progressive Vision-Language Alignment via LLM-based Embedder",id:"progressive-vision-language-alignment-via-llm-based-embedder",level:3},{value:"4. DSI-BENCH",id:"4-dsi-bench",level:2},{value:"A BENCHMARK FOR DYNAMIC SPATIAL INTELLIGENCE",id:"a-benchmark-for-dynamic-spatial-intelligence",level:3},{value:"5. VIDEO REASONING WITHOUT TRAINING <code>qualcomm</code>",id:"5-video-reasoning-without-training-qualcomm",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",...(0,d.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"10-22-papers-summary",children:"10-22 papers summary"})}),"\n",(0,s.jsxs)(n.h2,{id:"1-moga-bytedance",children:["1. ",(0,s.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18692",children:"MoGA"})," ",(0,s.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,s.jsx)(n.h3,{id:"mixture-of-groups-attention-for-end-to-end-long-video-generation",children:"Mixture-of-Groups Attention for End-to-End Long Video Generation"}),"\n",(0,s.jsxs)(n.h2,{id:"2-grasp-any-region-bytedance",children:["2. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18876",children:"Grasp Any Region"})," ",(0,s.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,s.jsx)(n.h3,{id:"towards-precise-contextual-pixel-understanding-for-multimodal-llms",children:"Towards Precise, Contextual Pixel Understanding for Multimodal LLMs"}),"\n",(0,s.jsx)(n.p,{children:"MLLM \ub17c\ubb38\uc758 \uc804\uccb4 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825 -> \ubcf5\uc7a1\ud55c \uc7a5\uba74\uc758 dense understanding + \ubbf8\uc138\ud55c pixel understanding \uc744 \ub2ec\uc131\ud560 \uc218 \uc788\ub3c4\ub85d \ub3de\ub294 \uc0c8\ub85c\uc6b4 \ud504\ub808\uc784 \uc6cc\ud06c\uc778 Grasp Any Region \uc744 \uc81c\uc548."}),"\n",(0,s.jsx)(n.p,{children:"\uc804\uccb4 \uc774\ubbf8\uc9c0 feature map \uc77c\ub2e8 \ud655\ubcf4\ud574\ub450\uace0, Region base featrure \ub3c4 \ucd94\ucd9c\ud558\ub294 \ubc29\uc2dd."}),"\n",(0,s.jsxs)(n.h2,{id:"3-proclip",children:["3. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18795",children:"ProCLIP"})]}),"\n",(0,s.jsx)(n.h3,{id:"progressive-vision-language-alignment-via-llm-based-embedder",children:"Progressive Vision-Language Alignment via LLM-based Embedder"}),"\n",(0,s.jsx)(n.p,{children:"CLIP text encdoer \ub97c BERT -> LLM-based embedder \ub85c \ub300\uccb4\ud558\ub294 \ub17c\ubb38. \uad73\uc774 \uc65c \ub300\uccb4\ud558\ub0d0?"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"\uc784\ubca0\ub529\uc758 \ud45c\ud604\ub825\uc740 contrastive learning \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc744 \ub584 \ub354 \ub192\ub2e4.\n-> \uc784\ubca0\ub529 \uc790\uccb4\uc758 \ud65c\uc6a9\ub3c4\uac00 \ub192\uc74c."}),"\n",(0,s.jsx)(n.li,{children:"\ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc784\ubca0\ub529 alignment \ub2a5\ub825\uc774 \ub9e4\uc6b0 \uc720\uc6a9\ud568."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"\uadf8\ub7ec\ubbc0\ub85c \uc800 \ub450 \ub2a5\ub825\uc744 \uc720\uc9c0\ub418\ub3c4\ub85d lm-based embedder \ud29c\ub2dd\ud558\ub294 \ub17c\ubb38\uc784."}),"\n",(0,s.jsx)("div",{style:{textAlign:"center"},children:(0,s.jsx)("img",{src:t,style:{width:500}})}),"\n",(0,s.jsx)(n.p,{children:"llm-based embedder \uac00 \uc544\ub2c8\ub77c MLP \ub97c \ud558\ub098 \ubd99\uc5ec\uc11c \uc598\ub9cc \ud559\uc2b5\uc2dc\ud0b4."}),"\n",(0,s.jsx)(n.p,{children:"1\ub2e8\uacc4\ub294 CLIP-text-encdoer \uc640 llm-based embedder \uc758 alignment \ud29c\ub2dd \uc791\uc5c5."}),"\n",(0,s.jsx)(n.p,{children:"2\ub2e8\uacc4\ub294 text-image alignment \uac00 \ub9de\ub3c4\ub85d CLIP \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ud568."}),"\n",(0,s.jsxs)(n.h2,{id:"4-dsi-bench",children:["4. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.18873",children:"DSI-BENCH"})]}),"\n",(0,s.jsx)(n.h3,{id:"a-benchmark-for-dynamic-spatial-intelligence",children:"A BENCHMARK FOR DYNAMIC SPATIAL INTELLIGENCE"}),"\n",(0,s.jsx)("div",{style:{textAlign:"center"},children:(0,s.jsx)("img",{src:a,style:{width:500}})}),"\n",(0,s.jsx)(n.p,{children:"Observer \uc640 Object \uac00 \ub3d9\uc2dc\uc5d0 \uc6c0\uc9c1\uc774\ub294 \ud604\uc2e4\uc801\uc778 3D \uc2dc\ub098\ub9ac\uc624 \ub370\uc774\ud130 \ubca4\uce58\ub9c8\ud06c."}),"\n",(0,s.jsxs)(n.h2,{id:"5-video-reasoning-without-training-qualcomm",children:["5. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2510.17045",children:"VIDEO REASONING WITHOUT TRAINING"})," ",(0,s.jsx)(n.code,{children:"qualcomm"})]}),"\n",(0,s.jsx)(n.p,{children:"\ud004\ucef4\uc774 \uac11\uc790\uae30 \uc660 \ub17c\ubb38\uc744."}),"\n",(0,s.jsx)(n.p,{children:"\ud6c8\ub828 \uc5c6\uc774 Video reasoning \ud488\uc9c8\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\ub860. \ud55c \uac00\uc9c0 \uad00\ucc30\uc5d0\uc11c \uc544\uc774\ub514\uc5b4\ub97c \uc5bb\uc74c."}),"\n",(0,s.jsx)(n.p,{children:"\ubaa8\ub378\uc758 \ucd9c\ub825 \uc5d4\ud2b8\ub85c\ud53c"})]})}function g(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},d=r.createContext(s);function t(e){const n=r.useContext(d);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(d.Provider,{value:n},e.children)}}}]);