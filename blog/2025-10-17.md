
import blog_20251017_img0 from './asset/blog_20251017_img0.png';
import blog_20251017_img1 from './asset/blog_20251017_img1.png';
import blog_20251017_img2 from './asset/blog_20251017_img2.png';

# 10-17 papers summary

## 1. UniFusion: Vision-Language Model as Unified Encoder in Image Generation `Adobe`

prompt base 이미지 편집 시 텍스트와 이미지 별도 인코더를 사용하는 부분으로 인해 모델 역량이 임베딩 aligning 에 소모된다. 

frozen VLM 을 통합된 멀티모달 인코더로 사용해서 임베딩을 추출하고 이로부터 이미지를 diffusion 방식을 생성함. 

구체적으로는 frozen VLM 에 이미지와 프롬프트 전체를 인입으로 사용해 모델 임베딩 추출. 이 때 하위 레이어부터 상위 레이어까지 다중 레이어 임베딩을 추출한다. 그 후 모든 임베딩을 통합해주는 Layer Attention Pooling (LAP) 모듈을 사용. 여러 레이어의 임베딩을 단일 임베딩을 통합시켜줌.

LAP 모델의 의의는 VLM 의 정보를 효과적으로 추출하고 통합하는 것. 특히, shallow layer 에서 강조되는 디테일, 세부사항이 last embedding 만 사용하는 과정에서 무시되어서 편집 작업에서 내용이 부자연스러워지는 결과를 낳았음. 

몇 가지 방식을 테스트함.

- (as-is) last layer hidden state encoding
- layer wise key-value fusion : 각 layer 에서 k, v 를 concat 해서 사용. 
- hidden state injection
- layerwise attention pooling 
  - transformer block 생각하면 됨.

결과적으로 통합 인코더 설계를 통해 zero-shot 성능이 크게 올랐음. 또한, 편집 작업에 대한 파인튜닝 성능 역시 개선됨.

## 2. SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model `bytedance` 

bytedance 뭐하나 체크용으로 읽어봄.

일단 기존 MLLM 모델이 industry 환경에서는 좋지 않다고 주장. 공감하는 부분. omni-modal 까지 추가해서 모델을 학습시켰고 이를 통해 item-to-item, query-to-item 성능이 높아짐. 특히 **비디오 기반 멀티미디어 검색에서 오디오 양식의 중요성**을 확인함.

데이터 구축이 중요하다고 봐서 좀 더 보긴함.

일단 10B 이상 i2i, q21, classification (검색 분류) 포함하는 데이터 셑을 구축함. dynamic hard negative mining 과 adaptive multi-score data balance 방식 도입. 

- dynamic hard negative minig
  - 고정된 전역 임계값 대신, 각 데이터셑의 특성과 테스크에 맞게 최적의 유사성 임계값을 결정함. 
  - 초기 모델을 통해을 통해서 모델이 가장 어려워하는 영역을 확인하고, 이 영역을 더 잘 구분하는 loss 를 추가해서 학습함.
- adaptive multi-source data balancing
  - 다양한 출처와 특성을 가진 훈련 데이터셑의 샘플링 가중치를 데이터 분포에 기반하여 자동으로 설정.
  - 검증 셋과 훈련 셋의 semantic similarity 를 측정.
  - 검증 셋과 유사한 훈련 셋일수록 더 가중치를 주어서 학습 -> 음...

## 3. HoneyBee: Data Recipes for Vision-Language Reasoners `meta`

고성능 VL 데이터셑 구축을 위한 프렝임 워크를 공개. 방법론이 꽤 복잡하나 "데이터의 설계" 라는 측면에서 아이디어를 잘 관조해볼만하다. 실제 산업 현장에서는 데이터의 구축에 충분히 심혈을 기울이기 힘들다. 작업의 일정이 있으니 인사이트와 감각, 경험에 의존하는 면이 많다.

크게 3단계로 데이터 구축 과정을 정의함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251017_img0} style={{width: 500}} />
</div>

context curation -> data interventions -> scalining.

- context curation
  - 데이터 수집과 검증.
  - Sourcing : 데이터셑을 먼저 수집하고 기본적인 정제.
    - CoT : 고성능 VLM 을 이용해서 수집된 이미지-쿼리에 맞는 CoT 를 생성한다.
    - 품질 : 여러 VLM 모델에서 다운 스트림 추론 테스크에 대한 평균 성능 기반으로 각 소스 데이터셋의 품질을 평가하고 순위를 매긴다.
  - Mixing : 최상위 성능 소스 데이터셋들의 CoT 를 혼합하여 개별 소스보다 더 나은 성능을 달성하는지 평가. 단일 ViRL 을 능가하지 못했다고 함. ( * ViRL : 데이터셑 이름임 )
  - * 근데 학습 시켜서 비교하는건 사실 어렵지..

- Data intervention
  - 최고 성능 데이터셋 기반으로 perception / problem-solving 능력 향상시키기 위해 맞춤형 개입 전략들을 평가.
  - perception : 이미지 회전, 방해물 추가, 이미지-질문 렌더링, 이미지 없이도 풀 수 있는 문제 제거.
    - 최종 선택된건 CoT 시작부분에 이미지 캡션을 포함하여 보조적인 시각 신호를 제공. -> 이미지 캡션은 stronger generator 모델을 사용하여 생성
  - Problem-Solving : 선택지 증가, 긴 CoT 만 남기기, 난이도 균등화.
    - 텍스트 전용 추론 데이터와 VL 전용 추론 데이터를 교차 태스크 전이 학습을 유도함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251017_img1} style={{width: 500}} />
</div>

- Scaling
  - CoT 데이터의 확장.
  - 고유 이미지 수 늘리기, (이미지, 질문) 기반으로 새로운 질문 합성하기, 여러개의 CoT 생성하기, 전략 조합 등.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251017_img2} style={{width: 500}} />
</div>

흠 역시 pretrain 과 task-specific post-train 사이에는 엄청난 간극이 있는 것 같다. pretrain 에서 최고의 전략과 posttrain 의 최고의 전략은 확실히 다르다. [이 글](/blog/2025-10-07.md) 의 **Front-Loading Reasoning `nvidia`** 만 봐도 알 수 있는 내용이지만...