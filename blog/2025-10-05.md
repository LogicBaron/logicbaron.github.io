import blog_20251005_img0 from './asset/blog_20251005_img0.png';

# 10-05 papers summary

## Winning the Pruning Gamble
### A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning  `alibaba`

LLM의 Supervised Fine-Tuning(SFT)은 단순한 후처리가 아니라 중간 훈련(mid-training) 수준의 계산비용이 필요한 단계가 되었다. post-training 단계에서 데이터 효율성은 매우 치명적인 이슈다. 이 내용은 내가 블로그, 또는 논문 정리를 하면서 작성했던 적이 있다. 일반적으로는 **data pruning** 방식을 사용한다. data pruning 사용 시 일반적으로 두 가지 방법 중 하나를 선택한다. 샘플 단위 혹은 토큰 단위. 두 가지 방식의 장단점이 있는데, 본 논문에서는 두 방식의 장점을 합친 방식을 제안한다.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251005_img0} style={{width: 500}} />
</div>

논문의 핵심은 entropy 와 perplexity 를 통해 데이터를 4가지로 분류할 수 있고, 각 데이터에 대해 서로 다른 pruning 방법을 적용해야 한다는 것. 

entropy 와 perplexity 가 전부 낮다. -> 이미 잘 알고있는 정보. 학습 중요도가 낮음.
entropy 와 perpelxity 가 둘다 높다. -> 노지으일 확률이 높은 데이터. 학습에서 제외.
entopy 는 낮으나 perplexity 는 높은 데이터 -> 모델이 확실하게 틀린 답을 내놓는 경우. decisoin boundary 이므로 학습 가치가 높음. 오개념을 수정하는 방향으로 학습.
entropy 는 높고 perplexity 는 높은 데이터 -> 모델이 불확실하다고 하며 틀린 답을 내놓는 경우가 많음. calibration 이 잘되고 있는 데이터. 그냥 사용.

## Long live
### REAL-TIME INTERACTIVE LONG VIDEO GENERATION `nvidia`

autoregressive 방식으로는 긴 영상을 잘 만들기 어려움. 근데 여기다가 prompt interaction 을 추가하기는 더 어려움. 그렇다고 diffusion 모델을 사용할수도 없다. 너무 느리니까.

논문에서는 prompt interactive video generation 방식을 소개함. 기본적으로 autoregressive 방식을 사용. 

autoregressive 방식을 사용.

prompt 전환시 kv-recahce 방식 사용. 이전 프레임의 시각 맥락은 유지하면서, prompt 의미 맥락만을 재활용.

steaming long-tuning : 과거 kv 를 이어받은 후 autoregressive 모델을 이용해 5초 단위 roll-out 생성 + DMD alignment

short video attention + frame sink 을 이용해서 영상 전반적인 일관성 유지 : 짧은 비디오 문맥만 파악하되, 영상 전반적으로 중요한 정보는 고정 키-밸류로 계속 참조시킴.

20.7FPS + 240 초까지 영상 생성했다고 함.

## OpenGPT-4o-Image

GPT4o-image 의 테크 리포트이자, 데이터를 어떻게 생성해서 어떻게 학습시켰는지에 대한 논문. 데이터 생성 방식의 중요성이 특히 pretraiing 에서 점점 커지고 있는 거 같다.

gpt-4o-image 는 이미지-테스트 멀티모달 관련 데이터셑을  충분히 만들기 위해 먼저 필요한 과제들을 매우 세세하게 체계화시킴. 각 과제들을 분류하고 또 과제들을 하위 과제로 체계화시켰음. 예를 들어서 인과 추론의 경우 명시적인 추론, 암묵적인 추론, 복합 인과 사슬등으로 세분화함. 저자들은 이 과정이 매우 중요하다고 하는데, 이렇게 분류를 해놓아야 명확하게 템플릿화된 프롬프트를 통해 대량의 학습 데이터를 생성할 수 있기 떄문임.

먼저 소스 이미지를 충분히 수집함. 실제 이미지 데이터 뿐 아니라 omniEdit, ImgEdit, gpt-4o 등도 활용함.
- https://tiger-ai-lab.github.io/OmniEdit/
- https://github.com/PKU-YuanGroup/ImgEdit

다만, 모델 학습 일관성을 위해 특정 모듈 학습에는 특정 소스의 데이터만 사용함. 예를 들어, subject-driven generation 에서는 OmniEdit 소스 데이터만 활용.

그 후, 체계화시킨 템플릿 기반으로 모듈별로 대량의 prompt 를 생성함. 

원본 이미지와 prompt 를 활용해서 gpt-1-image-api 를 활용해서 생성한 데이터를 사용함. reference image editing 의 경우 reference image, edited image 를 가지고 source image 를 다시 생성해내는 과정을 가짐. 그냥 학습 시켰을 때 잘 안되서 추가한 것으로 보임. gpt-1-api 는 비공개 모델. 

시스템화된 데이터 생성 파이프라인을 통해 고품질의 학습 데이터를 만들어서 모델 학습시킴으로써 오픈 소스 생태계 확장 및 distillation 의 중요성을 보여준다.