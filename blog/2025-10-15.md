# 10-15 papers summary

연휴가 끝나니 확실히 시간이 적다.

## Are Large Reasoning Models Interruptible?

- 최근의 LRM(Large Reasoning Models)은 긴 사고 체인을 생성하지만, 계산량이 매우 크고 중단(interruption) 시 활용이 어렵다.
- 실제 응용에서는 부분적인 사고만으로도 충분한 답변이 가능한 경우가 많음에도 불구하고, 전체 reasoning을 수행해야 결과를 얻을 수 있다.
- 모델의 reasoning trace(사고 과정)를 실시간으로 모니터링하며, 다양한 시점에서 interruption을 적용.
- 각 시점의 partial reasoning output을 통해 최종 정답 정확도와 reasoning 품질을 측정.
- 또한, interruption-aware decoding을 도입해 중단 이후에도 자연스러운 결과를 생성할 수 있도록 훈련.
- LRM은 **일정 길이 이전에는 reasoning quality가 급격히 떨어지지만, 특정 길이 이상에서는 점진적 개선만 존재함**을 발견.
- 즉, “충분한 사고 이후 중단”이 효율적 계산-성능 절충을 가능하게 한다.
- 일부 모델은 interruption-aware 학습 시, 중단 시점에서도 90% 이상의 정답률을 유지.

## QeRL: Beyond Efficiency
### Quantization-enhanced Reinforcement Learning for LLMs

- 기존 양자화(Quantization)는 모델 효율성은 높이지만, 성능 손실이 발생한다.
- 반면, RLHF(강화학습 기반 미세조정)는 성능 향상엔 도움이 되지만 계산 비용이 크다.
- 본 논문은 양자화가 단순한 효율화 도구가 아니라, 학습 성능을 높일 수 있는 조력자로 작동할 수 있음을 보인다.

- 양자화 과정을 RLHF 루프에 통합하여 모델을 효율적이면서도 더 강하게 학습시킨다.
- **양자화 잡음(quantization noise)을 exploration으로 활용해 policy diversity를 높인다.** -> 이게 가능해..?
- 양자화가 gradient update 시 regularization처럼 작용해 안정적인 학습을 유도한다.
- RL과 양자화의 상호작용을 수학적으로 분석하여 정량적 이득을 입증.
- QeRL은 기존 RLHF 대비 동일한 계산량으로 더 높은 reward 모델 점수를 달성. ->  3~5% 향상, 추론 효율은 2배 이상 개선.
- INT8 또는 INT4 수준의 압축에서도 성능 저하가 거의 없으며, 일부 과제에서는 향상.

> “Quantization noise acts as a stochastic perturbation that promotes exploration in the policy space.”