# LLM finetuning

LLM finetuning을 과제에서 진행해야 해서 수행해봤다. 물론 팀원분들이 짜주신 코드가 있었지만 그래도 생각보다 엄청 쉽게 돌아가는 라이브러리들이 잘 작성되어 있다는 사실이 신기했다. 예전에 공부하고 사용은 안 했던 deepspeed도 본격적으로 사용되는 현상도 봤다.

LORA, peft, deepspeed, SFT 등의 기술을 사용했는데 급하게 진행한다고 기술 내용까지는 상세히 못봤다. 또 LLM 논문에서 사용하는 metrics도 이해가 잘 안됐고, 마치 처음 DeepLearning을 공부하던 시절로 돌아간 것 같았다. 아직 Deep learning 분야에서도 공부해야할 것들이 많은데 LLM 쪽 기술 공부를 따라잡아야한다는 생각이 강하게 들었다. 

GPT technical Report 가 정석같은 책이지만 [HCX Technical Reprot](https://arxiv.org/pdf/2404.01954.pdf) 먼저 쭉 읽으며 공부를 주말부터 본격적으로 시작해보려 한다.