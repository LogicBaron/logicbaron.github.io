
import blog_20251007_img0 from './asset/blog_20251007_img0.png';

# 10-07 papers summary

## THE DRAGON HATCHLING
### THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN

개인적으로 흥미가 땡기는 논문. 보다 인간 뇌 뉴런 구조를 잘 모방한 AI 모델 구조를 연구한 논문이다. 최근 다양한 분야에서 연구되고 있는 장기 기억을 담당하는 : 예, long kv cache , 별도 장기기억 메모리 - 뉴런을 모방한 구조.

	•	BDH (Brain-like Dense Hierarchy): 뉴런 활성값을 벡터 공간으로 확장해, dense activation과 sparse positive activation을 결합.
	•	BDH-GPU: 병렬 계산에 맞춘 GPU 버전, 계층적 신경 interaction을 분산 병렬로 처리.
	•	Grandfather Neurons: 핵심 패턴을 장기 유지하는 메모리 유닛.
	•	Dense activation → “즉각적 반응”, Sparse activation → “장기 기억” 역할.

## Judging with Confidence `google`
### Calibrating Autoraters to Preference Distributions

기존 llm autorator 는 A/B 둘 중 하나를 선택하는 0/1 label 방식 선택함. 하지만 실제 사람들의 선호도는 딱 떨어지는 게 아니라 확률적으로 나뉘어짐. 해당 논문은 LLM autorator 가 선호도의 확률 분포를 반영할 수 있도록 학습시킴. 

예) "A와 B중 A가 70% 더 사람들에게 선호된다."

먼저, 데이터를 만듬. 답변의 선호 분포가 실제로 있는 케이스 그리고 선호 분포가 없는 "단일 답변" 마 소유하고 있는 케이스. 당연히 답변 선호 분포 데이터가 훨씬 만들기 어렵다.

답변 선호 분포를 아는 경우 **SFT** 방식으로 학습시킴. 그냥 regression 처럼 학습시켰다 보면 됨.

단일 답변인 경우 **RL** 방식을 사용함. binary classification 과 비슷. 답변의 선호 확률을 잘 맞춰야함. 답변이 1일 경우 확률을 1에 가깝게, 답변이 0일 경우 확률을 0에 가깝게. 별 효과가 있을까 의문이 드는 방식인데 논문에서는 이 방식으로도 충분히 의미있는 효과를 보여줬다고 함. RL 방식에서 loss 는 quadratic 과 cross-entropy 를 함께 사용. RL 이라 이름 붙인 이유는 loss 가 아니라 reward 형태로 학습시켰기 때문에. (RLHF)

결과적으로,
- 정량 지표 : MSE – 18~51% 감소, ECE – 44~45% 감소, 편향 감소.
- 인간 평가 일치도 : RL-Brier 모델이 GPT-4 및 JudgeLM 보다 우수.
- 객관 과제 : JudgeBench 정확도 46.6% 로 기존 모델과 동등 수준 유지 → 추론 능력 손상 없음.

## SWIREASONING `microsoft`
### SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS

pareto-optimal 또는 pareto-frontier 는 어떤 지표를 좋게 하려면 어떤 지표를 희생해야 하는 상태. 반대로 본 논문은 pareto 관계의 두 가지 지표를 모두 상승 시켜 pareto-superior 라고 이름붙임.

모델의 잠재 사고 과정은 token effciency 가 높지만 명시 사고보다 정확도가 떨어짐. 이를 해결하기 위해서 switch module 을 도입. 

<div style={{textAlign: 'center'}}>
 <img src={blog_20251007_img0} style={{width: 500}} />
</div>

모델이 헷갈려하는 순간, 즉 entropy 가 높아지는 순간은 모델이 명시적으로 사고하도록 하고 모델의 entropy 가 낮은 순간은 모델이 잠재 사고를 하도록함. 그리고 너무 많은 switch 가 일어나지 않도록 control.

이를 통해 정확도와 토큰 효율 둘다 달성했다고 함.


## Hybrid Architectures for Language Models `meta`
### Systematic Analysis and Design Insights

카이스트와 메타가 써서 흥미가 든 논문. transformer 와 mamba 구조의 장점을 잘 조합한 새로운 구조를 제안함. 나와 관련이 낮아 보여 가볍게 요약만 확인함.

## Front-Loading Reasoning `nvidia`
### The Synergy between Pretraining and Post-Training Data 

엔비디아니까 할만하다고 생각되는 규모의 대규모 실험. "추론 데이터" 를 언제, 어떻게 사용해야 최적의 효과를 볼 수 있는지에 대한 전방위적 분석. 기존 연구는 post-training 단계에 집중해서 분석하나 해당 논문은 pretraining 단계까지 분석 범위에 포함시켰고 다양한 재밌는 결과를 확인했다고 주장한다.

결론부터 말하면, 다양한 대규모 추론 데이터를 사전 학습 단계에 전방 배치하고 (fron-load) SFT 단계에서는 고품질 데이터를 사용하는 것이 최적이라는 것. 기존의 **추론 학습은 post-training (SFT, RL) 단계에서만 중요하다** 라는 관점을 정량지표와 함께 반박. 최대 19% 까지 수학, 과학, 코딩 벤치마크 지표 우위를 보였음.

pretraining 단계에서 대규모의 다양한 추론 데이터의 중요성과 함께 재밌는 지점도 많이 적혀있다. 

1. 사전학습 단계에서 추론을 연습하지 않으면, SFT 단계에서 추론 데이터를 2배까지 차이를 내도 추론을 사전 연습한 모델을 따라잡지 못한다. 즉, 사전 추론 학습이 모델의 추론 성능 상한선을 결정한다.
2. 사전학습에서는 다양성과 규모가 중요하고, SFT 에서는 품질이 중요하다.
3. 사전 학습 단계 데이터에서 고품질 데이터의 중요성 -> 사전 학습 대규모 추론 데이터에 고품질 데이터의 포함 유무에 따른 모델의 성능 차이는 바로 드러나지 않는다. 그러나 SFT 이후에 효과가 드러난다. 즉 사전 학습 추론 데이터 중 고품질 데이터는 "잠재적" 으로 모델에 좋은 영향을 미친다.

LLM pretraining 을 직접할 일은 거의 없겠지만 모델이 동작에서 얻어갈 수 있는 인사이트가 있을 것으로 보인다.

```
“We define pretraining as large-scale self-supervised learning over a mixture of general and reasoning data,
and post-training as any supervised fine-tuning or reinforcement stage that aligns the model toward specific reasoning quality.”
```