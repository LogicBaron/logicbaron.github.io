import blog_20251003_img0 from './asset/blog_20251003_img0.png';
import blog_20251003_img1 from './asset/blog_20251003_img1.png';

# 10-03 논문

메인으로 읽으려고 하는 논문은 두 편.

## VLA-RFT
### VISION-LANGUAGE-ACTION REINFORCEMENT FINE-TUNING WITH VERIFIED REWARDS IN WORLD SIMULATORS

VLA, vision-language-action 모델은 모방학습에 의존해서 다른 환경(분포) 에서 사용하기 어려웠고 일반화와 안정성이 부족했음. RL은 도움이 되나 현실적인 문제에 부딪힘. 실제 환경에서 실행하기 어렵거나, 시뮬레이션시 격차 이슈가 있었음. 

데이터 기반 world model 이 많이 발전함. 데이터가 많이 모였고 모델 구조도 개선됨. 그래서 world model 을 시뮬레이터로 사용해 행동 시퀀스로 미래 상태를 예측하고 GRPO 기반으로 최적화함.

결과적으로 400회 미만 파인튜닝으로 강력한 모델을 학습함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251003_img0} style={{width: 500}} />
</div>

## BEYOND LOG LIKELIHOOD
### PROBABILITY-BASED OBJECTIVES FOR SUPERVISED FINE-TUNING ACROSS THE MODEL CAPABILITY CONTINUUM

https://arxiv.org/pdf/2510.00526

SFT 방식은 주로 negative log likelihood minimize 가 목표함수이지만, 이는 pretrained LLM 의 성질과 맞지 않아 일반화 성능이 제약된다고 주장함.

NLL 은 "약한 모델" 에 최적화된 학습 방법론임. 이론적인 설명과 경험적인 설명 두 가지.
- 이론적인 설명 
  - NLL 은 정답 토큰 확률이 낮을수록 gradient 가 커짐. 즉, 모델이 정답을 low-probability 로 예측할 때 효과적인 방법.
  - 모델이 약할때는 정답이 낮은 확률로 나올 가능성이 크므로 강한 학습 신호를 주는게 유리함.
  - 반면 모델이 강할떄는 이미 정답의 확률이 높음. NLL 은 오히려 노이즈나 아웃라이어에 강한 신호를 줘서 오버피팅이나 부작용(e.g. hallucination)을 유발함.
- 경험적 설명.
  - 다양한 도메인에서 실험함.
  - pretrained LLM 이 이미 충분한 prior 정보를 가지고 있는 분야일수록 확률이 이미 높은 토큰에 더 큰 웨이트를 주는게 유효하다고 함.
  - 실제로 top-10% 토큰만 gradient 에 영향을 미치게할 때 가장 학습이 잘된다고 함.

실험은 수학처럼 모델이 pretrained 에서 충분히 배우는 문제, 의학처럼 중간 수준 그리고 퍼즐같이 약한 수준으로 분류함. 

강한 prior 일수록 prior-leaning -> 즉 확률이 높은 확률을 더 많이 고려하는게 중요하다고 함. 단순하게 top-10% 토큰만 고려하기, $-p$, 또는 $-p^k$ 같은 함수들. (concave)

## WHY CAN’T TRANSFORMERS LEARN MULTIPLICATION


transformer 구조가 곱셈을 못하는 이유? 특히 4자리 수 이상.

4자리 이상 곱셈부터는 long-range dependency 를 오래 유지해야하는덱 구조상 어려움. chain of thoughts 구조를 활용하면 이를 어느정도 해결가능함. 저자들은 CoT 구조를 모델 내부 attnetion 에 내재적으로 반영하고 싶었음. 예를 들어서, 곱셈의 중간 단계 값들이 attnetion 내부에 저장되도록.

그러기 위해서 실질적인 학습은 학습 초기에는 곱셈의 중간 단계를 전부 생성하도록 하고, 갈수록 중간 단계를 없애는 식으로 학습시킴.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251003_img1} style={{width: 500}} />
</div>

## GEM: A GYM FOR AGENTIC LLMS

GEM은 에이전틱 LLM을 위한 표준 멀티턴 환경/툴킷으로, 다양한 태스크·툴·비동기 실행을 제공해 경험 기반 RL을 실용화한다. 특히 멀티턴에서 REINFORCE+ReBN이 간단·강력한 베이스라인임을 보이며, γ·툴 접근 같은 RL 핵심 설계를 체계적으로 비교·검증한다. 요컨대 **“환경 표준화 + 가벼운 알고리즘 + 툴 연동”**으로 에이전틱 LLM의 연구·벤치마크·실험을 한 자리에서 가능하게 한다.