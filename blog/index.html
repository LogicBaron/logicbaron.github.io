<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">Blog | Logic Baron</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" name="twitter:image" content="https://logicbaron.github.io/img/logicbaron_social_card.jpg"><meta data-rh="true" property="og:url" content="https://logicbaron.github.io/blog"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Logic Baron"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/logicbaron_32.ico"><link data-rh="true" rel="canonical" href="https://logicbaron.github.io/blog"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://logicbaron.github.io/blog" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"Blog","@id":"https://logicbaron.github.io/blog","mainEntityOfPage":"https://logicbaron.github.io/blog","headline":"Blog","description":"Blog","blogPost":[{"@type":"BlogPosting","@id":"https://logicbaron.github.io/blog/2025/10/17/","mainEntityOfPage":"https://logicbaron.github.io/blog/2025/10/17/","url":"https://logicbaron.github.io/blog/2025/10/17/","headline":"10-17 papers summary","name":"10-17 papers summary","description":"1. UniFusion: Vision-Language Model as Unified Encoder in Image Generation Adobe","datePublished":"2025-10-17T00:00:00.000Z","author":[],"keywords":[]},{"@type":"BlogPosting","@id":"https://logicbaron.github.io/blog/2025/10/16/","mainEntityOfPage":"https://logicbaron.github.io/blog/2025/10/16/","url":"https://logicbaron.github.io/blog/2025/10/16/","headline":"10-16 papers summary","name":"10-16 papers summary","description":"1. Detect Anything via Next Point Prediction","datePublished":"2025-10-16T00:00:00.000Z","author":[],"keywords":[]},{"@type":"BlogPosting","@id":"https://logicbaron.github.io/blog/2025/10/15/","mainEntityOfPage":"https://logicbaron.github.io/blog/2025/10/15/","url":"https://logicbaron.github.io/blog/2025/10/15/","headline":"10-15 papers summary","name":"10-15 papers summary","description":"연휴가 끝나니 확실히 시간이 적다.","datePublished":"2025-10-15T00:00:00.000Z","author":[],"keywords":[]},{"@type":"BlogPosting","@id":"https://logicbaron.github.io/blog/2025/10/07/","mainEntityOfPage":"https://logicbaron.github.io/blog/2025/10/07/","url":"https://logicbaron.github.io/blog/2025/10/07/","headline":"10-07 papers summary","name":"10-07 papers summary","description":"THE DRAGON HATCHLING","datePublished":"2025-10-07T00:00:00.000Z","author":[],"keywords":[]},{"@type":"BlogPosting","@id":"https://logicbaron.github.io/blog/2025/10/05/","mainEntityOfPage":"https://logicbaron.github.io/blog/2025/10/05/","url":"https://logicbaron.github.io/blog/2025/10/05/","headline":"10-05 papers summary","name":"10-05 papers summary","description":"Winning the Pruning Gamble","datePublished":"2025-10-05T00:00:00.000Z","author":[],"keywords":[]}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Logic Baron RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Logic Baron Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Logic Baron JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Logic Baron" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.1bb1e14b.css">
<script src="/assets/js/runtime~main.3b7a2469.js" defer="defer"></script>
<script src="/assets/js/main.2b6eddc3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logicbaron.svg" alt="EyeStone Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LogicBaron</b></a><a class="navbar__item navbar__link" href="/docs/community/hello">Hello, Baron</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Concept</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/concepts/math/introduction">Math</a></li><li><a class="dropdown__link" href="/docs/concepts/mlconcept/introduction">Machine Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/deeplearning/introduction">Deep Learning</a></li><li><a class="dropdown__link" href="/docs/concepts/largemodel/introduction">Large Model</a></li><li><a class="dropdown__link" href="/docs/concepts/programming/introduction">Programming</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/data/image/introduction">Image</a></li><li><a class="dropdown__link" href="/docs/data/text/introduction">Text</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/models/mlmodel/pca">ML Models</a></li><li><a class="dropdown__link" href="/docs/models/aimodel/intro">AI Models</a></li><li><a class="dropdown__link" href="/docs/models/largemodel/introduction">Large Models</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Practice</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/practice/torch/intro">Torch</a></li><li><a class="dropdown__link" href="/docs/practice/efficienttrain/Efficient Train">Efficient Train</a></li><li><a class="dropdown__link" href="/docs/practice/mlops/intorduction">MLOPs</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Tasks</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/tasks/recommendation/hello">Recommendation</a></li><li><a class="dropdown__link" href="/docs/tasks/informationextraction/hello">Information Extraction</a></li><li><a class="dropdown__link" href="/docs/tasks/retrieval/intro">Retrieval</a></li><li><a class="dropdown__link" href="/docs/tasks/knowledgegraph/knowledgegraph">Knowledge Graph</a></li><li><a class="dropdown__link" href="/docs/tasks/llm&amp;prompt/intro">LLM &amp; Prompt</a></li></ul></div></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a href="https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">ALL posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/17/">10-17 papers summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/16/">10-16 papers summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/15/">10-15 papers summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/07/">10-07 papers summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/05/">10-05 papers summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/04/">10-04 papers summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/10/03/">10-23 Papers Summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/09/22/">09-23 Papers Summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/09/18/">09-18 Papers Summary</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/09/14/">LLM 한국어 튜닝 OpenAI cookbook</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/03/31/">Best_of vs BeamSearch</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2025/03/17/">Qwen 에서 중국어 없애기.</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2024/07/22/">DuckDB</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2024/04/04/">LLM finetuning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2024/02/14/">MatPlotlib 한글 설정 간편하게 하기</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/11/09/">tmux 수동 설치</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/11/07/">웹데이터 뷰어 만들기 (feat. postgres)</a></li></ul></div></nav></aside><main class="col col--7"><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/2025/10/17/">10-17 papers summary</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-17T00:00:00.000Z">October 17, 2025</time> · <!-- -->4 min read</div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-unifusion-vision-language-model-as-unified-encoder-in-image-generation-adobe">1. UniFusion: Vision-Language Model as Unified Encoder in Image Generation <code>Adobe</code><a href="#1-unifusion-vision-language-model-as-unified-encoder-in-image-generation-adobe" class="hash-link" aria-label="Direct link to 1-unifusion-vision-language-model-as-unified-encoder-in-image-generation-adobe" title="Direct link to 1-unifusion-vision-language-model-as-unified-encoder-in-image-generation-adobe" translate="no">​</a></h2>
<p>prompt base 이미지 편집 시 텍스트와 이미지 별도 인코더를 사용하는 부분으로 인해 모델 역량이 임베딩 aligning 에 소모된다.</p>
<p>frozen VLM 을 통합된 멀티모달 인코더로 사용해서 임베딩을 추출하고 이로부터 이미지를 diffusion 방식을 생성함.</p>
<p>구체적으로는 frozen VLM 에 이미지와 프롬프트 전체를 인입으로 사용해 모델 임베딩 추출. 이 때 하위 레이어부터 상위 레이어까지 다중 레이어 임베딩을 추출한다. 그 후 모든 임베딩을 통합해주는 Layer Attention Pooling (LAP) 모듈을 사용. 여러 레이어의 임베딩을 단일 임베딩을 통합시켜줌.</p>
<p>LAP 모델의 의의는 VLM 의 정보를 효과적으로 추출하고 통합하는 것. 특히, shallow layer 에서 강조되는 디테일, 세부사항이 last embedding 만 사용하는 과정에서 무시되어서 편집 작업에서 내 용이 부자연스러워지는 결과를 낳았음.</p>
<p>몇 가지 방식을 테스트함.</p>
<ul>
<li>(as-is) last layer hidden state encoding</li>
<li>layer wise key-value fusion : 각 layer 에서 k, v 를 concat 해서 사용.</li>
<li>hidden state injection</li>
<li>layerwise attention pooling<!-- -->
<ul>
<li>transformer block 생각하면 됨.</li>
</ul>
</li>
</ul>
<p>결과적으로 통합 인코더 설계를 통해 zero-shot 성능이 크게 올랐음. 또한, 편집 작업에 대한 파인튜닝 성능 역시 개선됨.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-sail-embedding-technical-report-omni-modal-embedding-foundation-model-bytedance">2. SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model <code>bytedance</code><a href="#2-sail-embedding-technical-report-omni-modal-embedding-foundation-model-bytedance" class="hash-link" aria-label="Direct link to 2-sail-embedding-technical-report-omni-modal-embedding-foundation-model-bytedance" title="Direct link to 2-sail-embedding-technical-report-omni-modal-embedding-foundation-model-bytedance" translate="no">​</a></h2>
<p>bytedance 뭐하나 체크용으로 읽어봄.</p>
<p>일단 기존 MLLM 모델이 industry 환경에서는 좋지 않다고 주장. 공감하는 부분. omni-modal 까지 추가해서 모델을 학습시켰고 이를 통해 item-to-item, query-to-item 성능이 높아짐. 특히 <strong>비디오 기반 멀티미디어 검색에서 오디오 양식의 중요성</strong>을 확인함.</p>
<p>데이터 구축이 중요하다고 봐서 좀 더 보긴함.</p>
<p>일단 10B 이상 i2i, q21, classification (검색 분류) 포함하는 데이터 셑을 구축함. dynamic hard negative mining 과 adaptive multi-score data balance 방식 도입.</p>
<ul>
<li>dynamic hard negative minig<!-- -->
<ul>
<li>고정된 전역 임계값 대신, 각 데이터셑의 특성과 테스크에 맞게 최적의 유사성 임계값을 결정함.</li>
<li>초기 모델을 통해을 통해서 모델이 가장 어려워하는 영역을 확인하고, 이 영역을 더 잘 구분하는 loss 를 추가해서 학습함.</li>
</ul>
</li>
<li>adaptive multi-source data balancing<!-- -->
<ul>
<li>다양한 출처와 특성을 가진 훈련 데이터셑의 샘플링 가중치를 데이터 분포에 기반하여 자동으로 설정.</li>
<li>검증 셋과 훈련 셋의 semantic similarity 를 측정.</li>
<li>검증 셋과 유사한 훈련 셋일수록 더 가중치를 주어서 학습 -&gt; 음...</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-honeybee-data-recipes-for-vision-language-reasoners-meta">3. HoneyBee: Data Recipes for Vision-Language Reasoners <code>meta</code><a href="#3-honeybee-data-recipes-for-vision-language-reasoners-meta" class="hash-link" aria-label="Direct link to 3-honeybee-data-recipes-for-vision-language-reasoners-meta" title="Direct link to 3-honeybee-data-recipes-for-vision-language-reasoners-meta" translate="no">​</a></h2>
<p>고성능 VL 데이터셑 구축을 위한 프렝임 워크를 공개. 방법론이 꽤 복잡하나 &quot;데이터의 설계&quot; 라는 측면에서 아이디어를 잘 관조해볼만하다. 실제 산업 현장에서는 데이터의 구축에 충분히 심혈을 기울이기 힘들다. 작업의 일정이 있으니 인사이트와 감각, 경험에 의존하는 면이 많다.</p>
<p>크게 3단계로 데이터 구축 과정을 정의함.</p>
<div style="text-align:center"><img src="/assets/images/blog_20251017_img0-1d04a30efa34e41c30470cf7dd68d64d.png" style="width:500px"></div>
<p>context curation -&gt; data interventions -&gt; scalining.</p>
<ul>
<li>
<p>context curation</p>
<ul>
<li>데이터 수집과 검증.</li>
<li>Sourcing : 데이터셑을 먼저 수집하고 기본적인 정제.<!-- -->
<ul>
<li>CoT : 고성능 VLM 을 이용해서 수집된 이미지-쿼리에 맞는 CoT 를 생성한다.</li>
<li>품질 : 여러 VLM 모델에서 다운 스트림 추론 테스크에 대한 평균 성능 기반으로 각 소스 데이터셋의 품질을 평가하고 순위를 매긴다.</li>
</ul>
</li>
<li>Mixing : 최상위 성능 소스 데이터셋들의 CoT 를 혼합하여 개별 소스보다 더 나은 성능을 달성하는지 평가. 단일 ViRL 을 능가하지 못했다고 함. ( * ViRL : 데이터셑 이름임 )</li>
<li>
<ul>
<li>근데 학습 시켜서 비교하는건 사실 어렵지..</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Data intervention</p>
<ul>
<li>최고 성능 데이터셋 기반으로 perception / problem-solving 능력 향상시키기 위해 맞춤형 개입 전략들을 평가.</li>
<li>perception : 이미지 회전, 방해물 추가, 이미지-질문 렌더링, 이미지 없이도 풀 수 있는 문제 제거.<!-- -->
<ul>
<li>최종 선택된건 CoT 시작부분에 이미지 캡션을 포함하여 보조적인 시각 신호를 제공. -&gt; 이미지 캡션은 stronger generator 모델을 사용하여 생성</li>
</ul>
</li>
<li>Problem-Solving : 선택지 증가, 긴 CoT 만 남기기, 난이도 균등화.<!-- -->
<ul>
<li>텍스트 전용 추론 데이터와 VL 전용 추론 데이터를 교차 태스크 전이 학습을 유도함.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="text-align:center"><img src="/assets/images/blog_20251017_img1-87520f41c94f6033b59d8aaa26e6c890.png" style="width:500px"></div>
<ul>
<li>Scaling<!-- -->
<ul>
<li>CoT 데이터의 확장.</li>
<li>고유 이미지 수 늘리기, (이미지, 질문) 기반으로 새로운 질문 합성하기, 여러개의 CoT 생성하기, 전략 조합 등.</li>
</ul>
</li>
</ul>
<div style="text-align:center"><img src="/assets/images/blog_20251017_img2-ff31205b7e6836b298b8d93ba1e87ac3.png" style="width:500px"></div>
<p>흠 역시 pretrain 과 task-specific post-train 사이에는 엄청난 간극이 있는 것 같다. pretrain 에서 최고의 전략과 posttrain 의 최고의 전략은 확실히 다르다. <a href="/blog/2025/10/07/">이 글</a> 의 <strong>Front-Loading Reasoning <code>nvidia</code></strong> 만 봐도 알 수 있는 내용이지만...</p></div><footer class="row docusaurus-mt-lg"></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/2025/10/16/">10-16 papers summary</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-16T00:00:00.000Z">October 16, 2025</time> · <!-- -->2 min read</div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-detect-anything-via-next-point-prediction">1. Detect Anything via Next Point Prediction<a href="#1-detect-anything-via-next-point-prediction" class="hash-link" aria-label="Direct link to 1. Detect Anything via Next Point Prediction" title="Direct link to 1. Detect Anything via Next Point Prediction" translate="no">​</a></h2>
<p>기존 object detection 모듈은 크게 전통적인 detection 모델과 MLLM 모델로 나뉘어짐. detection 모델은 언어 이해 능력이 부족하고, MLLM 모델은 정밀한 위치 파악 능력이 부족함. MLLM 에서 사용하는 loss 는 좌표 예측에 불리하기도 함. (좌표값은 연속 값. regression 으로 학습하는 게 맞으나 loss 는 cross-entropy 임.)</p>
<p><strong>Rex-Omni</strong> 모델은 MLLM 을 사용하여, 모든 시각 작업을 좌표 예측 문제로 해결함. 학습 난이도와 토큰 효율성을 개선하기 위해 0부터 999까지의 양자화된 좌표를 특수 토큰으로 표현하고, 고품질 데이터 엔진을 구축.</p>
<p>SFT 1단계 -&gt; GRPO 2단계로 학습. GRPO 가 SFT 의 한계로 저자들이 지목한 기하학적 보상: IoU, Point-in-mask 등 - 을 추가함.</p>
<div><p>MLLM 또는 VLM 계열에서 object detection 의 퀄리티를 더욱 짜내는 듯한 느낌의 논문. industry level , 특히 쇼핑 도메인에서는 소잡는칼이 아닌지 고민해볼만함. MLLM 을 활용한 object detection 자체는 의미있을 듯.</p><p>다만 데이터 구축 과정에서 사용된 grounding dino 를 활용하는게 단기간에는 더 효율적일듯?</p></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search-apple">2. DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search <code>apple</code><a href="#2-deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search-apple" class="hash-link" aria-label="Direct link to 2-deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search-apple" title="Direct link to 2-deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search-apple" translate="no">​</a></h2>
<p>기존 MLLM 에서 VQA 해결할때의 두가지 문제점.</p>
<ul>
<li>이미지 전체만을 검색 쿼리로써 사용함.</li>
<li>RAG 검색 방식은 단일 웹 검색 결과를 사용하고 피드백이 없음.</li>
</ul>
<p>두 가지 문제점을 각각 해결함.</p>
<ul>
<li>Grounding DINO 를 이용한 Cropped Image Search 사용.</li>
<li>on-demand 방식의 multi-turn 웹 검색. (최대 6턴) -&gt; 자기 반성 / 자기 교정 과정을 통해서 멀티 턴을 결정.</li>
</ul>
<p>이를 통해 comparable with gpt-o3 성능 달성. SFT + GRPO 통해서 학습.</p></div><footer class="row docusaurus-mt-lg"></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/2025/10/15/">10-15 papers summary</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-15T00:00:00.000Z">October 15, 2025</time> · <!-- -->2 min read</div></header><div class="markdown"><p>연휴가 끝나니 확실히 시간이 적다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="are-large-reasoning-models-interruptible">Are Large Reasoning Models Interruptible?<a href="#are-large-reasoning-models-interruptible" class="hash-link" aria-label="Direct link to Are Large Reasoning Models Interruptible?" title="Direct link to Are Large Reasoning Models Interruptible?" translate="no">​</a></h2>
<ul>
<li>최근의 LRM(Large Reasoning Models)은 긴 사고 체인을 생성하지만, 계산량이 매우 크고 중단(interruption) 시 활용이 어렵다.</li>
<li>실제 응용에서는 부분적 인 사고만으로도 충분한 답변이 가능한 경우가 많음에도 불구하고, 전체 reasoning을 수행해야 결과를 얻을 수 있다.</li>
<li>모델의 reasoning trace(사고 과정)를 실시간으로 모니터링하며, 다양한 시점에서 interruption을 적용.</li>
<li>각 시점의 partial reasoning output을 통해 최종 정답 정확도와 reasoning 품질을 측정.</li>
<li>또한, interruption-aware decoding을 도입해 중단 이후에도 자연스러운 결과를 생성할 수 있도록 훈련.</li>
<li>LRM은 <strong>일정 길이 이전에는 reasoning quality가 급격히 떨어지지만, 특정 길이 이상에서는 점진적 개선만 존재함</strong>을 발견.</li>
<li>즉, “충분한 사고 이후 중단”이 효율적 계산-성능 절충을 가능하게 한다.</li>
<li>일부 모델은 interruption-aware 학습 시, 중단 시점에서도 90% 이상의 정답률을 유지.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="qerl-beyond-efficiency">QeRL: Beyond Efficiency<a href="#qerl-beyond-efficiency" class="hash-link" aria-label="Direct link to QeRL: Beyond Efficiency" title="Direct link to QeRL: Beyond Efficiency" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-enhanced-reinforcement-learning-for-llms">Quantization-enhanced Reinforcement Learning for LLMs<a href="#quantization-enhanced-reinforcement-learning-for-llms" class="hash-link" aria-label="Direct link to Quantization-enhanced Reinforcement Learning for LLMs" title="Direct link to Quantization-enhanced Reinforcement Learning for LLMs" translate="no">​</a></h3>
<ul>
<li>
<p>기존 양자화(Quantization)는 모델 효율성은 높이지만, 성능 손실이 발생한다.</p>
</li>
<li>
<p>반면, RLHF(강화학습 기반 미세조정)는 성능 향상엔 도움이 되지만 계산 비용이 크다.</p>
</li>
<li>
<p>본 논문은 양자화가 단순한 효율화 도구가 아니라, 학습 성능을 높일 수 있는 조력자로 작동할 수  있음을 보인다.</p>
</li>
<li>
<p>양자화 과정을 RLHF 루프에 통합하여 모델을 효율적이면서도 더 강하게 학습시킨다.</p>
</li>
<li>
<p><strong>양자화 잡음(quantization noise)을 exploration으로 활용해 policy diversity를 높인다.</strong> -&gt; 이게 가능해..?</p>
</li>
<li>
<p>양자화가 gradient update 시 regularization처럼 작용해 안정적인 학습을 유도한다.</p>
</li>
<li>
<p>RL과 양자화의 상호작용을 수학적으로 분석하여 정량적 이득을 입증.</p>
</li>
<li>
<p>QeRL은 기존 RLHF 대비 동일한 계산량으로 더 높은 reward 모델 점수를 달성. -&gt;  3~5% 향상, 추론 효율은 2배 이상 개선.</p>
</li>
<li>
<p>INT8 또는 INT4 수준의 압축에서도 성능 저하가 거의 없으며, 일부 과제에서는 향상.</p>
</li>
</ul>
<blockquote>
<p>“Quantization noise acts as a stochastic perturbation that promotes exploration in the policy space.”</p>
</blockquote></div><footer class="row docusaurus-mt-lg"></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/2025/10/07/">10-07 papers summary</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-07T00:00:00.000Z">October 7, 2025</time> · <!-- -->4 min read</div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-dragon-hatchling">THE DRAGON HATCHLING<a href="#the-dragon-hatchling" class="hash-link" aria-label="Direct link to THE DRAGON HATCHLING" title="Direct link to THE DRAGON HATCHLING" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-missing-link-between-the-transformer-and-models-of-the-brain">THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN<a href="#the-missing-link-between-the-transformer-and-models-of-the-brain" class="hash-link" aria-label="Direct link to THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN" title="Direct link to THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN" translate="no">​</a></h3>
<p>개인적으로 흥미가 땡기는 논문. 보다 인간 뇌 뉴런 구조를 잘 모방한 AI 모델 구조를 연구한 논문이다. 최근 다양한 분야에서 연구되고 있는 장기 기억을 담당하는 : 예, long kv cache , 별도 장기기억 메모리 - 뉴런을 모방한 구조.</p>
<p>•	BDH (Brain-like Dense Hierarchy): 뉴런 활성값을 벡터 공간으로 확장해, dense activation과 sparse positive activation을 결합.
•	BDH-GPU: 병렬 계산에 맞춘 GPU 버전, 계층적 신경 interaction을 분산 병렬로 처리.
•	Grandfather Neurons: 핵심 패턴을 장기 유지하는 메모리 유닛.
•	Dense activation → “즉각적 반응”, Sparse activation → “장기 기억” 역할.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="judging-with-confidence-google">Judging with Confidence <code>google</code><a href="#judging-with-confidence-google" class="hash-link" aria-label="Direct link to judging-with-confidence-google" title="Direct link to judging-with-confidence-google" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="calibrating-autoraters-to-preference-distributions">Calibrating Autoraters to Preference Distributions<a href="#calibrating-autoraters-to-preference-distributions" class="hash-link" aria-label="Direct link to Calibrating Autoraters to Preference Distributions" title="Direct link to Calibrating Autoraters to Preference Distributions" translate="no">​</a></h3>
<p>기존 llm autorator 는 A/B 둘 중 하나를 선택하는 0/1 label 방식 선택함. 하지만 실제 사람들의 선호도는 딱 떨어지는 게 아니라 확률적으로 나뉘어짐. 해당 논문은 LLM autorator 가 선호도의 확률 분포를 반영할 수 있도록 학습시킴.</p>
<p>예) &quot;A와 B중 A가 70% 더 사람들에게 선호된다.&quot;</p>
<p>먼저, 데이터를 만듬. 답변의 선호 분포가 실제로 있는 케이스 그리고 선호 분포가 없는 &quot;단일 답  변&quot; 마 소유하고 있는 케이스. 당연히 답변 선호 분포 데이터가 훨씬 만들기 어렵다.</p>
<p>답변 선호 분포를 아는 경우 <strong>SFT</strong> 방식으로 학습시킴. 그냥 regression 처럼 학습시켰다 보면 됨.</p>
<p>단일 답변인 경우 <strong>RL</strong> 방식을 사용함. binary classification 과 비슷. 답변의 선호 확률을 잘 맞춰야함. 답변이 1일 경우 확률을 1에 가깝게, 답변이 0일 경우 확률을 0에 가깝게. 별 효과가 있을까 의문이 드는 방식인데 논문에서는 이 방식으로도 충분히 의미있는 효과를 보여줬다고 함. RL 방식에서 loss 는 quadratic 과 cross-entropy 를 함께 사용. RL 이라 이름 붙인 이유는 loss 가 아니라 reward 형태로 학습시켰기 때문에. (RLHF)</p>
<p>결과적으로,</p>
<ul>
<li>정량 지표 : MSE – 18<del>51% 감소, ECE – 44</del>45% 감소, 편향 감소.</li>
<li>인간 평가 일치도 : RL-Brier 모델이 GPT-4 및 JudgeLM 보다 우수.</li>
<li>객관 과제 : JudgeBench 정확도 46.6% 로 기존 모델과 동등 수준 유지 → 추론 능력 손상 없음.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="swireasoning-microsoft">SWIREASONING <code>microsoft</code><a href="#swireasoning-microsoft" class="hash-link" aria-label="Direct link to swireasoning-microsoft" title="Direct link to swireasoning-microsoft" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="switch-thinking-in-latent-and-explicit-for-pareto-superior-reasoning-llms">SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS<a href="#switch-thinking-in-latent-and-explicit-for-pareto-superior-reasoning-llms" class="hash-link" aria-label="Direct link to SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS" title="Direct link to SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS" translate="no">​</a></h3>
<p>pareto-optimal 또는 pareto-frontier 는 어떤   지표를 좋게 하려면 어떤 지표를 희생해야 하는 상태. 반대로 본 논문은 pareto 관계의 두 가지 지표를 모두 상승 시켜 pareto-superior 라고 이름붙임.</p>
<p>모델의 잠재 사고 과정은 token effciency 가 높지만 명시 사고보다 정확도가 떨어짐. 이를 해결하기 위해서 switch module 을 도입.</p>
<div style="text-align:center"><img src="/assets/images/blog_20251007_img0-849ef228d1d37e3487053a47fb5f067c.png" style="width:500px"></div>
<p>모델이 헷갈려하는 순간, 즉 entropy 가 높아지는 순간은 모델이 명시적으로 사고하도록 하고 모델의 entropy 가 낮은 순간은 모델이 잠재 사고를 하도록함. 그리고 너무 많은 switch 가 일어나지 않도록 control.</p>
<p>이를 통해 정확도와 토큰 효율 둘다 달성했다고 함.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="hybrid-architectures-for-language-models-meta">Hybrid Architectures for Language Models <code>meta</code><a href="#hybrid-architectures-for-language-models-meta" class="hash-link" aria-label="Direct link to hybrid-architectures-for-language-models-meta" title="Direct link to hybrid-architectures-for-language-models-meta" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="systematic-analysis-and-design-insights">Systematic Analysis and Design Insights<a href="#systematic-analysis-and-design-insights" class="hash-link" aria-label="Direct link to Systematic Analysis and Design Insights" title="Direct link to Systematic Analysis and Design Insights" translate="no">​</a></h3>
<p>카이스트와 메타가 써서 흥미가 든 논문. transformer 와 mamba 구조의 장점을 잘 조합한 새로운 구조를 제안함. 나와 관련이 낮아 보여 가볍게 요약만 확인함.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="front-loading-reasoning-nvidia">Front-Loading Reasoning <code>nvidia</code><a href="#front-loading-reasoning-nvidia" class="hash-link" aria-label="Direct link to front-loading-reasoning-nvidia" title="Direct link to front-loading-reasoning-nvidia" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-synergy-between-pretraining-and-post-training-data">The Synergy between Pretraining and Post-Training Data<a href="#the-synergy-between-pretraining-and-post-training-data" class="hash-link" aria-label="Direct link to The Synergy between Pretraining and Post-Training Data" title="Direct link to The Synergy between Pretraining and Post-Training Data" translate="no">​</a></h3>
<p>엔비디아니까 할만하다고 생각되는 규모의 대규모 실험. &quot;추론 데이터&quot; 를 언제, 어떻게 사용해야 최적의 효과를 볼 수 있는지에 대한 전방위적 분석. 기존 연구는 post-training 단계에 집중해서 분석하나 해당 논문은 pretraining 단계까지 분석 범위에 포함시켰고 다양한 재밌는 결과를 확인했다고 주장한다.</p>
<p>결론부터 말하면, 다양한 대규모 추론 데이터를 사전 학습 단계에 전방 배치하고 (fron-load) SFT 단계에서는 고품질 데이터를 사용하는 것이 최적이라는 것. 기존의 <strong>추론 학습은 post-training (SFT, RL) 단계에서만 중요하다</strong> 라는 관점을 정량지표와 함께 반박. 최대 19% 까지 수학, 과학, 코딩 벤치마크 지표 우위를 보였음.</p>
<p>pretraining 단계에서 대규모의 다양한 추론 데이터의 중요성과 함께 재밌는 지점도 많이 적혀있다.</p>
<ol>
<li>사전학습 단계에서 추론을 연습하지 않으면, SFT 단계에서 추론 데이터를 2배까지 차이를 내도 추론을 사전 연습한 모델을 따라잡지 못한다. 즉, 사전 추론 학습이 모델의 추론 성능 상한선을 결정한다.</li>
<li>사전학습에서는 다양성과 규모가 중요하고, SFT 에서는 품질이 중요하다.</li>
<li>사전 학습 단계 데이터에서 고품질 데이터의 중요성 -&gt; 사전 학습 대규모 추론 데이터에  고품질 데이터의 포함 유무에 따른 모델의 성능 차이는 바로 드러나지 않는다. 그러나 SFT 이후에 효과가 드러난다. 즉 사전 학습 추론 데이터 중 고품질 데이터는 &quot;잠재적&quot; 으로 모델에 좋은 영향을 미친다.</li>
</ol>
<p>LLM pretraining 을 직접할 일은 거의 없겠지만 모델이 동작에서 얻어갈 수 있는 인사이트가 있을 것으로 보인다.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">“We define pretraining as large-scale self-supervised learning over a mixture of general and reasoning data,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">and post-training as any supervised fine-tuning or reinforcement stage that aligns the model toward specific reasoning quality.”</span><br></span></code></pre></div></div></div><footer class="row docusaurus-mt-lg"></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/2025/10/05/">10-05 papers summary</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-05T00:00:00.000Z">October 5, 2025</time> · <!-- -->3 min read</div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="winning-the-pruning-gamble">Winning the Pruning Gamble<a href="#winning-the-pruning-gamble" class="hash-link" aria-label="Direct link to Winning the Pruning Gamble" title="Direct link to Winning the Pruning Gamble" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-unified-approach-to-joint-sample-and-token-pruning-for-efficient-supervised-fine-tuning--alibaba">A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning  <code>alibaba</code><a href="#a-unified-approach-to-joint-sample-and-token-pruning-for-efficient-supervised-fine-tuning--alibaba" class="hash-link" aria-label="Direct link to a-unified-approach-to-joint-sample-and-token-pruning-for-efficient-supervised-fine-tuning--alibaba" title="Direct link to a-unified-approach-to-joint-sample-and-token-pruning-for-efficient-supervised-fine-tuning--alibaba" translate="no">​</a></h3>
<p>LLM의 Supervised Fine-Tuning(SFT)은 단순한 후처리가 아니라 중간 훈련(mid-training) 수준의 계산비용이 필요한 단계가 되었다. post-training 단계에서 데이터 효율성은 매우 치명적인 이슈다. 이 내용은 내가 블로그, 또는 논문 정리를 하면서 작성했던 적이 있다. 일반적으로는 <strong>data pruning</strong> 방식을 사용한다. data pruning 사용 시 일반적으로 두 가지 방법 중 하나를 선택한다. 샘플 단위 혹은 토큰 단위. 두 가지 방식의 장단점이 있는데, 본 논문에서는 두 방식의 장점을 합친 방식을 제안한다.</p>
<p>ㅂ</p>
<p>논문의 핵심은 entropy 와 perplexity 를 통해 데이터를 4가지로 분류할 수 있고, 각 데이터에 대해 서로 다른 pruning 방법을 적용해야 한다는 것.</p>
<p>entropy 와 perplexity 가 전부 낮다. -&gt; 이미 잘 알고있는 정보. 학습 중요도가 낮음.
entropy 와 perpelxity 가 둘다 높다. -&gt; 노지으일 확률이 높은 데이터. 학습에서 제외.
entopy 는 낮으나 perplexity 는 높은 데이터 -&gt; 모델이 확실하게 틀린 답을 내놓는 경우. decisoin boundary 이므로 학습 가치가 높음. 오개념을 수정하는 방향으로 학습.
entropy 는 높고 perplexity 는 높은 데이터 -&gt; 모델이 불확실하다고 하며 틀린 답을 내놓는 경우가 많음. calibration 이 잘되고 있는 데이터. 그냥 사용.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="long-live">Long live<a href="#long-live" class="hash-link" aria-label="Direct link to Long live" title="Direct link to Long live" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="real-time-interactive-long-video-generation-nvidia">REAL-TIME INTERACTIVE LONG VIDEO GENERATION <code>nvidia</code><a href="#real-time-interactive-long-video-generation-nvidia" class="hash-link" aria-label="Direct link to real-time-interactive-long-video-generation-nvidia" title="Direct link to real-time-interactive-long-video-generation-nvidia" translate="no">​</a></h3>
<p>autoregressive 방식으로는 긴 영상을 잘 만들기 어려움. 근데 여기다가 prompt interaction 을 추가하기는 더 어려움. 그렇다고 diffusion 모델을 사용할수도 없다. 너무 느리니까.</p>
<p>논문에서는 prompt interactive video generation 방식을 소개함. 기본적으로 autoregressive 방식을 사용.</p>
<p>autoregressive 방식을 사용.</p>
<p>prompt 전환시 kv-recahce 방식 사용. 이전 프레임의 시각 맥락은 유지하면서, prompt 의미 맥락만을 재활용.</p>
<p>steaming long-tuning : 과거 kv 를 이어받은 후 autoregressive 모델을 이용해 5초 단위 roll-out 생성 + DMD alignment</p>
<p>short video attention + frame sink 을 이용해서 영상 전반적인 일관성 유지 : 짧은 비디오 문맥만 파악하되, 영상 전반적으로 중요한 정보는 고정 키-밸류로 계속 참조시킴.</p>
<p>20.7FPS + 240 초까지 영상 생성했다고 함.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="opengpt-4o-image">OpenGPT-4o-Image<a href="#opengpt-4o-image" class="hash-link" aria-label="Direct link to OpenGPT-4o-Image" title="Direct link to OpenGPT-4o-Image" translate="no">​</a></h2>
<p>GPT4o-image 의 테크 리포트이자, 데이터를 어떻게 생성해서 어떻게 학습시켰는지에 대한 논문. 데이터 생성 방식의 중요성이 특히 pretraiing 에서 점점 커지고 있는 거 같다.</p>
<p>gpt-4o-image 는 이미지-테스트 멀티 모달 관련 데이터셑을  충분히 만들기 위해 먼저 필요한 과제들을 매우 세세하게 체계화시킴. 각 과제들을 분류하고 또 과제들을 하위 과제로 체계화시켰음. 예를 들어서 인과 추론의 경우 명시적인 추론, 암묵적인 추론, 복합 인과 사슬등으로 세분화함. 저자들은 이 과정이 매우 중요하다고 하는데, 이렇게 분류를 해놓아야 명확하게 템플릿화된 프롬프트를 통해 대량의 학습 데이터를 생성할 수 있기 떄문임.</p>
<p>먼저 소스 이미지를 충분히 수집함. 실제 이미지 데이터 뿐 아니라 omniEdit, ImgEdit, gpt-4o 등도 활용함.</p>
<ul>
<li><a href="https://tiger-ai-lab.github.io/OmniEdit/" target="_blank" rel="noopener noreferrer">https://tiger-ai-lab.github.io/OmniEdit/</a></li>
<li><a href="https://github.com/PKU-YuanGroup/ImgEdit" target="_blank" rel="noopener noreferrer">https://github.com/PKU-YuanGroup/ImgEdit</a></li>
</ul>
<p>다만, 모델 학습 일관성을 위해 특정 모듈 학습에는 특정 소스의 데이터만 사용함. 예를 들어, subject-driven generation 에서는 OmniEdit 소스 데이터만 활용.</p>
<p>그 후, 체계화시킨 템플릿 기반으로 모듈별로 대량의 prompt 를 생성함.</p>
<p>원본 이미지와 prompt 를 활용해서 gpt-1-image-api 를 활용해서 생성한 데이터를 사용함. reference image editing 의 경우 reference image, edited image 를 가지고 source image 를 다시 생성해내는 과정을 가짐. 그냥 학습 시켰을 때 잘 안되서 추가한 것으로 보임. gpt-1-api 는 비공개 모델.</p>
<p>시스템화된 데이터 생성 파이프라인을 통해 고품질의 학습 데이터를 만들어서 모델 학습시킴으로써 오픈 소스 생태계 확장 및 distillation 의 중요성을 보여준다.</p></div><footer class="row docusaurus-mt-lg"></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/page/2"><div class="pagination-nav__label">Older entries</div></a></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/community/hello">Hello, Lapis</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.instagram.com/or7l_floll/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/jhpark9701/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://https://github.com/logicbaron" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://leetcode.com/superstone/" target="_blank" rel="noopener noreferrer" class="footer__link-item">leetcode<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>