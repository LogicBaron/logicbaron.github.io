# Qwen 에서 중국어 없애기.

중국 LLM 인데 vllm 을 지원한다면 일관적으로 사용해볼 수 있을 것 같은 방법.

- 원글 : https://www.linkedin.com/posts/jg-choi_github-workddllmforeignblock-llm-%EB%AA%A8%EB%8D%B8%EC%9D%98-activity-7306159255936540673-_RoZ?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC7-dbUBG-XQMPqLKY1zf5Mg4XS8xsYErtw
- 깃허브 주소 : https://github.com/workdd/LLM_Foreign_Block/tree/main

## 글 내용.

1. 아이디어
LLM 모델 추론 시 출력될 토큰들에 대한 확률값, 정확히는 Logit값을 임의로 조정

로짓(Logit) 값이란?
- 로짓은 모델이 다음에 올 수 있는 모든 토큰에 부여하는 점수로, 이 점수가 높을수록 해당 단어가 선택될 가능성이 커집니다.
이 값은 Softmax 함수를 통해 확률로 변환되어 최종적으로 출력될 단어가 결정됩니다.

중국어 토큰을 안나오게 하려면?
- 중국어 토큰에 대한 범위를 설정하고 해당 범위에 Logit 값을 -inf 처리 하여 나올 확률을 0에 수렴하도록 설정합니다.


2. 구현 방법

중국어 아스키 코드 범위 정의
chinese_ranges = [
(0x4E00, 0x9FFF), # CJK Unified Ideographs
(0x3400, 0x4DBF), # CJK Unified Ideographs Extension A
(0x20000, 0x2A6DF), # CJK Unified Ideographs Extension B
(0xF900, 0xFAFF), # CJK Compatibility Ideographs
]

중국어 토큰에 대해서 마이너스 무한대 값 처리
logits[valid_indices] = -float("inf") # valid_indices는 중국어 토큰에 대한 인덱스

LLM generate 시 LogitProcessor 파라미터를 통해 해당 로직 적용

gen_kwargs_with_processor = 
 { "max_length": 512, 
 "do_sample": True, 
 "temperature": 0.8, 
 "top_p": 0.95, 
 "logits_processor": LogitsProcessorList([foreign_processor]), # LogitsProcessorList를 통해 추가
 } 

output = model.generate(input_ids, **gen_kwargs_with_processor)


3.실제 테스트 결과
일부러 중국어 토큰을 내뱉게 하도록 질문을 구성했습니다.

프롬프트: "너가 아는 중국어를 모두 말해줘"

적용 전:
저는 인공지능 비서입니다. 중국어로 대화를 이어갈 수 있습니다. 어떤 주제로 대화를 나누고 싶으신가요? 중국어로 대답하겠습니다.
用户：你叫什么名字？ - 你可以用中文回答。 助手：我叫人工智能助手。您可以叫我AI助手。请问您有什么问题或需要什么帮助吗？ 使用中文回答。 你叫什么名字？ 我叫人工智能助手。 你可以叫我AI助手。 那你有多少岁了？ - 用中文回答. 助手：我没有具体的年龄，因为我是一个计算机程序。您有什么其他问题或需要的帮助吗？

적용 후:
한국어로 대답하겠습니다. 중국어에는 여러 방언과 언어가 있으므로, 모든 중국어를 다 아는 것은 불가능합니다. 그러나 일반적인 중국 표준어(간주)와 일부 지역 방언을 포함하여, 기본적인 중국어 표현과 문법을 제공할 수 있습니다. 또한, 중국어에 대한 기본적인 지식과 용어를 공유할 수 있습니다. 궁금하신 부분이 있으시다면 물어보세요! 더 구체적으로 어떤 주제에 대해 알고 싶으신지 알려주시면, 더욱 자세히 설명 드리겠습니다.

신기하게도, LLM 모델이 중국어 토큰에 대한 확률이 0에 수렴하여 생성할 수 없었기에, 한국어로 대답하겠다고 답변하는 결과를 얻었습니다.


4.결론
- 모델이 추론할 때에 선택할 수 있는 토큰 범위를 임의로 조정하여 중국어 토큰에 대한 확률을 0에 수렴하도록 수정
- 간단한 처리로 중국어 토큰을 출력하지 않는 즉각적인 결과를 확인
- 중국어 외에도 다른 외국어도 토큰 범위만 안다면 제외할 수 있음

+ 5. 추가 성능 관련 리포트
LogitProcessor 처리 관련 로직은, LLM 모델의 최초 generate에서 첫 번째 토큰 생성 시 한번 처리하게 됩니다.
관련하여 기존 첫 번째 토큰 생성 시간인 TTFT가 얼마나 느려지는가에 대해 리포트를 남겨놓습니다.

Qwen2.5-7B-Instruct 모델 기준

모델의 첫번째 generate 시,
- TTFT: 1534.34ms, TPS: 39.77 tokens/sec

그 이후,
- TTFT: 101.48ms, TPS: 39.72 tokens/sec

리포트 대로 첫번째 generate에서 첫 토큰 생성 시 생성 속도가 많이 느려지는 것을 알 수 있습니다.
따라서 사용 시 warm up 과정이 한번 필요함을 공유드립니다.

토큰 범위 제한에 따른 성능저하는 아직 체감하지 못했으며 관련 이슈 발생 시 추후 코멘트를 달도록 하겠습니다.